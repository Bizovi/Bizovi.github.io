[
  {
    "objectID": "art.html",
    "href": "art.html",
    "title": "{Art, Math, Philosophy}",
    "section": "",
    "text": "Winter, Chisinau 24x30cm\n\n\n\n\n\n\n\n\n\n\n\nChurch courtyard, Breb, 24x30cm\n\n\n\n\n\n\n\nThe highline, NY, 30x40cm\n\n\n\n\n\n\n\n\n\nIn bloom, IOR, 30x24cm\n\n\n\n\n\n\n\nWalking home, Bucharest, 30x24cm\n\n\n\n\n\n\n\n\n\nDuality, Prague, 30x24cm\n\n\n\n\n\n\n\nAt the office, Bucharest, 30x24cm\n\n\n\n\n\n\n\n\n\nVillage museum, Negresti-Oas, 24x30cm\n\n\n\n\n\n\n\nStorm, Mangalia, 24x30cm\n\n\n\n\n\n\n\n\n\nBreaking the waves, Mangalia, 24x30cm\n\n\n\n\n\n\n\nDirt road, 24x34cm\n\n\n\n\n\n\n\n\n\nShade, Toulon, 24x30cm\n\n\n\n\n\n\n\nFall, Wyndham, 24x30cm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParcul National, Bucharest, 24x30cm\n\n\n\n\n\n\n\nParcul National, Bucharest, 24x30cm\n\n\n\n\n\n\n\n\n\nHammock, Balotesti, 30x24cm\n\n\n\n\n\n\n\nReflections, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\nAt the willows, IOR, 19x26cm\n\n\n\n\n\n\n\nThe duck spot, IOR, 19x24cm\n\n\n\n\n\n\n\n\n\nOvercast, IOR, 19x26cm\n\n\n\n\n\n\n\nFall, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChurch, Piatra Neamt, 30x24cm\n\n\n\n\n\n\n\nMovie night, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\nBefore sunset, Bucharest, 24x30cm\n\n\n\n\n\n\n\nView from the garage, Bucharest, 13x19cm\n\n\n\n\n\n\n\n\n\nPiata Alba Iulia, 19x26cm\n\n\n\n\n\n\n\nDambovita, 19x26cm"
  },
  {
    "objectID": "art.html#light-and-shadow-in-watercolor",
    "href": "art.html#light-and-shadow-in-watercolor",
    "title": "{Art, Math, Philosophy}",
    "section": "",
    "text": "Winter, Chisinau 24x30cm\n\n\n\n\n\n\n\n\n\n\n\nChurch courtyard, Breb, 24x30cm\n\n\n\n\n\n\n\nThe highline, NY, 30x40cm\n\n\n\n\n\n\n\n\n\nIn bloom, IOR, 30x24cm\n\n\n\n\n\n\n\nWalking home, Bucharest, 30x24cm\n\n\n\n\n\n\n\n\n\nDuality, Prague, 30x24cm\n\n\n\n\n\n\n\nAt the office, Bucharest, 30x24cm\n\n\n\n\n\n\n\n\n\nVillage museum, Negresti-Oas, 24x30cm\n\n\n\n\n\n\n\nStorm, Mangalia, 24x30cm\n\n\n\n\n\n\n\n\n\nBreaking the waves, Mangalia, 24x30cm\n\n\n\n\n\n\n\nDirt road, 24x34cm\n\n\n\n\n\n\n\n\n\nShade, Toulon, 24x30cm\n\n\n\n\n\n\n\nFall, Wyndham, 24x30cm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParcul National, Bucharest, 24x30cm\n\n\n\n\n\n\n\nParcul National, Bucharest, 24x30cm\n\n\n\n\n\n\n\n\n\nHammock, Balotesti, 30x24cm\n\n\n\n\n\n\n\nReflections, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\nAt the willows, IOR, 19x26cm\n\n\n\n\n\n\n\nThe duck spot, IOR, 19x24cm\n\n\n\n\n\n\n\n\n\nOvercast, IOR, 19x26cm\n\n\n\n\n\n\n\nFall, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChurch, Piatra Neamt, 30x24cm\n\n\n\n\n\n\n\nMovie night, IOR, 30x24cm\n\n\n\n\n\n\n\n\n\nBefore sunset, Bucharest, 24x30cm\n\n\n\n\n\n\n\nView from the garage, Bucharest, 13x19cm\n\n\n\n\n\n\n\n\n\nPiata Alba Iulia, 19x26cm\n\n\n\n\n\n\n\nDambovita, 19x26cm"
  },
  {
    "objectID": "art.html#landscape-impressions-in-oil",
    "href": "art.html#landscape-impressions-in-oil",
    "title": "{Art, Math, Philosophy}",
    "section": "Landscape impressions in oil",
    "text": "Landscape impressions in oil\n\n\n\nAfter the Storm, IOR, 30x30cm\n\n\n\n\n\nSunset, IOR, 24x18cm\n\n\n\n\n\n\n\n\n\n\n\nAltar, Busteni, 30x30cm\n\n\n\n\n\n\n\nThe hills of Piatra Neamt, 30x30cm\n\n\n\n\n\n\n\n\n\nSunlit park in Sevilla, 30x40cm\n\n\n\n\n\n\n\nAzuga trail, 18x23cm\n\n\n\n\n\n\n\n\n\nMontjuic Castle, 30x40cm\n\n\n\n\n\n\n\nToulon Cypresses, 15x20cm\n\n\n\n\n\n\n\n\n\nCaucasus, 15x20cm\n\n\n\n\n\n\n\nFall in Fagarasi, 15x20cm\n\n\n\n\n\n\n\n\nSpring in Bergamo, 20x40cm\n\n\n\n\n\nMorining in Wyndham, 24x33cm\n\n\n\n\n\n\n\n\nArchive\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA memory from Boian, 30x30cm\n\n\n\n\n\n\n\nSunset at Arsenale, 30x30cm"
  },
  {
    "objectID": "art.html#miscellanea",
    "href": "art.html#miscellanea",
    "title": "{Art, Math, Philosophy}",
    "section": "Miscellanea",
    "text": "Miscellanea\n\nPastel\n\n\n\n\n\n\n\n\n\nLast days of summer, 20x30cm\n\n\n\n\n\n\n\nThe lone tree, 20x30cm\n\n\n\n\n\n\n\n\n\nOld town, 15x20cm\n\n\n\n\n\n\n\nUntitled, Bucharest, 20x30cm\n\n\n\n\n\n\n\n\n\nSunlit Trees at Muncii, 20x15cm\n\n\n\n\n\n\n\nCoffee break, 16x13cm\n\n\n\n\n\n\n\nStill life\n\n\n\n\n\n\n\n\n\nStill life, 24x16cm\n\n\n\n\n\n\n\nStill life (sketchbook), 16x21cm\n\n\n\n\n\n\n\n\n\nBluestone Lane, NY, 24x12cm\n\n\n\n\n\n\n\nPaper bag, 24x18cm\n\n\n\n\n\n\n\n\n\nAll dry, 40x30cm\n\n\n\n\n\n\n\nStill Life after Ian Roberts, 40x40cm\n\n\n\n\n\n\n\nPortraits\n\n\n\n\n\n\n\n\n\nStudy after McCurry, 30x20cm\n\n\n\n\n\n\n\nAM hackathon illustration, 40x30cm\n\n\n\n\n\n\n\n\n\nEleonora, 20x15cm\n\n\n\n\n\n\n\nEarth’s World 1, 30x20cm\n\n\n\n\n\n\n\n\n\nEarth’s World 2, 30x20cm\n\n\n\n\n\n\n\nEarth’s World 3, 30x20cm\n\n\n\n\n\n\n\n\n\nEarth’s World 4, 30x20cm\n\n\n\n\n\n\n\nEarth’s World 5, 30x20cm\n\n\n\n\n\n\n\n\n\nNMA reference, 20x15cm\n\n\n\n\n\n\n\nNMA reference, 20x15cm\n\n\n\n\n\n\n\n\n\nNMA reference, 20x15cm\n\n\n\n\n\n\n\nNMA reference, 20x15cm\n\n\n\n\n\n\n\nPets\n\n\n\n\n\n\n\n\n\nArtemis by the window, 20x15cm\n\n\n\n\n\n\n\nArtemis and Tigrisor, 15x20cm\n\n\n\n\n\n\n\nFrigure and life drawing\n\n\n\nStudy after B.C. Liston reference, 20x15cm\n\n\n\n\n\n\n\n\n\n\n\nLife drawing, 21x30cm\n\n\n\n\n\n\n\nLife drawing, 21x30cm\n\n\n\n\n\n\n\n\n\nLife drawing, 21x30cm\n\n\n\n\n\n\n\nLife drawing, 10min, 15x30cm\n\n\n\n\n\n\n\n\n\nGestures, 20x30cm\n\n\n\n\n\n\n\nGestures, 20x30cm\n\n\n\n\n\n\n\n\n\nGestures, 20x30cm\n\n\n\n\n\n\n\nGestures, 20x30cm\n\n\n\n\n\n\n\nUrban sketching\n\n\n\n\n\n\n\n\n\nTea at Bernschultz, 20x30cm\n\n\n\n\n\n\n\nWires, 20x15cm\n\n\n\n\n\n\n\n\n\nCoffee at Loom 68, 20x30cm\n\n\n\n\n\n\n\nTrain to Brasov, 20x30cm\n\n\n\n\n\n\n\n\n\nCoffee at Loom 68, 20x30cm\n\n\n\n\n\n\n\nCoffee at Boiler 2, 20x30cm\n\n\n\n\n\n\n\n\n\nCoffee at Boiler 1, 20x30cm\n\n\n\n\n\n\n\nUrban sketchers meetup, 20x30cm\n\n\n\n\n\n\n\nNotans and composition studies\n\n\n\n\n\n\n\n\n\nBoatyard in Venice, 24x30cm\n\n\n\n\n\n\n\nFence in Predeal, 20x30cm"
  },
  {
    "objectID": "art.html#photography",
    "href": "art.html#photography",
    "title": "{Art, Math, Philosophy}",
    "section": "Photography",
    "text": "Photography\n\n\n\nBefore the rain, Rimini\n\n\n\n\n\n\n\n\n\n\n\nSerenity, Rimini\n\n\n\n\n\n\n\nSunset, Rimini\n\n\n\n\n\n\n\nSheep, Busteni\n\n\n\n\n\n\n\n\n\nThe pier, Rimini\n\n\n\n\n\n\n\nCliffs, Busteni\n\n\n\n\n\n\n\nRolling hills, Livigno\n\n\n\n\n\n\n\n\n\nSheep, Busteni\n\n\n\n\n\n\n\nCliffs, Livigno\n\n\n\n\n\n\n\nWires, Livigno\n\n\n\n\n\n\n\n\n\nTrail, Livigno\n\n\n\n\n\n\n\nTrail, Livigno\n\n\n\n\n\n\n\nTrail, Livigno\n\n\n\n\n\n\n\n\n\nRolling hills, Livigno\n\n\n\n\n\n\n\nRolling hills, Livigno\n\n\n\n\n\n\n\nRolling hills, Livigno\n\n\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\n\nClouds, Rimini"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interdisciplinary Unorthodoxy",
    "section": "",
    "text": "Mihai Bizovi | VP of Decision Science @AdoreMe\n\nAt my day job, I’m a generalist – navigating uncertainty and complexity to improve decision-making at scale.\nAs a teacher, I aspire to contribute to the understanding of AI’s complex landscape; how to navigate it, develop valuable skills, and become more effective at problem-solving.\nAs a person, I aspire to a life of wisdom and meaning.\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n10 Lessons in Humility\n\n\n\n\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nBizovi Mihai\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Analysis of Facebook Network of Friends\n\n\n\n\n\n\nnetworks\n\n\n\n\n\n\n\n\n\nJan 9, 2019\n\n\nBizovi Mihai\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Caratheodori Extension Theorem\n\n\n\n\n\n\nprobability\n\n\n\n\n\n\n\n\n\nJun 1, 2018\n\n\nBizovi Mihai\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Friends with Probability\n\n\n\n\n\n\nprobability\n\n\nbayes\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\nAug 7, 2017\n\n\nBizovi Mihai\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html",
    "href": "posts/2018-intro-measure/measure_theory.html",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "",
    "text": "The reason we need probability theory is that it’s a formal language of uncertainty. Even though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its measure-theoretic foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.\nAs an economist and modeler, the goal of this project is to step-by-step explain the Caratheodori Extension Theorem in order to understand the language, gain some intuition and insight about it. The value of studying it comes not from the theorem itself, but from the process of discovery and understanding a proof forces you to go through. (Landim 2016) If you’re a mathematician and for some reason reading this, you might ask:\nThe answer is that I want to learn the language, and not to achieve excellence in Measure Theory. Also, I can hardly take things for granted and need a justification of why things (in probability) are done exactly this way. (Rhosental 2006) We’ll develop a plan of attack and a network of ideas and concepts that need to be understood in order to tackle the problem, hoping that resourcefulness and intuitions will compensate for the lack of rigor."
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#motivation-for-measure-theory-in-a-practice-oriented-world",
    "href": "posts/2018-intro-measure/measure_theory.html#motivation-for-measure-theory-in-a-practice-oriented-world",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Motivation for measure theory in a practice-oriented world",
    "text": "Motivation for measure theory in a practice-oriented world\nThe field of Data Mining moved a long way, becoming accessible and bringing value for individuals and industries. A lot of Machine Learning and Statistical models are available with a few lines of code. If it should be obvious why we need probability theory, it’s not so with measure theory.\n\nSee the Andrew Gelan (a giant of Bayesian statistics) and Cosma Shalizi (an expert in Data Mining) disagreement on the subject\n\nI’ll give an analogy: even though the models can be easily applied in a high-level language like R or Python, understanding the Learning Theory can bring you on another level, closer to excellence. In the case of measure theory, some argue that there are alternative things to study which can bring more value, and they’re not wrong, but for an ambitious field like Bayesian Nonparametrics, it’s hard to make even little progress because of the understanding barrier. This is why I like to think of it as a language (Lawrence 2012) extremely useful in the fields of stochastic processes and learning theory. So, the truth is somewhere in between and key to learning these subjects is a personally optimal balance of theoretical understanding, practice on real data problems and simulation exercises.\nStudying measure theory might look like a gruesome process to do on your own, but it makes sense posing a reverse question: what do I need to know to understand all these awesome papers where there is a urge to ask: “The probability over what?”\n\nA personal experience was in an attempt to study nonlinear state-space models, where there are some exciting papers on Bayesian Nonparametrics and Stochastic Filtering. Reading and working through papers felt like missing a good part of the story, because of some lacking fundamentals. It’s extremely important to recognize what you don’t know. That’s right, I want to be able to formulate meaningful statements about distributions of more abstract objects, like functions, graphs, etc and to reason about stochastic processes.\n\nFor example, having a great understanding of probability helps to define in a clear and rigorous way difficult concepts used in statistics and econometrics (which might look deceptively simple at first) as p-values, confidence intervals, power, hypothesis testing and helps avoiding a lot of confusion. Let’s take the idea of power of the test in its simplest form (for a one-sided Z test), which a lot of practitioners struggle to define when asked.\n\nThe mathematical formulation uncovers some of the assumptions we’re making and suggests the interpretation. Notice how \\[\\frac{\\mu_a - \\mu_0}{\\sigma}\\] is a proxy for a “unit free” effect size.\n\n\n\\(\\beta\\) Type II error: Failure to reject \\(H_0\\) when it’s false\n\\(\\alpha\\) Type I error: Falsely rejecting a true \\(H_0\\)\n\\(1 - \\beta\\) is the power\n\\(\\mu_0\\) the null hypothesis\n\\(\\mu_a\\) the alternative hypothesis\n\nThe response of the power being the probability of rejecting a null hypothesis when it’s false might not suggests that there is much going on.\n\\[\\begin{align*}\n    1- \\beta &= \\mathbb{P} \\bigg( \\frac{\\bar{X} - \\mu_0}{\\sigma / \\sqrt{n}} &gt;\n    \\mathbf{Z}_{1-\\alpha}  \\bigg\\lvert \\mu = \\mu_a  \\bigg) \\\\\n    ~ &= \\mathbb{P} \\bigg( \\frac{\\bar{X} - \\mu_a + \\mu_a - \\mu_0}{\\sigma / \\sqrt{n}} &gt;\n    \\mathbf{Z}_{1-\\alpha}  \\bigg\\lvert \\mu = \\mu_a  \\bigg) \\\\\n    ~ &= \\mathbb{P} \\bigg( \\frac{\\bar{X} - \\mu_a}{\\sigma / \\sqrt{n}} &gt;\n    \\mathbf{Z}_{1-\\alpha} - \\frac{\\mu_a - \\mu_0}{\\sigma / \\sqrt{n}} \\bigg\\lvert \\mu = \\mu_a  \\bigg) \\\\\n    ~ &= \\mathbb{P} \\bigg( \\mathbf{Z} &gt;\n    \\mathbf{Z}_{1-\\alpha} - \\frac{\\mu_a - \\mu_0}{\\sigma / \\sqrt{n}} \\bigg\\lvert \\mu = \\mu_a  \\bigg)\n\\end{align*}\\]\nBut there is a lot going on, the power depending on the effect size, assumed level for the type I error and the sample size. The distribution of the term \\[\\mathbf{Z} = \\frac{\\bar{X} - \\mu_a}{\\sigma / \\sqrt{n}}\\] is actually the one under the alternative hypothesis. Note that power calculations done post-hoc are usually a terrible idea.\nAlso, it’s almost impossible to sense the dangers of interpretations of p-values, types of errors and confidence intervals without trying to understand the mathematics behind statistical testing. Tests are also models, little “Golemns of the Prague” and they might fail in unexpected ways when the assumptions do not hold. (McElreath 2015)\nAs it’s often the case in mathematics, things have a deep justification behind them and even though you can successfully apply the models in practice, understanding is what separates a great modeler. Often, a breakthrough comes in the form of something that nobody have thought before.\n\nI think we’ll appreciate the input from mathematicians more in an applied field like Data Mining, as it will help figure out why deep neural networks work so well. Same is true for Extreme Gradient Boosting and other things that just seem to work. It took me some time trying to solve real world problems, in order to appreciate the usefulness of deeply understanding different ideas in mathematics.\n\nThis is why, in the path to mastery of machine learning, certain topics appear which might change your perspective forever. One of these is Measure Theory. Is it useful in practice? Probability Theory taught in undergraduate courses might be what most people need, but it’s limited in a certain sense, imposing an artificial dichotomy between discrete and continuous random variables and thinking in terms of probability density functions and cumulative distribution functions.\nRegarding (Caratheodori), it’s not the formulation of the theorem which brings the most insight, but ideas in the proof as measurable sets and outer measures."
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#measure-theory-in-machine-learning",
    "href": "posts/2018-intro-measure/measure_theory.html#measure-theory-in-machine-learning",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Measure Theory in Machine Learning",
    "text": "Measure Theory in Machine Learning\n\nSome courses will mention it, but as a side for the mathematically inclined students and not appearing anywhere later\n\nIn undergraduate probability we can get away with the lack of measure-theoretic notions, as we’re working on real spaces, continuous functions and the instruments we have in these tame cases seem enough. There are also wilder cases, in which we need new tools and language to be rigorous, as otherwise we would just hope for the best (that the probability measure is defined). In some of the fields mentioned above researchers have to deal with weird stuff like distributions which have continuous and discrete elements, when a mixture of a density with point masses isn’t very helpful to work with.\n\n\n Source: brownsharpie\nEvans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If \\(\\chi_{[0,1]}(x) = (e^{ix} - 1)/ix\\) is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.\n\\[\\begin{equation}\n    p(x) = w_1 \\chi_{[0,1]}(x) +  w_2\\delta_1(x)\n\\end{equation}\\]\nEven though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.\nJeffrey Rosenthal begins his book (Rhosental 2006) by a similar motivation, constructing the following random variable as a coin toss between a discrete \\(X \\sim Pois(\\lambda)\\) and continuous \\(Y \\sim \\mathcal{N}(0,1)\\) r.v.\n\\[\\begin{equation}\n    Z = \\begin{cases}\n    X, p = 0.5 \\\\\n    Y, p = 0.5\n    \\end{cases}\n\\end{equation}\\]\nHe then challenges the readers to come up with the expected value \\(\\mathbb{E}[Z^2]\\) and asks on what is it defined? It is indeed a hard question.\nIt is not surprising for me that measure theory becomes important in the Learning Theory, even though lighter courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr). According to Mikio’s Brown answer it’s essential in the idea of uniform convergence and its bounds, where “you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”.\nIf we’re thinking about a regression from the nonparametric perspective \\(f(x) \\in \\mathscr{C}^2:X \\rightarrow \\mathbb{R}\\), we might want to know how a draw from a (infinite) set of continuous differentiable functions might look like. The questions arises: how to define a PDF in this space? In my thesis (Mihai 2017) I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.\n\n\n An example of reasoning about distributions of random functions from my Thesis. The prior distribution\n\n Only the functions that explain the data well survive\n\n\\[\\begin{equation}\np(f(x) \\, |\\left \\{ x\\right \\})=\\frac{p(\\left \\{ x\\right \\}| \\, f) \\, \\mathbf{p(f)}}{p(\\left \\{ x\\right \\})}\n\\end{equation}\\]\n\\[\\begin{equation*}\nf(x) \\sim GP(\\mu(x); K(x,x'))\n\\end{equation*}\\]\nThe result was that only the functions that explained the data well survived. While this reasoning makes intuitive sense, there are things “swiped under the carpet”. If we were to model stock prices, where there are jumps and the process itself is less smooth, measure theory would be very hard to avoid. If we would want to reason in terms of densities, ask with respect to what? So, the focus is shifted towards the question of what is the probability of every possible event. This leads us back to the fundamental object of state (outcome) space \\(\\mathbf{\\Omega}\\).\n\nEach element \\(\\omega_i \\in \\mathbf{\\Omega}\\) is an elementary event (outcome), while \\(A \\subset \\mathbf{\\Omega}\\) is an event\n\nAs we will shortly see, it’s impossible to define the probability (measure) on the set of all subsets \\(2^\\Omega\\), except for the simple finite cases, without having to let go of a fundamental axiom like countable additivity. This will be the first step in getting closer to defining a Uniform distribution on \\([0, 1]\\).\n\nSurprisingly, to rigorously define an Uniform distribution is not a trivial task, because of the mentioned above impossibility, proved later by contradiction. In contrast, Caratheodori theorem allows us to do exactly that\n\nTarun Chitra in the same thread argues that many classification problems are ill-posed mathematically, and the ones which can be formulated in a measure-theoretic way have very nice results, like SVMs with Reproducing Kernel Hilbert Spaces, where you cannot apply the Mercer’s Theorem unless the Kernel is measurable. The second example he gives is proving some results about Stochastic Gradient Descent, an optimization algorithm often successfully used, which has connection with Brownian Motion, thus Weiner measures.\nMeasure theory is also important in rigorously defining distances and divergences (for example between two distributions as in Kullback-Leiber)"
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#measure-theory-and-the-fundamentals-of-probability-theory",
    "href": "posts/2018-intro-measure/measure_theory.html#measure-theory-and-the-fundamentals-of-probability-theory",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Measure Theory and the fundamentals of Probability Theory",
    "text": "Measure Theory and the fundamentals of Probability Theory\nIt is useful to step back and see where does Measure Theory fit in the framework of Probability Theory. The following list will be a summary of a lecture at doctoral school by (Ruxanda 2017) Here are 8 steps to mastery of the basics by Gheorghe Ruxanda:\n\nA random experiment (\\(\\mathscr{E}\\)) is a set of conditions which are favorable for an event in a given form with the following properties:\n\nPossible results are known apriori\nIt’s never known which of the results of \\(\\mathscr{E}\\) will exactly appear\nDespite (b), there is a perceptible regularity, (encoding the idea of a probabilistic “law”) in the results. Also, it could be as a result of the large scale of the phenomena.\nRepeatability of the conditions, i.e. the comparability and perservation of context are key.\n\nElementary event as an auxiliaty construction: one of the possible results of \\(\\mathscr{E}\\), \\(\\omega_i \\in \\Omega\\)\nUniversal set \\(\\Omega = \\{ \\omega_1, \\omega_2, \\dots \\}\\) Also called (Outcome/ State/ Selection space), it suggests the idea of complementarity and stochasticity: we don’t know which \\(\\omega_i\\), is a key object for a further formalization of probability measures.\nWe care not only about an event \\(A = \\bigcup\\limits_{i = 1}^n \\omega_i\\) and its realization, but also about other events in the Universal Set, because they might add information about the probability of occurring of our event of interest\nThe event space \\(\\mathcal{F}\\) should be defined on sets of subsets of \\(\\Omega\\) and this is where measure theory shines. We’ll discuss later in extensive detail the following conditions on the way to defining sigma-algebras. As can be seen later, we usually can’t define a probability measure on all sets of subsets.\nProbability as an extension of the measure: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability to elementary events: \\(\\mathbb{P}(\\omega_i)\\). This is where additivity properties are key. A long discussion on Frequentist vs Bayesian interpretation of it can follow from here.\nA probability triple \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\)\nThe idea of Random Variable\n\nBefore moving on to probability measures, it’s useful to think about what a random variable really is and does, because formally, it’s neither a variable, nor random. That should be another motivation for speaking the language of measure theory.\n\n\n\nIdea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable\n\n\nThe idea of the random variable as being a quantificator of elementary events (function defined on the outcome space which maps the elementary events to the real line in 1d) that perserves the informational structure of the sample space is very powerful, is formally defined and related to the idea of measurability.\n\nStart from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantificable characteristics of the objects.\n\n\\[\\begin{align}\nX(\\omega):\\Omega \\rightarrow \\mathbb{R} \\\\\ns.t. ~~ \\{\\omega \\in \\Omega | X(\\omega) \\leq r, \\forall r \\in \\mathbb{R} \\} \\in \\mathcal{F}\n\\end{align}\\]\nThe idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest. The idea is that the preimage defined above \\(X^{-1}((-\\infty,r]) = E \\in \\mathcal{F}\\) on the following interval corresponds to an event E which should be in the event space \\(\\mathcal{F}\\). Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is \\(F_X(X \\le r)\\)."
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#cant-have-it-all-the-trouble-with-the-uniform",
    "href": "posts/2018-intro-measure/measure_theory.html#cant-have-it-all-the-trouble-with-the-uniform",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Can’t have it all: The trouble with the Uniform",
    "text": "Can’t have it all: The trouble with the Uniform\nFollowing the previous discussions we would want to define a probability measure \\(\\mathbb{P} :2^\\Omega \\rightarrow [0, 1]\\) on the set of all subsets of \\(\\Omega = [0, 1]\\) for the uniform distribution. Unfortunately we can’t have that and perserve essential properties of probability measures.\n\nAll other properties can be easily derived from these, thus these are minimal requirements for a probability measure. Nonetheless, these conditions are too restrictive if we want to define an Uniform Distribution on \\(2^\\Omega\\).\n\n\n\\(\\mathbb{P}(\\Omega) = 1\\) and \\(\\mathbb{P}(\\varnothing) = 0\\)\n\\(\\mathbb{P}(A) \\in [0, 1]\\)\nIf \\(A \\cap B = \\varnothing \\implies  \\mathbb{P}( A \\cap B) =  \\mathbb{P}(A) + \\mathbb{P}(B)\\)\nIf \\(\\{ A_i \\}_{i=1}^\\infty\\) s.t. \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing \\implies \\mathbb{P} ( \\bigcup\\limits_{i = 1}^\\infty A_i) = \\sum\\limits_{i = 1}^{\\infty}\\mathbb{P}(A_i)\\)\n\nThe idea of uniform distribution is closely related to the one of length, area, volume, depending on what space are we into. That means the probability measure will look like this:\n\\[\\begin{equation}\n    \\mathbb{P}([a, b]) = b - a\n\\end{equation}\\]\nwhere \\(\\mathbb{P} :2^\\Omega \\rightarrow [0, 1]\\) and \\(0 \\le a \\le b \\le 1\\)\nThe proof is done by contradiction, but the implications are a little bit deeper, related to paradoxes like Banakh-Tarsky and Vitali Sets, which are counter-intuitive but closely related to the idea of something being unmeasurable. Because we can’t get rid of any of the axioms, we should deal with the fact that we can’t define the measure on \\(2^\\Omega\\).\nInstead define a set of subsets \\(\\mathcal{A} \\subset 2^\\Omega\\) such that \\(\\mathbb{P}: \\mathcal{A} \\rightarrow [0,1]\\). These sets will have to obey certain properties and this is where all the terminology from measure theory comes in with algebras, semi-algebras and sigma-algebras. Each of these concepts and objects will be stepping stones towards understanding Caratheodori"
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#background-concepts-towards-caratheodori",
    "href": "posts/2018-intro-measure/measure_theory.html#background-concepts-towards-caratheodori",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Background concepts towards Caratheodori",
    "text": "Background concepts towards Caratheodori\nLet’s continue on this upbeat note and try to figure out what kind of sets \\(\\mathcal{A}\\) are measurable, for which we can define their probabilities.\nDef: Algebra and Semi Algebra: A set of subsets \\(\\mathcal{A} \\subset 2^\\Omega\\) is an algebra (field) if the following holds:\n\n\\(\\Omega \\in \\mathcal{A}\\) and \\(\\varnothing \\in \\mathcal{A}\\)\nIf \\(A \\in \\mathcal{A}\\) then \\(A^C \\in \\mathcal{A}\\) (closed under complements)\nIf \\(A, B \\in \\mathcal{A}\\) then $ A B $ (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection\nFor sigma-algebra: sigma refers to countability} If \\(\\{  A_i \\}_{i \\ge 1} \\in \\mathcal{A}\\) then \\(\\bigcup\\limits_{i \\ge 1} A_i \\in \\mathcal{A}\\) (closed under countable union)\n\nOn an intuitive note, we define the probability measure on sigma-algebras because if certain conditions did not hold, the measure wouldn’t make sense.\nDef: Probability Measure: Suppose we have defined a measurable space \\((\\Omega, \\mathcal{A})\\), where \\(\\mathcal{A}\\) is a sigma-algebra. A probability measure is the function \\(\\mathbb{P}:\\mathcal{A} \\rightarrow [0, 1]\\) such that:\n\n\\(\\mathbb{P}(\\Omega) = 1\\) \n\\(\\forall \\{ A_i \\}_{i \\ge 1}\\) where \\(A_i \\bigcap\\limits_{i \\ne j} A_j = \\varnothing\\) (countable sequences of mutually disjoint effects), \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\)\n\nAs stated earlier, for more difficult cases, when it’s hard or impossible to reason in terms of probability density functions, it is more convenient to talk about measures. For the previous cases of point masses \\(\\delta_k(x)\\) and continuous functions we can ask the question what is the probability of a certain outcome directly if using measure-theoretic formalism. \\(\\Omega = \\mathbb{R}\\) and \\(\\mathcal{A} = 2^\\Omega\\) and the point mass looks basically like a spike at \\(k^{th}\\) place in the real line.\n\\[\\begin{equation}\n    \\mathbb{P}(A) = \\begin{cases}\n    1, ~~ k \\in A\\\\\n    0, ~~ k \\notin A\n    \\end{cases}\n\\end{equation}\\]\nIn order to define the probability measure for the continuous measure, much deeper results should be invoked. &gt; The Borel Spaces in itself encode a chain of new concepts that need to be understood from Banakh Spaces, Normed Spaces and how to close them under complement and union.\n\n\\(\\Omega = \\mathscr{C}([0,1];\\mathbb{R})\\)\n\\(\\mathcal{A} = \\mathcal{B}(\\mathscr{C}([0,1];\\mathbb{R}))\\)\n\nThis might be one of the reasons why Stochastic Processes is such a difficult and powerful field, because of the amount of knowledge encoded even in the “simplest” Brownian Motion (where \\(\\mathbb{P}\\) is a Weiner measure).\nGoing back to our pursuit of Caratheodori theorem, it is useful to understand why do we need countable additivity. If the finite additivity is clear, for example in the case of disjoint segments of the uniform distribution \\(X \\sim Unif([0,1])\\), \\([a_1, b_1]\\) and \\([a_2, b_2]\\), it’s essential that the following holds.\n\\[\n\\mathbb{P}([a_1, b_1] \\cup [a_2, b_2] ) = \\mathbb{P}(a_1 \\le X \\le b_1) + \\mathbb{P}(a_2 \\le X \\le b_2)\n\\]\nCountable additivity \\(\\mathbb{P}(\\bigcup\\limits_{i \\ge 1} A_i) = \\sum\\limits_{i \\ge 1} \\mathbb{P}(A_i)\\) is useful to prove that limits exists, which is very important in various statistical procedures. We can’t say anything about uncountable additivity because the measure of each element on the r.h.s. will be zero, while the measure of the interval is one, which is a contradiction\nTo get our feet wet, let’s see what techniques are employed by various authors in order to prove the impossibility of constructing a measure which has the idea of length while keeping the axioms.\nProposition: There does not exist a (probability) measure \\(\\lambda(A)\\) (Note that we’ll switch conventions to the measure-theoretic one employed by the Claudio Landim’s course as it’s one of the very few available online and it should be easier to follow in parallel with this reading) defined for all subsets of \\(A \\subseteq [0, 1]\\) satisfying.\n\n\\(\\lambda: \\mathscr{P}([0,1]) \\rightarrow [0,1]\\) which could be all rational numbers, for example\n\\(\\lambda([a, b])  = b - a\\) as an extension of the idea of length\n\\(\\forall A \\subseteq [0,1]\\) and \\(\\forall x \\in [0,1]\\) translation invariance \\(\\lambda(A + x)  = \\lambda(A)\\). Alternatively stated, \\(A + x = \\{x + y ~|~ y \\in A \\}\\).\nIf \\(A = \\bigcup\\limits_{j \\ge 1} A_j\\) is an union of mutually disjoint sets \\(A_i \\cap A_j = \\varnothing\\) then \\(\\lambda(A) = \\sum\\limits_{j \\ge 1} \\lambda(A_j)\\) This is exactly the notion of sigma-additivity encontered over and over again.\n\n\nWe can use \\([0, 1]\\) as in Rosenthal without loss of generality. Landim uses \\(\\mathbb{R}_+\\cup\\{ +\\infty \\}\\). Note that \\(\\mathscr{P}(\\cdot)\\) is the power set}\n\nProof: Assume that \\(\\exists\\) a measure \\(\\lambda\\) such that above conditions hold. First, we need to introduce the notion of equivalence relation (in order to say “x is related to y”: \\(x \\sim y\\)), which is key to proving this. The point is that the equivalence relation will partition a set, which allows us by invoking the Axiom of Choice to get towards the desired contradiction.\n\nRelation set: S is a boolean function with \\(x, y \\in S\\) \\[R:S \\times S \\rightarrow \\{0, 1\\}\\] Thus \\(x \\sim y\\) means x is related to y.\n\nGiven an equivalence relation ~ and \\(x \\in S\\) the equivalence class of x is \\(\\{ y \\in S \\lvert y \\sim x  \\}\\). If x is an equivalence class then any pair of equivalence classes is either identical or disjoint. So, the relation forms equivalence classes, which form a partition on S.\nDef: A relation is an equivalence relation if\n\nreflexive: \\(x \\sim x\\) \\(~~ \\forall x \\in S\\)\nsymmetric: \\(x \\sim y \\implies y \\sim x\\) \\(~~ \\forall x, y \\in S\\)\ntranzitive: \\(x \\sim y\\) and \\(y \\sim z  \\implies x \\sim z\\) \\(~~ \\forall x, y, z \\in S\\)\n\nBoth Rosenthal and Landim use a special equivalence class involving a relation \\(x \\sim y\\), \\(x, y \\in \\mathbb{R}\\) for rational numbers: \\(y - x \\in \\mathbb{Q}\\). The equivalence class for x becomes\n\\[\\begin{align}\n    [x] = \\{ y \\in \\mathbb{R} | y - x \\in \\mathbb{Q} \\} \\\\\n     \\Lambda = \\{ \\alpha, \\beta \\dots \\} = \\mathbb{R} \\lvert \\sim\n\\end{align}\\]\n\\(\\Lambda\\) is another important object (the set of equivalence classes) which is R modulo the equivalence relation, clearly uncountable, because \\([x]\\) are countable. Now, using the Axiom of Choice a new set \\(\\Omega\\) is constructed in the following way: for each equivalence class \\(\\alpha, \\beta\\) one and only one element is selected.\n\nThere are deep philosophical discussions regarding it, but it’s outside the scope of current project. Basically what we need to know is that it allows us to “simultaneously” choose from \\(\\Lambda_j\\)\n\n\n\n\nA graphical representation assuming the Axiom of Choice.\n\n\n\\([x]\\) can be chosen in such a way that \\(\\Omega \\in [0, 1]\\)\nWe reached a little milestone, as now there is a handle and structure to the problem, in contrast it wasn’t clear where to start from in the beginning. Here, Rosenthal is very brief, finishes the proof quickly and it seems that Landim chooses a much longer, but much more explicit way. A link to the alternative proof. I’ll choose the first one, because there are not many concepts used later for us to benefit by going through it.\n\\[\\begin{equation}\n    \\Omega = \\{ \\omega_1 \\in \\alpha , \\omega_2 \\in \\beta, \\dots  \\} \\in [0, 1]\n\\end{equation}\\]\nA key claim is that if we translate \\(\\Omega\\) by \\(p, q \\in \\mathbb{Q}\\), the following dichotomy is true: either the sets are equal or disjoint (which was mentioned at the beginning of the proof). \n\\[\\begin{equation}\n\\begin{cases}\n    \\Omega + q = \\Omega + p   \\\\\n    (\\Omega + q) \\cap (\\Omega + p) = \\varnothing\n    \\end{cases}\n\\end{equation}\\]\nSince \\(\\Omega\\) contains an element from each equivalence class, each point in \\((0, 1] \\subseteq \\bigcup\\limits_{r \\in \\mathbb{Q}} (\\Omega + r)\\) (is contained in the union of rational shifts of \\(\\Omega\\)).\nSince \\(\\Omega\\) has only one element \\(\\forall [x] \\implies \\Omega + r, \\forall r \\in [0, 1]\\) are disjoint. Thus for \\(r \\in [0, 1]\\),\n\\[\\begin{align}\n    \\lambda([0, 1]) &= \\sum\\limits_{r \\in \\mathbb{Q}} \\lambda(\\Omega + r) \\\\\n    &= \\sum\\limits_{r \\in \\mathbb{Q}} \\lambda(\\Omega)\n\\end{align}\\]\nNotice the rhs is countably infinite sum, so it can be either zero or \\(+\\infty\\) or \\(-\\infty\\), but lhs is one. So, we arrive at the contradiction. For the last steps, some more understanding is needed. This is why the second proof is great and even though longer, very explicit.\nSo, what’s the trouble with the Uniform? Nothing particular, it’s just that not all subsets are measurable (have an associated measure). This is why we need concepts like semi-algebra, algebra and sigma-algebra in order to reason about what subsets of the power set are measurable."
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#plan-of-attack",
    "href": "posts/2018-intro-measure/measure_theory.html#plan-of-attack",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nThe idea is to extend the following measure \\(\\lambda([a, b]) = b - a\\) (of the length) to increasingly more strict scope with respect to \\(\\mathscr{P}(\\mathbb{R})\\), while keeping the desired properties. I jumped a little bit ahead of myself defining the sigma-algebra, as there are more prerequisites and intermediary steps needed. Defining the following objects (classes of subsets) will help demistify a lot of terminology used\n\nsemi-algebra\nalgebra\nsigma-algebra\n\nWe’ll construct an increasing set of more restrictive conditions. Basically, semi-algebra is weaker than algebra which is weaker than sigma-algebra. These classes of subsets have certain properties and subtle relationships between them and without understanding these, it is very hard to move on to extend the measure.\nDef: Semi-Algebra is a class of subsets \\(\\mathcal{S} \\subseteq \\mathscr{P}(\\Omega)\\) if the following holds\n\n\\(\\Omega \\in \\mathcal{S}\\) and \\(\\varnothing \\in \\mathcal{S}\\)\n\\(A, B \\in \\mathcal{S} \\implies A \\cap B \\in \\mathcal{S}\\) (closed by finite intersections)\n\\(\\forall A \\in \\mathcal{S}\\), \\(\\exists \\{ E_1 , \\dots, E_n \\} \\implies A^C = \\sum\\limits_{j = 1}^n E_j\\) Complement viewed as a finite union of elements of \\(\\mathcal{S}\\)\n\nImagine a segment \\([a, b]\\) on \\([0, 1]\\) and take its complement, it is obvious that it can be represented as a finite union of sets of \\(\\mathcal{S}\\). Actually, these simple examples inspired the definition of semi algebra, but as we can see the conditions are weaker than the ones for algebra. The algebra and sigma-algebra were defined above, but it doesn’t hurt to inspect the relationship between the two in more detail.\nDef: Algebra is a class of subsets \\(\\mathcal{A} \\subseteq \\mathscr{P}(\\Omega)\\) if the following holds\n\n\\(\\Omega \\in \\mathcal{A}\\) and \\(\\varnothing \\in \\mathcal{A}\\)\n\\(A, B \\in \\mathcal{A} \\implies A \\cap B \\in \\mathcal{A}\\) (closed by finite intersections)\n\\(A \\in \\mathcal{A} \\implies A^C \\in \\mathcal{A}\\) (closed under complements, a stronger condition)\n\nRemark: If \\(\\mathcal{A}\\) is a sigma-algebra then \\(\\mathcal{A}\\) is also a semi-algebra. This is important because we want to make statements about algebras generated by semi-algebras which have very nice properties.\nRemark: Let \\(\\mathcal{A}_i \\subseteq \\mathscr{P}(\\Omega)\\) be an algebra of subsets of \\(\\Omega\\) where \\(i \\in I\\) could be any index. Then \\(\\bigcap\\limits_{i \\in I} \\mathcal{A}_i = \\mathcal{A}\\) is also an algebra. This is verified by the definition of algebra.\nDef: Sigma-Algebra is a class of subsets \\(\\mathcal{F} \\in \\mathscr{P}(\\Omega)\\) if the following holds\n\n\\(\\Omega \\in \\mathcal{F}\\) and \\(\\varnothing \\in \\mathcal{F}\\)\n\\(A_j \\in \\mathcal{F}\\) \\(\\implies \\bigcap\\limits_{j \\ge 1} A_j \\in \\mathcal{F}\\) (closed by countable intersections)\n\\(A \\in \\mathcal{F} \\implies A^C \\in \\mathcal{F}\\) (closed under complements)\n\nRemark: Let \\(\\mathcal{F}_i \\in \\mathscr{P}(\\Omega)\\) be a sigma-algebra of subsets of \\(\\Omega\\) where \\(i \\in I\\) could be any index. Then \\(\\bigcap\\limits_{i \\in I} \\mathcal{F}_i = \\mathcal{F}\\) is also an algebra. Same as in case of algebra\n\n\n\nThe plan of attack. A sigma-additive measure defined on semi-algebra is extended to the algebra generated by the semi-algebra. The latter in turn is extended by Caratheodori theorem to the sigma-additive measure defined on sigma-algebra generated by the semi-algebra\n\n\nAlso needs to be shown that these extensions are unique at each step. Let’s take a break in order not to get lost in detail and terminology: What are we doing here? It was proven that we cannot define a measure extending the idea of length on \\(\\mathscr{P}(\\Omega)\\) under standard axioms of probability.\n\\(\\Omega = [0, 1], \\mathbb{P}([a, b]) = b - a\\), but what is \\(\\mathcal{F}\\)? A sigma-algebra, but it’s not obvious at all why.\n\\[\\begin{equation}\n    Unif([0, 1]) \\longrightarrow (\\Omega, \\mathcal{F}, \\mathbb{P})\n\\end{equation}\\]\nThat gave a motivation of defining measures on algebras with conditions of ever increasing strength/restriction. Relationships between these types of objects and their properties are key in making progress, which is quite clear from the diagram above. Even though these steps might look similar, the techniques and concepts employed are quite different. Nonetheless, the question always stays conceptually the same: On what can we define a (probability) measure? In essence, the whole language is developed in order to reason and make meaningful statements about sets of subsets and whether it can be “measured”.\n\nAlso notice that we haven’t still reached even the start of what most measure-theoretic probability courses begin from. This is good, because we’re spending time on understanding the fundamentals on which all that theory is built.\n\nThe next element we need is to define relationships between fields such as algebras generated by semi-algebras and so on. First, C. Landim introduces an algebra \\(\\mathcal{A}(\\mathscr{C})\\) generated by a class of sets \\(\\mathscr{C} \\subseteq \\mathscr{P}(\\Omega)\\) such that the following holds\n\n\\(\\mathscr{C} \\subseteq \\mathcal{A}\\), where \\(\\mathcal{A}\\) is the smallest algebra that contains \\(\\mathscr{C}\\)\nIf \\(\\mathscr{B} \\supseteq \\mathscr{C}\\) is a sigma-algebra \\(\\implies \\mathscr{B} \\supseteq \\mathcal{A}\\)\n\nRemarkably, the same is true for sigma-algebras, proven by a chain of thought involving all sets \\(\\mathcal{A}_\\alpha\\) which contain \\(\\mathscr{C}\\). Their intersection \\(\\bigcap\\limits_\\alpha \\mathcal{A}_\\alpha  = \\mathcal{A}\\) contains \\(\\mathscr{B}\\) and since it contains $ $, it belongs to that intersection, hence, it’s the smallest sigma-algebra that contains \\(\\mathscr{C}\\).\nIf the underlying class \\(\\mathscr{C}\\) is a semi-algebra, then \\(\\mathcal{A}(\\mathscr{C})\\) has an explicit form as a finite union of elements of semi-algebra. In the case of sigma-algebras, other techniques and arguments are needed because we don’t have such a form, which will be a major blocker.\n\nProve and formalize the last statement about sigma-algebras generated by semi-algebras\nExplore additivity in measure functions\nExplore sigma-additivity in measure functions\nUnderstand Continuity from Above and Below and its connections with additivity\nExtend the measure defined by semi-algebra on sigma-algebra generated by semi-algebra. Prove uniqueness\n\nTheorem: (Caratheodori) Let \\(\\mathcal{F}(\\Omega)\\) be a semi-algebra of subsets of \\(\\Omega\\) and \\(\\pi: \\mathcal{F} \\rightarrow [0, 1]\\) with \\(\\pi(\\varnothing) = 0\\) and \\(\\pi(\\Omega) = 1\\) satisfying the superadditivity property:\n\n\\(\\pi(\\bigcup\\limits_{i = 1}^k A_i) \\ge \\sum\\limits_{i = 1}^k \\pi(A_i)\\) where \\(A_i \\in \\mathcal{F}\\) and \\(\\bigcup\\limits_{i = 1}^k A_i \\in \\mathcal{F}\\) disjoint.\n\\(\\pi(A) \\le \\sum\\limits_n \\pi(A_n)\\) where \\(A_i \\in \\mathcal{F}\\) and \\(A \\in \\bigcup\\limits_n A_n\\)\nThen \\(\\exists\\) a sigma-algebra \\(\\mathcal{M} \\supseteq \\mathcal{F}\\) and a countably additive probability measure \\(\\pi^*:\\mathcal{M}\\rightarrow [0, 1]\\) such that \\(\\pi^*(A) = \\pi(A) ~~ \\forall A \\in \\mathcal{F} \\implies (\\Omega, \\mathcal{F},  \\pi^*)\\) is a valid probability triple, which agrees with previous probabilities on \\(\\mathcal{F}\\) (Rhosental 2006)"
  },
  {
    "objectID": "posts/2018-intro-measure/measure_theory.html#proof-left-as-an-exercise-just-kidding",
    "href": "posts/2018-intro-measure/measure_theory.html#proof-left-as-an-exercise-just-kidding",
    "title": "Understanding Caratheodori Extension Theorem",
    "section": "Proof Left as an Exercise [Just Kidding]",
    "text": "Proof Left as an Exercise [Just Kidding]\nWe’re middle way through and already discovered a lot of insights, but there are more things to be done:\n\nDefine and understand the concept of outer measure\nProve that \\(\\mathcal{M}\\) is a sigma-algebra\nConstruct the extension on the new restriction\nLearn about monotone classes\nProve the extension is unique via monotone classes\nLook into “Extension of Caratheodori Extension Theorem”"
  },
  {
    "objectID": "posts/2017-probability/stochastic.html",
    "href": "posts/2017-probability/stochastic.html",
    "title": "Making Friends with Probability",
    "section": "",
    "text": "Thinking deeply about uncertainty and probability theory changed my worldview and eventually became an essential part of my job. In 2023, I’m looking back at the game-changing encounters which instilled a passion for probability, followed by some paragraphs from the bachelor’s thesis1.\n1 Bizovi Mihai - Stochastic Modeling and Bayesian Inference (2017)\nIn 2013, it was Richard Feynman’s “The pleasure of finding things out”, which put the question of uncertainty somewhere on the back of my mind\nIn 2014, Taleb Nassim’s “Fooled by randomness” and “Black Swan” deeply resonated with me. There were many instances when I asked myself: “What would the academic do?”, “What would Taleb and Fat Tony do?”, “What would Hume and Popper say about that?”\nIn 2015, I was obsessed about systems’ dynamics and nonlinear differential equations for modeling the stocks and flows in an economy. The big question arose: how to actually represent the uncertainty and do inference for those parameters? It was also when the “Visual guide to Bayesian thinking” by Julia Galef popped up on my youtube recommendation feed.\nIn 2016, it was clear that the best job I could get was in ML and Data Science. Probabilistic ML and Probabilistic Graphical Models were the thing which I wanted to master, after Zoubin Ghahramani’s brilliant lectures in the Tubingen MLSS.\n\nA bit earlier, we had our probability and mathematical statistics class – which I skipped to study after Joe Blitzstein’s lectures and book. Brilliant story proofs, relatable examples!\n\nIn 2017, after the brilliant lectures in Multidimensional Data Analysis by Gheorghe Ruxanda at our university, I realized I know nothing about probability. There was so much more depth, nuance, philosophy, and history to it: Kolmogorov’s breakthrough2, measure-theoretic foundations and those stochastic processes.\n\n2 \n\n\nAndrey Kolmogorov\n\n\nIn the thesis, I was interested in probabilistic approaches to modeling complex, dynamic economic phenomena. What stayed the same all these years, is a preference for the development of custom models for specific applications, where it is possible to explicitly declare domain and statistical assumptions, over an out-of-the-box Machine Learning solution.\nSince then, after years of experience and practice, you can find an evolving presentation of the subject in the course website. It is a work-in-progress, with the purpose of reminding students why did they study probability and what really matters."
  },
  {
    "objectID": "posts/2017-probability/stochastic.html#foreword-from-2023",
    "href": "posts/2017-probability/stochastic.html#foreword-from-2023",
    "title": "Making Friends with Probability",
    "section": "",
    "text": "Thinking deeply about uncertainty and probability theory changed my worldview and eventually became an essential part of my job. In 2023, I’m looking back at the game-changing encounters which instilled a passion for probability, followed by some paragraphs from the bachelor’s thesis1.\n1 Bizovi Mihai - Stochastic Modeling and Bayesian Inference (2017)\nIn 2013, it was Richard Feynman’s “The pleasure of finding things out”, which put the question of uncertainty somewhere on the back of my mind\nIn 2014, Taleb Nassim’s “Fooled by randomness” and “Black Swan” deeply resonated with me. There were many instances when I asked myself: “What would the academic do?”, “What would Taleb and Fat Tony do?”, “What would Hume and Popper say about that?”\nIn 2015, I was obsessed about systems’ dynamics and nonlinear differential equations for modeling the stocks and flows in an economy. The big question arose: how to actually represent the uncertainty and do inference for those parameters? It was also when the “Visual guide to Bayesian thinking” by Julia Galef popped up on my youtube recommendation feed.\nIn 2016, it was clear that the best job I could get was in ML and Data Science. Probabilistic ML and Probabilistic Graphical Models were the thing which I wanted to master, after Zoubin Ghahramani’s brilliant lectures in the Tubingen MLSS.\n\nA bit earlier, we had our probability and mathematical statistics class – which I skipped to study after Joe Blitzstein’s lectures and book. Brilliant story proofs, relatable examples!\n\nIn 2017, after the brilliant lectures in Multidimensional Data Analysis by Gheorghe Ruxanda at our university, I realized I know nothing about probability. There was so much more depth, nuance, philosophy, and history to it: Kolmogorov’s breakthrough2, measure-theoretic foundations and those stochastic processes.\n\n2 \n\n\nAndrey Kolmogorov\n\n\nIn the thesis, I was interested in probabilistic approaches to modeling complex, dynamic economic phenomena. What stayed the same all these years, is a preference for the development of custom models for specific applications, where it is possible to explicitly declare domain and statistical assumptions, over an out-of-the-box Machine Learning solution.\nSince then, after years of experience and practice, you can find an evolving presentation of the subject in the course website. It is a work-in-progress, with the purpose of reminding students why did they study probability and what really matters."
  },
  {
    "objectID": "posts/2017-probability/stochastic.html#thoughts-from-the-thesis",
    "href": "posts/2017-probability/stochastic.html#thoughts-from-the-thesis",
    "title": "Making Friends with Probability",
    "section": "Thoughts from the thesis",
    "text": "Thoughts from the thesis\n\n”I can live with doubt and uncertainty and not knowing. We absolutely must leave room for doubt or there is no progress and there is no learning. There is no learning without having to pose a question. And a question requires doubt. People search for certainty, but there is no certainty.” — Richard Feynman, The pleasure of finding things out\n\n\nThe need for stochastic modeling and Bayesian thinking\nCurrent challenges in economics need an interdisciplinary, multidimensional approach, and different perspectives. In order to have more powerful models in our toolbox, we’ll investigate statistical modeling based on three schools of thought: Stochastic Modeling3, Machine Learning, and Bayesian Analysis – which has applications ranging from economics and finance to genetics, linguistics and artificial intelligence.\n3 Stochastic, meaning probabilistic, specifically in the Fisherian and Neyman-Pearson frameworks.This approach is extremely general and allows us to make inferences about latent quantities and relationships, to identify what is hidden beneath appearance, to account for the uncertainty and nonlinearities of economic phenomena. The idea of learning from data, flexibility of representing uncertainty and the parsimony principle leads to adaptive and robust modeling, able to capture non-trivial aspects of the problem. The focus will be very much on models and a deep understanding of concepts on which they’re built.\nThe ugly truth is that humans are not good at prediction, neither consistent with probability theory. We are “suffering” from multiple evolutionary mechanisms, cognitive biases and even statisticians, formally trained in the language of uncertainty, aren’t guarded against pitfalls that arise in practice and everyday life.\nThe same story is in reconciliation of new evidence with prior beliefs, processing multidimensional data, ignorance of feedback loops and dynamics. The reason we are building models is to better understand the world, phenomena around us and in the end to improve our mental representations4. Predictions that are less wrong usually take into account a variety of perspectives, a proven fact in ensemble modeling.\n4 The only thing which I would add here in 2023, is an appreciation for Action and Decisions. Back then, knowledge was foregrounded for me.\n\nThree perspectives\nMachine learning models have become an essential part of online services and is taking over new fields, bringing firms and people lots of value. The multidimensional approach is what makes it all this possible. In contrast with classical statistics and NHST (Null Hypothesis Significance Testing)5, our focus will be on generalization performance and the idea of a model. Even though these models have their roots in statistics, the field can be viewed in its own right.\n5 In 2023, I wouldn’t equivocate between NHST and the Neyman-Pearson, action-oriented frequentism, which can be done wellAn important extension for which there is an acute need is dynamical models. A lot of interesting problems are multidimensional time series, which might invalidate the hypotheses behind classical data mining models. Coupled with the need of accounting for uncertainty, this brings us to the idea of introducing stochastic elements and to extend the idea of learning and generalization.\nThe third perspective which brings together stochastic elements with machine learning is bayesian inference, as a method to update beliefs and knowledge based on data from different sources. Bayesian modeling enables us to build hierarchical models with latent structures, while encoding the uncertainty in parameters and operating with probability distributions. On the shoulders of these three “giants”, a stochastic perspective over Machine Learning emerges over the last two decades, which is flexible, general and more transparent than neural networks, works on “thin”6 data.\n6 Perhaps I meant small samples at group level, which would make sense in the context of multilevel models.There are several reasons the field is not yet in the mainstream: scalability and complexity of the models, in the sense of necessary knowledge and skill to start modeling. Once the software implementations, approximation methods and probabilistic programming languages will become better, the Bayesian methods will see another rise in popularity.\nThe next chapters will be dedicated to each of these fundamental perspectives, in which we’ll try to get to the essence. The last ones will be on supervised and unsupervised probabilistic models such as Gaussian Processes and Mixture Models. An essential concept will be the ARD (Automatic Relevance Determination) and Kernels which encourage sparse solutions.\n\n\nThe goal\nThe motivation behind the choice of probabilistic machine learning models is that there are a lot of opportunities to extend them and reuse as parts of a more complicated model.\nFor example, Gaussian Processes7 can be transformed into a Relevance Vector Machine (the probabilistic version of SVM) or used for Bayesian Optimization of expensive functions, in learning of complex dynamical phenomena, spatio-temporal modeling (also known as Kreiging). The Probabilistic Principal Component Analysis can be used for a latent representation of a multidimensional time series and intrinsic dimensionality determination. On the over hand, Mixture Models are a powerful model-based clustering technique, especially coupled with nonparametric techniques.\n7 This is still true, although fallen a bit out-of-fashion. However, the Bayesian Additive Regression Trees have risen to the challenge, especially in the field of Causal Inference.This approach is extremely powerful, intellectually fascinating and vibrant. It is also in the spirit of economic complexity, with ideas which bridge together different branches in data analysis. The catch is, that there are enormous barriers for beginners to start developing such models. First, a qualitative jump in understanding of probability theory and stochastic processes, Bayesian inference and theory of statistical learning is needed. Another barrier is how to go from understanding the models towards the implementation in actual software products.\nSo, the main goal is to find connections between the three perspectives described before, to recognize Bayesian generalizations of classical machine learning models. The most important is the process of exploration and understanding, internalization of models, which will suggest new areas of study and practical opportunities."
  },
  {
    "objectID": "posts/2019-networks/networks.html",
    "href": "posts/2019-networks/networks.html",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "",
    "text": "The goal of this project is to get some hands-on experience in working with Graphs and Network Data based on a dataset from my personal life. It will involve some theoretical understanding and programming.\nGraph Theory is a fascinating subject which I studied before, but as I found out – the understanding remains, but the skill perishes if left untrained or unused. Besides getting comfortable with this type of data, there is an opportunity to build the ground for future research1. Moreover, ideas from graph theory can be useful in the field of Machine Learning, with the emergence of Graph Neural Networks."
  },
  {
    "objectID": "posts/2019-networks/networks.html#crawling-the-data",
    "href": "posts/2019-networks/networks.html#crawling-the-data",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "Crawling the data",
    "text": "Crawling the data\nIn search of a suitable dataset for statistical analysis of social networks, I stumbled upon many interesting applications, from the classical Karate Club to Email, Citations of Scientific Articles, Transportation and Biology. Unfortunately, there is a tradeoff between how manageable it is to work with the data in terms of volume and how interesting are the patterns to be discovered. The most promising way seemed to explore the scientific literature or knowledge graphs like DBPedia, but it’s quite a difficult and heavy task.\n\n\n\nFacebook Social Network extracted though lostcircles. The shadowed area over clusters represent different communities like the place I work at, chess, ASE and Moldavian diaspora. The red circles represent people connecting different communities or central figures for a given cluster.\n\n\nThis is why I extracted my own network of Facebook friends2, because I knew there are a few heterogeneous communities and the results could be interesting. It turned out even more surprising and insightful when applying the algorithms for the layout and node importance. The immediate contribution is that this anonimized dataset can be used for teaching and academic purposes, testing of algorithms on graphs.\n2 Lostcircles.com A scraper for Facebook friend network (Lostcircles, 2017)Moreover, it enables us to validate the results of algorithms with the actual knowledge of the communities and “important” people in it. First, I use the default visualization of the crawler and encode the knowledge I have about the network on the graph in the next page. Then, using python and networkx package to visualize it, but this time by calculating importance metrics via PageRank, Hits, the central and peripheral nodes according to the definition.\nThe observation is that the overlap between statistical insights and “ground truth” is quite large. Because the network is real and not idealized, some “hubs”, i.e. people connecting different clusters aren’t identified. Instead, it finds important people within the clusters, and interpretation of this will be discussed later."
  },
  {
    "objectID": "posts/2019-networks/networks.html#interpretation",
    "href": "posts/2019-networks/networks.html#interpretation",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "Interpretation",
    "text": "Interpretation\nThe first graph is quite complicated, so let’s review the communities which are clearly seen and shaded for clarity. The top cluster represents people from my highschool in Chisinau, Moldova. They are connected by a chess player to the quite large chess community of which I took part for many years. The chess cluster in turn is connected to my family, which joined later and are also interested in this game.\nMoreover, the network doesn’t have timestamps, but hypothetically, if I had them, I could tell how the network developed to its final state. It is by no means an impediment of telling the story. We can move on to the times I moved to Bucharest for CSIE bachelor, and we see within the central, big cluster several sub-communities, the first one being the foreign students, with which other foreign students, of course interact a lot. The second part, people from university are pretty much the same during the Bachelor and Masters. The person connecting the highschool, diaspora and university is my roommate and chess with CSIE a grandmaster who is also a professor.\nAnother source of connections are different conferences, summer schools and workshops, for example in Risk Management, Mathematics and Data Science. These nodes are not differentiated too well in the network, but the fact that the algorithm placed them in the same neighborhood is very insightful. Inside the Cybernetics Masters’, the red point, also confirmed by PageRank is the most central person, the chief of students’ senate.\nMoving next, we see a sparsely connected region of PhD students and Teaching Assistants. They, and the Cybernetics cluster are connected to the firm I’m working with by a professor who was a consultant at our firm. The last community is the Bucharest Machine Learning and Tech people, who use to be very active in meetups and conferences inside the capital."
  },
  {
    "objectID": "posts/2019-networks/networks.html#analysis-in-networkx",
    "href": "posts/2019-networks/networks.html#analysis-in-networkx",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "Analysis in NetworkX",
    "text": "Analysis in NetworkX\nNow we are armed with the knowledge and interpretation of this graph and can start with a blank slate, using the raw data, which is stored in a json format {’source’: 476, ’target’: 87}, later transformed in an edge list and finally a networkx Graph() object. Now we can visualize this object in python, using different layout algorithms, calculate statistics and starting exploring the dataset as if we knew nothing about it. The node labels are numbers in order to anonimize the data, without much loss of insight.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json\nimport networkx as nx\nfrom pprint import pprint\n%matplotlib inline\n\nwith open('bizovi.json', 'rb') as f:\n    data = json.load(f)\n    \ndf = pd.DataFrame([(x[\"source\"], x[\"target\"]) for x in data[\"links\"]], \n    columns=[\"n1\", \"n2\"])\ndf.head()\n\nG = nx.from_pandas_edgelist(df, 'n1', 'n2')\nClassical measures of centrality like degree, average clustering, betweenness centrality, despite their simplicity and power they all have the downside of how complex of patterns can they represent or suggest. This is where iterative, algorithmic approaches like Scaled PageRank and HITS come into play. The first one can be interpreted as the probability of ending up in a node when traversing the graph by a random walk a sufficiently long time. Hits uses another approach with the concepts of Authority and Hubs. It is important to note that they both were developed for directed graphs in the context of web pages and hyperlinks pointing to them and they show their full power there. Nonetheless, they can still be used for undirected graphs, as I did here.\n_ = plt.hist(nx.pagerank(G).values(), bins=30, color='.5')\nplt.xlabel(\"PageRank\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of the PageRank Score\")\nplt.show()\n\npagerank = nx.pagerank(G, alpha=0.8)\nhigh_rank = [key for key, val in pagerank.items() if val &gt; 0.0042]\n\nhits = nx.hits(G, max_iter=1000)\n_ = plt.hist(hits[0].values(), bins=30, color='.5')\nplt.xlabel(\"Authority Score\")\nplt.ylabel(\"Count\")\nplt.title(\"Hits\")\nplt.show()\n\nhigh_hits = [key for key, val in hits[0].items() if val &gt; 0.005]\nFirst, I apply the algorithms and then look at the distribution of node “importance” in order to identify a good treshold for the visualization. The important nodes are then stored into high_rank and high_hits variables. The parameter alpha is essential for the PageRank not to get stuck into loops and distorting the distribution.\n\n\n\n\n\n\n\n\n\nScaled PageRank\n\n\n\n\n\n\n\nHits\n\n\n\n\n\nThe distribution of node importance according to PageRank and Hits algorithms. HITS needed a much larger number of iterations to converge and is much more strict in what it considers to be important nodes, most of them being concentrated around zero. For this particular network, PageRank scores seem a bit more sensible overall, but HITS differentiates better between importance.\nAs a further warm-up, let’s calculate a few statistics on this graph, which interpretation we will discuss later. The two statistics which can vary greatly in conclusions by network type, in this case pretty much agree: the average clustering (0.49) and transitivity (0.45). The central and peripheral nodes will be displayed directly on the graph. The diameter of the network is 8, radius is 4 and the average shortest path between any node to other node is 3.26.\nperiphery = nx.periphery(G)\ncenter = nx.center(G)\nmin_node_cut = nx.minimum_node_cut(G)\nnx.average_clustering(G), nx.transitivity(G)\nprint(nx.average_shortest_path_length(G))\nprint(nx.diameter(G))\nNow it is time to visualize the network and add the nodes which were calculated above according to criteria of importance. It is important to note that networkx is NOT primarily a visualization of network package: it uses the standard matplotlib, but it is good enough for small networks such as this one. In just 12 lines of code we get a publication-ready visualization. For more advanced use-cases graphviz and Gephi, igraph in R can be more suitable. In the next part, these repetitive operations will be wrapped up in a generic function which automates this a little bit painful process.\nnp.random.seed(3245)\nlegend_elements = [Line2D([0], [0], marker='o', color='wheat', \n        markersize=10, label='Periphery'),\n    Line2D([0], [0], marker='o', color='steelblue', \n        label='PageRank', markersize=10), \n    Line2D([0], [0], marker='o', color='indianred', \n        label='Centrality', markersize=10), \n    Line2D([0], [0], marker='o', color='green', \n        label='Min Node Cut', markersize=10)]\n\nplt.figure(figsize=(13, 11))\nplt.axis(\"off\")\nplt.title(\"Network of Friends. Fruchterman Reigold Layout\")\npos = nx.fruchterman_reingold_layout(G)\nnx.draw_networkx(G, node_color=\".3\", pos=pos, node_size=60, \n    with_labels=False, edge_color=\".8\")\nnx.draw_networkx_nodes(periphery, pos=pos, \n    node_color='wheat', node_size=50)\nnx.draw_networkx_nodes(high_rank, pos=pos, \n    node_color='steelblue', node_size=50)\nnx.draw_networkx_nodes(center, pos=pos, \n    node_color='indianred', node_size=90)\nnx.draw_networkx_nodes(min_node_cut, pos=pos, \n    node_color='green', node_size=90)\n\nplt.legend(legend_elements, [\"Periphery\", \n    \"PageRank\", \"Centrality\", \"Min Node Cut\"])\nplt.show()\n The final result of applying classical algorithms turned out extremely close to the ground truth, i.e. the interpretation I gave in the beginning\nThe network is reversed, but even cleaner than the one generated by lostcircles. We can see on the graph on the next page that the periphery is not that large as expected. Three, or even four important people who connect clusters are correctly identified either by centrality or PageRank algorithm. Moreover, it identifies very well connected people inside the clusters.\nFor the minimal node cut there are multiple choices, but it seems that the algorithm “thinks” that the family is the most poorly connected cluster to the other ones, which, looking at the picture is absolutely reasonable, as we have to remove only one node to achieve a disconnection, the green one.\nOne a side note, good graph datasets for teaching are relatively rare, and with closure of Facebook API and LinkedIn API, researchers working with network data may stick with the old datasets. This one seems interesting enough for tutorials and projects.\nA few comments on what the code does: first it sets up the environment, defining the size of the picture and deleting the axis, then it calculates the position of each node by the Fruchterman Reigold Algorithm. The first layer of grey nodes is added, together with all of the connections. Then the peripheral nodes calculated above are displayed in beige with the function nx.draw_networkx_nodes, which also requires the position, the Centrality, Minimum Cut and PageRank are added in turn. The finishing touch is the legend.\nThe last two obvious things to try are calculating the Local Clustering Coefficient and seeing the correlation between Node degree and LCC. We see that the first follows a pretty symmetric distribution, with exception of the stack at zero, which is expected in such a network. It means that there are nodes which are highly clustered and some not at all, but the majority at around 0.5.\nThe correlation with the Node Degree shows that as node degree increases, the expected LCC drops. This relationship is usually visualized at a log scale, but our network is not enough for this to be necessary and for the pattern to emerge. There is also an interesting boundary on the top of the point cloud. Remember how we calculated the average clustering above (0.49) and it agreed in large with the alternative method of transitivity, which is based on the concept on “triangle closure”. It means that networks, in which pairs of 2 out of 3 nodes are connected, tend to “produce” closures within those triplets.\n\n\n\n\n\n\n\n\n\nLCC\n\n\n\n\n\n\n\nCorrelation\n\n\n\n\n\nTwo classical measures of clustering\nBelow is the code which uses list comprehensions and nx functions on the graphs in order to compute and plot the distributions of the degree and LCC.\n_ = plt.hist(list(nx.clustering(G).values()), bins=30, color='.5')\nplt.xlabel(\"Local clustering coefficient\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of the Local clustering coefficient\")\nplt.show()\n\nplt.scatter([x[1] for x in list(nx.degree(G))], \n    list(nx.clustering(G).values()))\nplt.xlabel(\"Node Degree\")\nplt.ylabel(\"Local Clustering Coefficient\")\nplt.title(\"Distribution of the Node Degree\")\nplt.show()\n\n# Bonus: Node degree distribution\n_ = plt.hist([x[1] for x in list(nx.degree(G))], bins=30, color='.5')\nplt.xlabel(\"Node Degree\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of the Node Degree\")\nplt.show()"
  },
  {
    "objectID": "posts/2019-networks/networks.html#research-citations-and-ontologies",
    "href": "posts/2019-networks/networks.html#research-citations-and-ontologies",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "Research, Citations and Ontologies",
    "text": "Research, Citations and Ontologies\nThe techniques and algorithms applied on these toy examples can be and have been successfully used in discovering patterns of citation networks for research pages, knowledge graphs, blogs and web pages. It turns out to be an extremely important tool for discovery and even recommendation, as a researcher or reader has limited time and capacity, it is reasonable to suggest the most relevant or important research on her topic.\nAlso, papers which do meta-analyses, combined with Natural Language Processing techniques are extremely interesting, as they attempt to draw the landscape of research, opinions and topics. It can also be applied to News in real time, which was successfully accomplished by Marko Grobelnik and his group in Slovenia."
  },
  {
    "objectID": "posts/2019-networks/networks.html#conclusions",
    "href": "posts/2019-networks/networks.html#conclusions",
    "title": "Statistical Analysis of Facebook Network of Friends",
    "section": "Conclusions",
    "text": "Conclusions\nAs a data scientist/ machine learning researcher it was useful to step outside the (statistical) approaches on cross-sectional and panel data with which we usually work and explore applications driven by the network data.\nWe looked at a fun and practical example of analysing a network of friends by applying some classical algorithms on it, managing to characterize quite a large part of its features and structure. A future area of improvement and research would be to see how such networks evolve in time, by enriching the data with timestamps of friendship requests acceptance. In this process, I learned a lot about practical statistical analysis of graph data and hope to pass this hands-on experience to the readers."
  },
  {
    "objectID": "posts/2023-rules/principles.html",
    "href": "posts/2023-rules/principles.html",
    "title": "10 Lessons in Humility",
    "section": "",
    "text": "I was trying to articulate a philosophy for the courses I teach and accidentally wrote 10 non-exhaustive principles for life, which are also lessons in humility. It will not be surprising if the humble wonder practice by John Vervaeke shaped these ideas."
  },
  {
    "objectID": "posts/2023-rules/principles.html#john-vervaekes-humble-wonder",
    "href": "posts/2023-rules/principles.html#john-vervaekes-humble-wonder",
    "title": "10 Lessons in Humility",
    "section": "John Vervaeke’s Humble Wonder",
    "text": "John Vervaeke’s Humble Wonder\n\n\n\nCheck out John Vervaeke’s After Socrates, Lecture 2.\nPracticing the Socratic learned ignorance about the self:\n\nThere is so much I do not know about myself because of all of the [combinatorially explosive] facts.\nThere is so much I shall never know about myself because of all of the fate.\nThere is so much I refuse to see about myself because of all of my foolishness.\nThere is so much I am unable to see about myself because of all of my faults.\n\nPracticing the learned ignorance about the world:\n\nThere is so much I do not know about the world because of all of the facts.\nThere is so much I shall never know about the world because of all of the fate.\nThere is so much I refuse to see about the world because of all of my foolishness.\nThere is so much I am unable to see about the world because of all of my faults."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Who am I? Tough question …",
    "section": "",
    "text": "Mihai Bizovi | VP of Decision Science @AdoreMe\nIt is important to me that these conceptual frames fit into a coherent whole and contribute in a practical way towards a good life. When I started the blog in 2017, my work was at the center (metaphorically and literally), but now the golden thread is philosophy as a way of life.\nI can’t emphasise enough the importance of different levels of “knowing”: propositional, procedural, perspectival, and participatory – as it is not enough to know the facts (or have beliefs), but to know how to do something, to have a perspective of the “world” and a sense of participation in whatever you’re engaged in. I mean that we’re agents in different arenas of life, and the sense of meaning comes from an attunement to those arenas. We participate in a course of something, which has impact on the environment, which changes us and how we view and relate to the world, self and others."
  },
  {
    "objectID": "about.html#long-story-short",
    "href": "about.html#long-story-short",
    "title": "Who am I? Tough question …",
    "section": "Long Story Short",
    "text": "Long Story Short\n\nGraduate of Cybernetics (2017) and Quantitative Economics (2019)\n\nDid some research in Complex Systems, Agent-Based Modeling, Systems’ Dynamics, and Heterodox Economics, which was lots of fun\nMy serious work was at the intersection of Bayesian Statistics and Machine Learning (thesis, dissertation)\n\nTeaching Data Science (by which I mean Decision Science) at university and private courses. You can check out the key ideas in the course homepage.\nVP of Decision Science at Adore Me Tech (with the firm since 2016)\n\nLingerie e-commerce in the U.S., with a PAYG, subscription, try-at-home, marketplace business models. Acquired by Victoria’s Secret in 2022\n70 tech people in Bucharest, over 300 employees\nMy work involves AI Strategy, Product Management, ML Systems Design 1\n\nSome typical applications of AI in an e-commerce I contributed to:\n\nDemand Planning and Inventory Optimization systems to prevent lost sales and excess inventory across multiple distribution channels\nRecommender Systems to help users find relevant, personalized items and bundles, the most challenging use-case being in home-try-on\nMarketing optimizations in Acquisition, Advertisement, Engagement, CRM, Merchandising, Pricing and Promotion\n\nHobbies and miscellanea:\n\nReading about “Philosophy as a Way of Life” and Cognitive Science\nPainting and drawing places I love, learning art history\nHiking and long walks; going to Jazz, Classical, Metal concerts\nPlayed chess professionally, but as most, gave it up as an adult\n\n\n1 \n\n\nSee one of my conference talks at BigDataWeek, intended for a mixed tech/business audience: Pragmatic AI in Google Cloud Platform\n\n\nUnsurprisingly, there will be lots of painting metaphors when it comes to simplicity, and cognitive science references when talking about ways in which we’re biased and foolish. Chess, of course, inspires analogies of competition, strategy, and tactics to its service."
  }
]