<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Analysis &amp; Economic Cybernetics on Data Analysis &amp; Economic Cybernetics</title>
    <link>https://bizovi.github.io/</link>
    <description>Recent content in Data Analysis &amp; Economic Cybernetics on Data Analysis &amp; Economic Cybernetics</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Bizovi Mihai</copyright>
    <lastBuildDate>Tue, 01 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Reproducing The Economist Chart</title>
      <link>https://bizovi.github.io/post/2017-11-25-reproducing-the-economist-chart/</link>
      <pubDate>Sat, 25 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/post/2017-11-25-reproducing-the-economist-chart/</guid>
      <description>&lt;p&gt;While searching for solutions to fine-tune ggplot2 visualizations, I stumbled upon&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; a nice challenge: to reproduce &lt;code&gt;The Economist&lt;/code&gt;’s plot showing the correlation between &lt;em&gt;Corruption Perception Index&lt;/em&gt; and the &lt;em&gt;Human Development Index&lt;/em&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(knitr)
knitr::include_graphics(path = &amp;quot;data/economist.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;data/economist.png&#34; width=&#34;1200&#34; /&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The conclusion from this exercise is that even a simple task like reproducing a chart involves various analysis and modeling decisions. The only way to reduce the amount of mistakes is to make the analysis reproducible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Following the data trails, there are two sources of data:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Human Development Index in a .csv file from &lt;a href=&#34;http://hdr.undp.org/en/data&#34;&gt;&lt;code&gt;UN Human Development Report&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corruprion perception Index in a spreadsheet &lt;a href=&#34;https://www.transparency.org/cpi2015&#34;&gt;&lt;code&gt;from Transparency International&lt;/code&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note, that for any serious analysis we need to take into account the methodology of how these indicators are calculated and in the spreadsheet there are some details related to that. The first thing to notice by looking at the data is that the names of countries do not match, which is to be expected since the souces are different. Also, there are countries for which the CPI is not available.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)  # data processing and visualization tools
library(readxl)     # tidy reading of spreadsheets
library(ggrepel)    # repel text
library(data.table) # isn&amp;#39;t really needed, but hope you will discover this amazing package&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;data-preprocessing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data Preprocessing&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# skip the first row as it only contains text
hdi &amp;lt;- fread(&amp;quot;data/Human development index (HDI).csv&amp;quot;, skip = 1) %&amp;gt;% 
  select(Country, `2015`)  %&amp;gt;% 
  rename(HDI_2015 = `2015`) %&amp;gt;% 
  mutate(HDI_2015 = as.numeric(HDI_2015)) %&amp;gt;% 
  # next line deletes the space before all countries, 
  # which would mess up the joining
  mutate(Country = substr(Country, 2, nchar(Country))) %&amp;gt;% 
  as_tibble()
hdi %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##               Country HDI_2015
##                 &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1         Afghanistan    0.479
## 2             Albania    0.764
## 3             Algeria    0.745
## 4             Andorra    0.858
## 5              Angola    0.533
## 6 Antigua and Barbuda    0.786&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It is probable The Economist used Region Mappings from UN in the following table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cpi &amp;lt;- read_excel(&amp;quot;data/CPI_2015_FullDataSet.xlsx&amp;quot;, sheet = 1) %&amp;gt;% 
  rename(Country  = `Country/Territory`, 
         CPI_2015 = `CPI 2015 Score`) %&amp;gt;% 
  select(Country, Region, CPI_2015)
head(cpi)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##       Country Region CPI_2015
##         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     Denmark  WE/EU       91
## 2 New Zealand     AP       91
## 3     Finland  WE/EU       90
## 4      Sweden  WE/EU       89
## 5      Norway  WE/EU       88
## 6 Switzerland  WE/EU       86&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Notice that the HDI dataset has data for 188 countries availabele, but CPI is available only for 168. If you try to join the data you will get an unpleasant surprise: some names do not match and we have to fix that.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;not.matching &amp;lt;- cpi  %&amp;gt;% left_join(hdi, by = &amp;quot;Country&amp;quot;) %&amp;gt;% 
  filter(is.na(HDI_2015))

# identify corresponding places hdi dataset
idx &amp;lt;- c(74, NA, 29, 90, 168, 21, NA, 112, 46, 185,
         166, 139, 79, 93, 39, 40, 164, 184, NA, NA)
match &amp;lt;- data.frame(
  country = as.character(not.matching$Country),
  idx_hdi = idx
) %&amp;gt;% na.omit

hdi[match$idx_hdi, ]$Country &amp;lt;- as.character(match$country)

data &amp;lt;- cpi %&amp;gt;% left_join(hdi, by = &amp;quot;Country&amp;quot;)
head(data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 4
##       Country Region CPI_2015 HDI_2015
##         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;
## 1     Denmark  WE/EU       91    0.925
## 2 New Zealand     AP       91    0.915
## 3     Finland  WE/EU       90    0.895
## 4      Sweden  WE/EU       89    0.913
## 5      Norway  WE/EU       88    0.949
## 6 Switzerland  WE/EU       86    0.939&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to have full names for regions, there are three options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;To leave as-is and fix in the plot&lt;/li&gt;
&lt;li&gt;Second, to rename the factors&lt;/li&gt;
&lt;li&gt;Third, add a new column using a mapping table with a left_join&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# data$Region %&amp;gt;% unique()
mapping &amp;lt;- data.frame(
  Region      = c(&amp;quot;WE/EU&amp;quot;, &amp;quot;AP&amp;quot;, &amp;quot;AME&amp;quot;, &amp;quot;MENA&amp;quot;, &amp;quot;SSA&amp;quot;, &amp;quot;ECA&amp;quot;), 
  Region_Name = c(&amp;quot;OECD&amp;quot;, &amp;quot;Asia &amp;amp; Oceania&amp;quot;, &amp;quot;Americas&amp;quot;, 
                  &amp;quot;Middle East &amp;amp; North Africa&amp;quot;, &amp;quot;Sub-Saharan Africa&amp;quot;, 
                  &amp;quot;Eastern &amp;amp; Central Europe&amp;quot;)
)

data &amp;lt;- data %&amp;gt;% left_join(mapping, by = &amp;quot;Region&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Column `Region` joining character vector and factor, coercing into
## character vector&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##       Country Region CPI_2015 HDI_2015    Region_Name
##         &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;    &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;         &amp;lt;fctr&amp;gt;
## 1     Denmark  WE/EU       91    0.925           OECD
## 2 New Zealand     AP       91    0.915 Asia &amp;amp; Oceania
## 3     Finland  WE/EU       90    0.895           OECD
## 4      Sweden  WE/EU       89    0.913           OECD
## 5      Norway  WE/EU       88    0.949           OECD
## 6 Switzerland  WE/EU       86    0.939           OECD&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;building-the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Building the Plot&lt;/h2&gt;
&lt;p&gt;The regression line used to fit the data looks like a logarithmic one, which is the model most probaly also used by the economist&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- data %&amp;gt;% 
  mutate(CPI_2015 = CPI_2015 / 10)

# choose country labels to display (could be according to some smart criteria)
# like the top and bottom countries for each Region
# here is just an arbitrary set

country.labels &amp;lt;- c(&amp;quot;New Zeeland&amp;quot;, &amp;quot;Norway&amp;quot;, &amp;quot;Singapore&amp;quot;, 
                     &amp;quot;Japan&amp;quot;, &amp;quot;Germany&amp;quot;, &amp;quot;Britain&amp;quot;, &amp;quot;Barbados&amp;quot;, 
                     &amp;quot;United States&amp;quot;, &amp;quot;France&amp;quot;, &amp;quot;Spain&amp;quot;, &amp;quot;Italy&amp;quot;, 
                     &amp;quot;China&amp;quot;, &amp;quot;Rwanda&amp;quot;, &amp;quot;South Africa&amp;quot;, &amp;quot;Greece&amp;quot;, 
                     &amp;quot;Iraq&amp;quot;, &amp;quot;Congo&amp;quot;, &amp;quot;Afghanistan&amp;quot;, 
                     &amp;quot;Venezuela&amp;quot;, &amp;quot;Russia&amp;quot;, &amp;quot;India&amp;quot;, &amp;quot;Bhutan&amp;quot;, 
                     &amp;quot;Cape Verde&amp;quot;, &amp;quot;Argentina&amp;quot;, &amp;quot;Romania&amp;quot;, &amp;quot;Moldova&amp;quot;
                    )
# for colors the trick is to use a named color vector
# fancy colors, close to the ones chosen by The Economist
clr &amp;lt;- c(&amp;quot;#006D6F&amp;quot;, &amp;quot;lightskyblue1&amp;quot;, &amp;quot;steelblue2&amp;quot;, 
         &amp;quot;indianred1&amp;quot;, &amp;quot;brown&amp;quot;, &amp;quot;seagreen&amp;quot;, &amp;quot;tomato&amp;quot;)
names(clr) &amp;lt;- c(as.character(mapping$Region_Name), &amp;quot;regression&amp;quot;)

p &amp;lt;- data %&amp;gt;% ggplot(aes(x = CPI_2015, y = HDI_2015)) + 
  stat_smooth(
    method = &amp;quot;lm&amp;quot;, se = FALSE, 
    formula = y ~ log(x),         # the linear model
    size = 1.1,  colour = &amp;quot;tomato&amp;quot;, # regression line colors
    aes(fill = &amp;quot;R=52%&amp;quot;)
    )  + 
  geom_point(
    aes(color = Region_Name), 
    # 21 is the only shape allowing both fill and color
    size = 3, shape = 21, fill = &amp;quot;white&amp;quot;, 
    stroke = 1.6 # thikness of point margin
             ) + 
  geom_text_repel(aes(label = 
      ifelse(Country %in% country.labels, Country, NA)), 
      color = &amp;quot;grey20&amp;quot;,
      segment.color = &amp;quot;grey80&amp;quot;,
      # very important in order not to overlap with points
      point.padding = unit(0.025, &amp;#39;npc&amp;#39;), 
      force = 1, 
      nudge_y = 0.03, 
      nudge_x = -0.015) + 
  scale_color_manual(
    values = clr, 
    breaks = c(&amp;quot;OECD&amp;quot;, &amp;quot;Americas&amp;quot;, &amp;quot;Asia &amp;amp; Oceania&amp;quot;, 
               &amp;quot;Middle East &amp;amp; North Africa&amp;quot;, 
               &amp;quot;Eastern &amp;amp; Central Europe&amp;quot;,
               &amp;quot;Sub-Saharan Africa&amp;quot;, &amp;quot;Rsq=52%&amp;quot;),
    labels = c(&amp;quot;OECD&amp;quot;, &amp;quot;Americas&amp;quot;, &amp;quot;Asia &amp;amp; \n Oceania&amp;quot;,
                &amp;quot;Middle East &amp;amp; \n North Africa&amp;quot;,
                &amp;quot;Central &amp;amp; \n Eastern Europe&amp;quot;,
                &amp;quot;Sub-Saharan \n Africa&amp;quot;, &amp;quot;Rsq=52%&amp;quot;)
               ) +
  theme_minimal() +
  theme(
    legend.position    = &amp;quot;top&amp;quot;, 
    panel.grid.major.x = element_blank(), 
    panel.grid.minor.x = element_blank(), 
    panel.grid.minor.y = element_blank(),
    panel.grid.major.y = element_line(color = &amp;#39;grey85&amp;#39;, size = 1), 
    axis.ticks.x = element_line(), 
    plot.title = element_text(size = 16, face = &amp;quot;bold&amp;quot;), 
    text = element_text(family = &amp;quot;sans&amp;quot;), 
    # more space in legend
    legend.key.size = unit(1.8, &amp;quot;lines&amp;quot;), 
    legend.spacing.x = unit(-0.5, &amp;quot;cm&amp;quot;)
    ) + 
  guides(colour = guide_legend(nrow  = 1, title = &amp;quot;&amp;quot;),
         fill   = guide_legend(title = &amp;quot;&amp;quot;)) +
  labs(x = expression(italic(&amp;quot;Corruption Perceptions Index, 2015 (10=least corrupt)&amp;quot;)), 
       y = expression(italic(&amp;quot;Human Development Index, 2015 (1=best)&amp;quot;)), 
       title = &amp;quot;Corruption and human development&amp;quot;, 
       caption = &amp;quot;Sources: Transparency International; UN Human Development Report&amp;quot;) + 
  expand_limits(y = 0.3) + 
  expand_limits(x = 1) + 
  scale_x_continuous(breaks = seq(1, 10, 1)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) + 
  coord_cartesian(ylim = c(0.28, 1), xlim = c(1, 10))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://bizovi.github.io/post/2017-11-25-reproducing-the-economist-chart_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(filename = &amp;quot;data/economist.png&amp;quot;, plot = p, width=8, height=5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 4 rows containing non-finite values (stat_smooth).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 4 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: Removed 146 rows containing missing values (geom_text_repel).&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html&#34; class=&#34;uri&#34;&gt;http://tutorials.iq.harvard.edu/R/Rgraphics/Rgraphics.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.economist.com/blogs/dailychart/2011/12/corruption-and-development&#34; class=&#34;uri&#34;&gt;https://www.economist.com/blogs/dailychart/2011/12/corruption-and-development&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R Tutorials for Behavioral Economics Class</title>
      <link>https://bizovi.github.io/post/2017-11-18-r-tutorial-behavioral-economics-2/</link>
      <pubDate>Wed, 22 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/post/2017-11-18-r-tutorial-behavioral-economics-2/</guid>
      <description>&lt;p&gt;In the last blog post I took a bird’s eye (personal) perspective of R programming and suggested not to be discouraged by early encounters with this seemingly weird language. The conclusion was that by following “the right tool for the right job” principle, R is a great language for statistical research and the ecosystem of packages improves the data analysis workflow and gives the modeler more tools to extract insights from data.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;What is the link to Behavioral Economics? R will be used (in class) as a tool for exploring domain-specific data in order to make informed decisions, fitting different kinds of models and validating the intuition from readings like Tversky &amp;amp; Kahneman, Thaler on real data.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div id=&#34;a-tutorial-on-metropolitan-area-housing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A tutorial on Metropolitan Area Housing&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse) # the ecosystem of data processing tools
library(tidyr)     # useful for getting the data in short-long formats
library(reshape2)  # for the function melt (long format data)
library(glue)      # string formatting
library(assertive) # assertions (active tests)
library(ochRe)     # stunning color palettes
library(readxl)    # read from excel spreadsheets&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data &amp;lt;- read_excel(&amp;quot;data/landdata-msas-2016q1.xls&amp;quot;, skip = 1)
data &amp;lt;- data %&amp;gt;% 
  mutate(MSA = as.factor(MSA), 
         Date = zoo::as.yearqtr(Date, format = &amp;quot;%YQ%q&amp;quot;)
         ) %&amp;gt;%
  dplyr::rename(home_value = `Home Value`, 
                structure_cost = `Structure Cost`, 
                land_value = `Land Value`, 
                land_share = `Land Share (Pct)`, 
                home_pi = `Home Price Index`, 
                land_pi = `Land Price Index`
                )
data %&amp;gt;% head()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 8
##       MSA          Date home_value structure_cost land_value land_share
##    &amp;lt;fctr&amp;gt; &amp;lt;S3: yearqtr&amp;gt;      &amp;lt;dbl&amp;gt;          &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;      &amp;lt;dbl&amp;gt;
## 1 ATLANTA       1984 Q4   92856.39       67112.34   25744.05  0.2772459
## 2 ATLANTA       1985 Q1   93044.12       67826.46   25217.66  0.2710291
## 3 ATLANTA       1985 Q2   93269.26       68444.29   24824.97  0.2661646
## 4 ATLANTA       1985 Q3   97490.47       69026.93   28463.55  0.2919623
## 5 ATLANTA       1985 Q4   98809.50       69562.32   29247.18  0.2959956
## 6 ATLANTA       1986 Q1   99846.67       70078.32   29768.36  0.2981407
## # ... with 2 more variables: home_pi &amp;lt;dbl&amp;gt;, land_pi &amp;lt;dbl&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ny &amp;lt;- data %&amp;gt;% dplyr::filter(MSA == &amp;quot;NEWYORK&amp;quot;) 
ny %&amp;gt;% 
  tidyr::gather(key = variable, value = value, -c(MSA, Date)) %&amp;gt;% 
  dplyr::select(-MSA) %&amp;gt;% 
  ggplot(aes(x = value, fill = variable)) + 
  geom_histogram(alpha = 0.5) + 
  facet_wrap(~ variable, scale = &amp;quot;free&amp;quot;) + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(y = &amp;quot;Frequency&amp;quot;, title = &amp;quot;Distributions of the variables for NY&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  scale_fill_ochre(palette = &amp;quot;nolan_ned&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://bizovi.github.io/post/2017-11-18-r-tutorial-behavioral-economics-2_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting-theoretical-utility-and-value-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting theoretical Utility and Value functions&lt;/h2&gt;
&lt;p&gt;Hyperbolic Absolute Risk Aversion (Linear Risk Tolerance) is a class of utility functions, that can easily describe the particular cases of Constant Absolute Risk Aversion (CARA) and Constant Relative Risk Aversion (CRRA). &lt;span class=&#34;math display&#34;&gt;\[U(x;\gamma,a,b) = \frac{1 - \gamma}{\gamma}\bigg( \frac{ax}{1-\gamma} +b   \bigg) ^ \gamma\]&lt;/span&gt; It is designed in such a way that the risk tolerance is linear &lt;span class=&#34;math display&#34;&gt;\[\tau(x)=-\frac{U&amp;#39;(x)}{U&amp;#39;&amp;#39;(x)} = \frac{1}{1-\gamma}x + 
\frac{b}{a}
\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(a &amp;gt; 0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\frac{ax}{1-\gamma} + b &amp;gt; 0\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function tends to logarithmic if gamma tends to zero (by L’Hopital rule) and linear when goes to one. &lt;span class=&#34;math display&#34;&gt;\[U(x) \xrightarrow[\gamma \to 0]{} log(ax+b)\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The reason we’re going to overkill a simple plotting of HARA utility function is to showcase some important R concepts.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, we test user inputs via the &lt;code&gt;assertthat&lt;/code&gt; package, which is a first step moving from manual validation to more automated methods of code testing. This doesn’t kick in when doing an exploratory analysis, but hopefully when starting to build Packages, learning how to test code in R&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; will pay off.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hara &amp;lt;- function(x, a, b, gamma) {
  # check that inputs to the function are good
  # using assetthat package
  assert_all_are_non_negative(x)
  assert_all_are_greater_than(x = a * x / (1 - gamma) + b, y = 0)
  assert_all_are_true(
    c(    
      is_positive(a), 
      is_not_equal_to(a, 0), 
      is_in_range(gamma, lower = 0, upper = 1)
      )
  )
  
  # return the function itself
  return(
    ((1 - gamma) / gamma) * (a * x / (1 - gamma) + b)^gamma
  )
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second, we want to run the function a number of times by varying a parameter. The first idea of would be to write a for loop, but there is a more elegant way. The apply family of functions allows to run a function on elements of objects like lists. In this particular case I used a &lt;code&gt;sapply&lt;/code&gt; function which returns a data frame with a column for values of function given a parameter.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# check if the function returns expected values
# hara(x = 1:20, a = 36, b = 52, gamma = 0.5)

gammas &amp;lt;- c(0.5, 0.4, 0.3, 0.25, 0.1)
a &amp;lt;- 36 # the functions start to look more linear as a is small
b &amp;lt;- 52 

# apply the function on using 4 different gamma parameters
df &amp;lt;- sapply(X = gammas, FUN = hara, x = 0:30,  a = a, b = b) %&amp;gt;% 
  as_tibble()
# add the appropriate values to column names
colnames(df) &amp;lt;- as.character(gammas)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Third, we use the tidy way of data processing by heavily relying on the pipe operator &lt;code&gt;%&amp;gt;%&lt;/code&gt;, which passes an output of a function or an object to the next one. This makes the code much more readable and shows its power in Exploratory Data Analysis. The concept of Tidy Data&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; is inspired from relational databases and in order to use R eficiently one needs to understand the use-cases for long and wide data formats, what classifies as tidy.&lt;/p&gt;
&lt;p&gt;The fourth point is that using &lt;code&gt;ggplot2&lt;/code&gt; as the go-to visualization tool is not harder or more advanced, but simpler! Once you understand the &lt;code&gt;grammar of graphics&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; you get an incredibly general tool for data visualization, in which you can do stunning graphs with very little effort once you get the data into the right format.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;df %&amp;gt;% 
  mutate(x = 1:nrow(df)) %&amp;gt;% 
  reshape2::melt(id.vars = &amp;quot;x&amp;quot;) %&amp;gt;% # could use tidyr::gather as an alternative
  rename(gamma = variable) %&amp;gt;%
  ggplot(aes(x = x, y = value, color = gamma)) + 
  geom_line(size = 1) + 
  theme_minimal() + 
  labs(title = &amp;quot;Hyperbolic Absolute Risk Aversion function&amp;quot;, 
       subtitle = glue(&amp;quot;with hyperparameters a = {a}, b = {b}&amp;quot;)) + 
  scale_color_ochre(palette = &amp;quot;nolan_ned&amp;quot;) # if you feel fancy today&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://bizovi.github.io/post/2017-11-18-r-tutorial-behavioral-economics-2_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## Explain what is a color palette and why is it useful&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;a-tutorial-on-analysis-of-returns-and-investment-decisions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A tutorial on Analysis of Returns and Investment Decisions&lt;/h2&gt;
&lt;!--

```r
library(tidyquant) # tidy data analysis and financial data interfacing
```


```r
# get stock prices for seven years using the Yahoo Finance API
stocks &lt;- c(&#34;ORCL&#34;, &#34;FSLR&#34;, &#34;AMZN&#34;, &#34;GOOG&#34;, &#34;GE&#34;) %&gt;% tq_get(
  get  = &#34;stock.prices&#34;, 
  from = &#34;2010-01-01&#34;, 
  to   = &#34;2017-10-30&#34;
)

# calculate the monthly returns
monthly.returns &lt;- stocks %&gt;% 
  group_by(symbol) %&gt;% 
  tq_transmute(
    select     = adjusted, # column to calculate returns from 
    mutate_fun = periodReturn, # function from quantmod
    period     = &#34;monthly&#34;,    # alternatively, &#34;weekly&#34; or daily
    col_rename = &#34;return&#34;)

# need to compare against a general price index
baseline &lt;- &#34;SPX&#34; %&gt;%
    tq_get(
      get  = &#34;stock.prices&#34;,
      from = &#34;2010-01-01&#34;,
      to   = &#34;2017-10-30&#34;) 


monthly.baseline &lt;- baseline %&gt;%
    tq_transmute(
      select     = adjusted, 
      mutate_fun = periodReturn, 
      period     = &#34;monthly&#34;, 
      col_rename = &#34;base.returns&#34;)
```

```
## Warning in to_period(xx, period = on.opts[[period]], ...): missing values
## removed from data
```


```r
# In order to perserve some information when averaging at monthly level, it would be great to represent the uncecrtainty

custom_stat_fun &lt;- function(x, na.rm = TRUE, ...) {
    # ...   = additional arguments
    c(mean    = mean(x, na.rm = na.rm),
      stdev   = sd(x, na.rm = na.rm),
      quantile(x, na.rm = na.rm, ...)) 
}

# quantiles
pr &lt;- c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)

# Applying the custom function by week
monthly_returns &lt;- stocks %&gt;%
  group_by(symbol) %&gt;%
    tq_transmute(
        select = close,
        mutate_fun = apply.monthly, 
        FUN = custom_stat_fun,
        na.rm = TRUE,
        probs = pr
    )

monthly_returns %&gt;%
    ggplot(aes(x = date, y = `50%`, color = symbol)) +
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`), 
                color = NA, fill = palette_light()[[1]], alpha = 0.3) +
    geom_point() +
    geom_vline(xintercept = as.Date(c(&#34;2011-09-01&#34;, &#34;2012-07-01&#34;, 
                                      &#34;2013-07-01&#34;, &#34;2014-02-01&#34;, 
                                      &#34;2015-01-01&#34;, &#34;2016-02-01&#34;)), 
               lty = 2, color = &#34;grey40&#34;) + 
    geom_ma(n = 5, size = 1, color = &#34;steelblue2&#34;, lty = 1) + 
    facet_wrap(~ symbol, ncol = 2, scale = &#34;free_y&#34;) +
    labs(title = &#34;Standard and Poor&#39;s Average Monthly Price&#34;, x = &#34;&#34;,
         subtitle = &#34;Volatility shown as 1st and 3rd quantiles&#34;,
         y = &#34;Price (k$)&#34;) +
  #  expand_limits(y = 50) + 
    scale_color_ochre(palette = &#34;lorikeet&#34;) +
    theme_bw() + 
  theme(legend.position = &#34;none&#34;)
```

&lt;img src=&#34;https://bizovi.github.io/post/2017-11-18-r-tutorial-behavioral-economics-2_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;672&#34; /&gt;

--&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Testing R Code &lt;a href=&#34;http://r-pkgs.had.co.nz/tests.html&#34; class=&#34;uri&#34;&gt;http://r-pkgs.had.co.nz/tests.html&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Hadley Wickham’s R for Data Science &lt;a href=&#34;http://r4ds.had.co.nz/&#34; class=&#34;uri&#34;&gt;http://r4ds.had.co.nz/&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Paper on the grammar of Graphics &lt;a href=&#34;http://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf&#34; class=&#34;uri&#34;&gt;http://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>R Programming. The big picture</title>
      <link>https://bizovi.github.io/post/2017-11-01-r-tutorial-behavioral-economics/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/post/2017-11-01-r-tutorial-behavioral-economics/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;I wish somebody showed me the real power of R earlier and explained the big picture&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is not an usual tutorial on R, my goal being to make you aware and curious about various topics related to Data Analysis in R, which I learned the hard way during a year of nearly daily use. It is not supposed to be easy or have a particular application in mind, but rather to suggest many possibilities. Also, even though this was originally written for a Behavioral Economics class, I’ll be speaking from a Data Scientist’s perspective and often drift away and not get in too much details, preferring to provide great (external) resources for learning.&lt;/p&gt;
&lt;div id=&#34;a-personal-encouter-with-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A personal encouter with R&lt;/h2&gt;
&lt;p&gt;First, a few words on how I decided to choose R as my go-to language. Stepping back, my first encounter with scientific computing was Nathan Kutz’s brilliant course on &lt;code&gt;Mathematical Methods for Data Analysis&lt;/code&gt;, emphasizing that we’d be doing well if we were MATLAB superstars. Another motivation for using MATLAB was my passion for the field of economic complexity, which involved nonlinear differential equations and required simulation. I played around with software like Steve Keen’s &lt;code&gt;Minsky&lt;/code&gt; for System Dynamics Models, &lt;code&gt;NetLogo&lt;/code&gt; for Agent-Based Models. This was during a time when I wasn’t thrilled by the teaching of statistics and econometrics, as those (linear, gaussian) models seemed far from satisfactory for explaining complex economic phenomena.&lt;/p&gt;
&lt;p&gt;The first course in R did not leave the impression that R was a powerful language, but rather weird and potentially useful for statistics. As time passed I shifted completely from Economic Research and criticising Neoclassical Economics towards Machine Learning and Data Science. I decided to learn Python over MATLAB, as it was clear I would probably apply Machine Learning models in a business environment and not in academia. R seemed at the time as a second choice to Python, but I had the chance to discover it a little bit more, especially the &lt;code&gt;John Hopkins R Programming&lt;/code&gt; courses. This led to me finding a lot of great blogs, books and articles by following the experts in R community. The priority then was getting a great theoretical understanding of Learning Theory and Data Science landscape, which brought me back to appreciating some of the deceptively “simple” concepts and models in statistics and econometrics.&lt;/p&gt;
&lt;p&gt;It was a matter of time, readings and practice which made me prefer &lt;code&gt;RMarkdown&lt;/code&gt; and &lt;code&gt;knitr&lt;/code&gt; over Jupyter Notebooks (IPython), and R over Python for the problems I wanted to solve at &lt;code&gt;Adore Me&lt;/code&gt;. I shifted my focus completely on understanding R and taking advantage of the features of this domain-specific language. In the end, it depends on what problems are you trying to solve. If I wanted to focus on Deep Learning research, it’s extremely probable that my language of choice would be Python. In a business setting, in order to bring value through statistical modeling, diverse perspectives are needed: from Econometrics and Machine Learning to Bayesian Statistics, Time Series Analysis and Simulation. R is perfect in that respect, while also helping in communicating the data analysis results and incentivising researchers to adhere to good practices.&lt;/p&gt;
&lt;p&gt;During a year I learned a lot of tricks in Exploratory Data Analysis using &lt;code&gt;tidyverse&lt;/code&gt; and &lt;code&gt;ggplot2&lt;/code&gt;, automated report generation using RMarkdown and Latex, built an interactive decision-support tool using Shiny, implemented my first end-to-end machine learning project, including a package with a custom model for TV Attribution, learned the importance of code versioning, wrote my bachelor thesis and created this website in R.&lt;/p&gt;
&lt;p&gt;Still, every time I find out new things to learn, like the tidy way of doing time series analysis using &lt;code&gt;tidyquant&lt;/code&gt;, the rlang evaluations, unit testing, paralell computation and lots of cutting-edge statistical models.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Why would R be a great choice for Behavioral Economics labs?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;R is a flexible and in my opinion underrated (Especially with all the hype surrounding Deep Learning and Python, frameworks like Tensorflow) language for &lt;em&gt;statistical research&lt;/em&gt;, which means it’s specialized and really good at data analysis. It’s a nice match, as in Behavioral Economics we want to look at evidence, real choices, analyze and model it in a reproducible fashion. It took time to appreciate some of the unconventional methods and techniques in R, the elegance of graphics and data processing. As I believe in using &lt;code&gt;the right tool for the right job&lt;/code&gt;, both in modeling and developing data products, learning R paid off.&lt;/p&gt;
&lt;p&gt;Because of the fast-growing open-source community and the ecosystem of R packages, a lot of cutting-edge research is made freely available. Some of these extensions changed in a fundamental way how we analyze data in R. Even though its flexibility allows to have a lot of solutions to the same problem, we should keep in mind the pitfalls of flexible languages and be rigurous in the way we write our code and do the analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;towards-reproducible-research-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Towards Reproducible Research in R&lt;/h2&gt;
&lt;p&gt;The replication crisis&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; is a hot topic right now in different fields. There are many great meta-studies, but my goal is very modest here: to suggest simple techniques that will help improve the reproducibility and transparency of the research. By making the source code freely available and reproducible we reduce the “research debt”&lt;a href=&#34;#fn2&#34; class=&#34;footnoteRef&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, making the progress a lot faster by involving people that have the curiosity to reproduce and improve on existing research.&lt;/p&gt;
&lt;p&gt;It has three main components and R does have nice solutions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Code reproducibility and versioning&lt;/li&gt;
&lt;li&gt;Data and Analysis&lt;/li&gt;
&lt;li&gt;But it works on my machine: Keeping the environment consistent&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, we can use &lt;code&gt;Rmarkdown&lt;/code&gt;&lt;a href=&#34;#fn3&#34; class=&#34;footnoteRef&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; to combine the content, code and outputs rendered in a single pdf document or html page, which can be easily published on the web using a static page generator like Hugo/Jekyll. Second, using a code versioning tool like Git&lt;a href=&#34;#fn4&#34; class=&#34;footnoteRef&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt; is a no-brainer, even if you’re working alone. It has nice integrations with RStudio and will save the future you a lot of time.&lt;/p&gt;
&lt;p&gt;What cannot be solved by packages alone, should be obtained as a result of a rigorous statistical methodology&lt;a href=&#34;#fn5&#34; class=&#34;footnoteRef&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, combined with a quality, clean code. It requires a lot of discipline amid seemingly endless possibilities of R.&lt;/p&gt;
&lt;p&gt;The third key element to reproducibility is also very challenging. The hot solution right now is using &lt;code&gt;Docker Containers&lt;/code&gt;&lt;a href=&#34;#fn6&#34; class=&#34;footnoteRef&#34; id=&#34;fnref6&#34;&gt;&lt;sup&gt;6&lt;/sup&gt;&lt;/a&gt;, which are lightweight, immutable and disposable, providing necessary libraries, binaries and OS components to run the analysis. They should be viewed more like a process, than a virtual machine which is started from an image (remember .iso). It saves a lot of time and effort trying to install all the packages manually, dealing with versioning issues and so on. Imagine some of the harder to install setups for Bayesian Analysis, involving STAN or JAGS are taken care of by somebody else, so you can focus on really important things like improving the method, the quality, robustness and generality of the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tidy-data-analysis-and-visualization&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tidy Data Analysis and Visualization&lt;/h2&gt;
&lt;p&gt;There is an emerging way of data analysis in R which old-school users might mistakenly ignore: the &lt;code&gt;tidy&lt;/code&gt; way.&lt;a href=&#34;#fn7&#34; class=&#34;footnoteRef&#34; id=&#34;fnref7&#34;&gt;&lt;sup&gt;7&lt;/sup&gt;&lt;/a&gt; It had such an impact on the community, that it made R cool again. It’s not without a flaw, especially when talking about production environments and writing packages from scratch, but that’s why we have the right tool for the right job principle. If you can do an analysis using this ecosystem and principles much easier and in a readable, intuitive way, why not use the simpler tool.&lt;/p&gt;
&lt;p&gt;For R novices, a simple advice is that everything you see is an object and everything that does something is a function. The second thing is that the power of R lies in vectorisation, done behind the scenes by extremely fast libraries. Of course there are cases when you might use a for loop and it will be a good solution, but for the most purposes it isn’t, so pay a close attention to the functions in &lt;code&gt;*apply&lt;/code&gt; family.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Replication_crisis&#34; class=&#34;uri&#34;&gt;https://en.wikipedia.org/wiki/Replication_crisis&lt;/a&gt;&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://distill.pub/&#34; class=&#34;uri&#34;&gt;https://distill.pub/&lt;/a&gt;&lt;a href=&#34;#fnref2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;On using Rmarkdown &lt;a href=&#34;https://www.youtube.com/watch?v=DNS7i2m4sB0&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=DNS7i2m4sB0&lt;/a&gt;&lt;a href=&#34;#fnref3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Rstudio github integration: &lt;a href=&#34;http://www.datasurg.net/2015/07/13/rstudio-and-github/&#34; class=&#34;uri&#34;&gt;http://www.datasurg.net/2015/07/13/rstudio-and-github/&lt;/a&gt;&lt;a href=&#34;#fnref4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Columbia University stats course: &lt;a href=&#34;https://courses.edx.org/courses/course-v1:ColumbiaX+DS101X+1T2016/course/&#34; class=&#34;uri&#34;&gt;https://courses.edx.org/courses/course-v1:ColumbiaX+DS101X+1T2016/course/&lt;/a&gt;&lt;a href=&#34;#fnref5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn6&#34;&gt;&lt;p&gt;An intro to Docker: &lt;a href=&#34;https://www.youtube.com/watch?v=NBrQbkZ3Lzw&amp;amp;t=2177s&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=NBrQbkZ3Lzw&amp;amp;t=2177s&lt;/a&gt;&lt;a href=&#34;#fnref6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn7&#34;&gt;&lt;p&gt;A playlist on the tidyverse &lt;a href=&#34;https://www.youtube.com/watch?v=K-ss_ag2k9E&amp;amp;list=PLNtpLD4WiWbw9Cgcg6IU75u-44TrrN3A4&#34; class=&#34;uri&#34;&gt;https://www.youtube.com/watch?v=K-ss_ag2k9E&amp;amp;list=PLNtpLD4WiWbw9Cgcg6IU75u-44TrrN3A4&lt;/a&gt;&lt;a href=&#34;#fnref7&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Modeling and Bayesian Inference. Part I</title>
      <link>https://bizovi.github.io/post/stochastic/</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/post/stochastic/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;”I can live with doubt and uncertainty and not knowing. We absolutely must leave room for doubt or there is no progress and there is no learning. There is no learning without having to pose a question. And a question requires doubt. People search for certainty, but there is no certainty.”
— Richard Feynman &lt;em&gt;The pleasure of finding things out&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first series of posts gives an overview of the probabilistic perspective on Machine Learning &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Bizovi-Mihai-Sto&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Bizovi-Mihai-Sto&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, a flexible and adaptive approach which can be used in modeling complex phenomena. This allows the development of models for specific applications, in contrast with a toolkit view of the Machine Learning.&lt;/p&gt;

&lt;h3 id=&#34;the-need-for-stochastic-modeling-and-bayesian-thinking&#34;&gt;The need for stochastic modeling and Bayesian thinking&lt;/h3&gt;

&lt;p&gt;Current challenges in economics need an interdisciplinary, multidimensional approach and different perspectives. In order to have more powerful models in our toolbox, we&amp;rsquo;ll investigate statistical modeling based on three schools of thought: Stochastic Modeling, Machine Learning and Bayesian Analysis, which has applications from economics and finance to genetics, linguistics and artificial intelligence.&lt;/p&gt;

&lt;p&gt;This approach is extremely general and allows us to make inferences about latent quantities and relationships, to identify what is hidden beneath appearance, to account for the uncertainty and nonlinearities of economic phenomena. The idea of learning from data, flexibility of representing uncertainty and the parsimony principle leads to adaptive and robust modeling, able to capture non-trivial aspects of the problem. The focus will be very much on models and a deep understanding of concepts on which they&amp;rsquo;re built.&lt;/p&gt;

&lt;p&gt;The ugly truth is that humans are not good at prediction, neither consistent with probability theory. We are &amp;ldquo;suffering&amp;rdquo; from multiple evolutionary mechanisms, cognitive biases and even statisticians, formally trained in the language of uncertainty, aren&amp;rsquo;t guarded against pitfalls that arise in practice and everyday life.&lt;/p&gt;

&lt;p&gt;The same story is in reconciliation of new evidence with prior beliefs, processing multidimensional data, ignorance of feedback loops and dynamics. The reason we are building models is to better understand the world, phenomena around us and in the end to improve our mental representations. Predictions that are less wrong usually take into account a variety of perspectives, a proven fact in ensemble modeling.&lt;/p&gt;

&lt;h3 id=&#34;three-perspectives&#34;&gt;Three perspectives&lt;/h3&gt;

&lt;p&gt;Machine learning models have become an essential part of online services and is taking over new fields, bringing firms and people lots of value. The multidimensional approach is what makes it all this possible. In contrast with classical stats and NHST (Null Hypothesis Significance Testing), our focus will be on generalization performance and the idea of a model. Even though these models have their roots in statistics, the field can be viewed in its own right.&lt;/p&gt;

&lt;p&gt;An important extension for which there is an acute need is dynamical models. A lot of interesting problems are multidimensional time series, which might invalidate the hypotheses behind classical data mining models. Coupled with the need of accounting for uncertainty, this brings us to the idea of introducing &lt;strong&gt;stochastic&lt;/strong&gt; elements and to extend the idea of &lt;strong&gt;learning&lt;/strong&gt; and &lt;strong&gt;generalization&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The third perspective which brings together stochastic elements with machine learning is &lt;em&gt;bayesian inference&lt;/em&gt;, as a method to update beliefs and knowledge based on data from different sources. Bayesian modeling enables us to build hierarchical models with latent structures, while encoding the uncertainty in parameters and operating with probability distributions. On the shoulders of these three &amp;ldquo;giants&amp;rdquo;, a stochastic perspective over Machine Learning emerges over the last two decades, which is flexible, general and more transparent than neural networks, works on thin data.&lt;/p&gt;

&lt;p&gt;There are several reasons the field is not yet in the mainstream: scalability and complexity of the models, in the sense of necessary knowledge and skill to start modeling. Once the software implementations, approximation methods and probabilistic programming languages will become better, the Bayesian methods will see another rise in popularity.&lt;/p&gt;

&lt;p&gt;The next posts will be dedicated to each of these fundamental perspectives, in which we&amp;rsquo;ll try to get to the essence. The last ones from this series will be on supervised and unsupervised probabilistic models such as Gaussian Processes and Mixture Models. An essential concept will be the ARD (Automatic Relevance Determination) and Kernels which encourage sparse solutions.&lt;/p&gt;

&lt;h3 id=&#34;the-goal&#34;&gt;The goal&lt;/h3&gt;

&lt;p&gt;The motivation behind the choice of probabilistic machine learning models is that there are a lot of opportunities to extend them and reuse as parts of a more complicated model.&lt;/p&gt;

&lt;p&gt;For example, Gaussian Processes can be transformed into a Relevance Vector Machine (the probabilistic version of SVM) or used for Bayesian Optimization of expensive functions, in learning of complex dynamical phenomena, spatio-temporal modeling (also known as Kreiging). The Probabilistic Principal Component Analysis can be used for a latent representation of a multidimensional time series and intrinsic dimensionality determination. On the over hand, Mixture Models are a powerful model-based clustering technique, especially coupled with nonparametric techniques.&lt;/p&gt;

&lt;p&gt;This approach is extremely powerful, intellectually fascinating and vibrant. It is also in the spirit of economic complexity, with ideas which bridge together different branches in data analysis. The catch is, that there are enormous barriers for beginners to start developing such models. First, a qualitative jump in understanding of probability theory and stochastic processes, bayesian inference and theory of statistical learning is needed. Another barrier is how to go from understanding the models towards the implementation in actual software products.&lt;/p&gt;

&lt;p&gt;So, the main goal is to find connections between the three perspectives described before, to recognize Bayesian generalizations of classical machine learning models. The most important is the process of exploration and understanding, internalization of models, which will suggest new areas of study and practical opportunities, like learning STAN and improving R modeling skills.&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Bizovi-Mihai-Sto&#34;&gt;Bizovi Mihai - Stochastic Modeling and Bayesian Inference (2017, Thesis) &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Bizovi-Mihai-Sto&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Dynamic Modeling (wip)</title>
      <link>https://bizovi.github.io/project/probabilistic-dynamic-modeling/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/project/probabilistic-dynamic-modeling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Time Series Clustering (wip)</title>
      <link>https://bizovi.github.io/project/ts-clustering/</link>
      <pubDate>Sun, 06 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/project/ts-clustering/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Thesis: Stochastic Modeling and Bayesian Inference</title>
      <link>https://bizovi.github.io/project/probabilistic-machine-learning/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/project/probabilistic-machine-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New Economic Thinking for a Knowledge-Based Society</title>
      <link>https://bizovi.github.io/project/new-economic-thinking/</link>
      <pubDate>Tue, 03 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://bizovi.github.io/project/new-economic-thinking/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
