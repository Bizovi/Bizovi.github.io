<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>{Art, Math, Philosophy}</title>
<link>https://blog.economic-cybernetics.com/</link>
<atom:link href="https://blog.economic-cybernetics.com/index.xml" rel="self" type="application/rss+xml"/>
<description>{Art, Math, Philosophy}</description>
<generator>quarto-1.6.42</generator>
<lastBuildDate>Sun, 09 Apr 2023 21:00:00 GMT</lastBuildDate>
<item>
  <title>10 Lessons in Humility</title>
  <dc:creator>Bizovi Mihai</dc:creator>
  <link>https://blog.economic-cybernetics.com/posts/2023-rules/principles.html</link>
  <description><![CDATA[ 





<p>I was trying to articulate a philosophy for the courses I teach and accidentally wrote 10 non-exhaustive principles for life, which are also lessons in humility. It will not be surprising if the humble wonder practice by John Vervaeke shaped these ideas.</p>
<ul>
<li>There is no shortcut to <strong>deep understanding</strong>
<ul>
<li>Of a domain, especially in an <strong>interdisciplinary</strong> setting</li>
<li>With communities engaged in an evolving <strong>dialogue</strong></li>
</ul></li>
<li>There is no shortcut to being <strong>skillful</strong> at something</li>
<li>The journey from novice to expert is <strong>not linear</strong>, however, the “interest compounds”</li>
<li>The journey need not be painful, but it can be <strong>seriously playful</strong>, a source of wonder and meaning</li>
<li>Without <strong>skin in the game</strong>, we can’t claim we truly get something</li>
<li>Without a <strong>vision</strong> which is flexible enough, but at the same time long-lived:
<ul>
<li>In the case of <strong>rigidity</strong> - there is a risk of being stuck, pursue obsessively, counterproductively the wrong thing</li>
<li>In the case of <strong>everything goes</strong> - there is a risk of wandering aimlessly and not finding a home</li>
</ul></li>
<li>Fixating on beliefs and <strong>propositional knowing</strong> (the facts!) is counterproductive. Which should put into question all written above</li>
<li>Fixating on <em>skills</em> makes you lose the grasp of the <strong>big picture</strong></li>
</ul>
<section id="john-vervaekes-humble-wonder" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="john-vervaekes-humble-wonder">John Vervaeke’s Humble Wonder</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://blog.economic-cybernetics.com/posts/2023-rules/socrates.jpeg" class="img-fluid" style="width:100.0%"></p>
<p>Check out John Vervaeke’s After Socrates, <a href="https://youtu.be/erxLwlk6RCQ?t=4276">Lecture 2</a>.</p>
</div></div><p>Practicing the Socratic learned ignorance about the self:</p>
<ul>
<li>There is so much I do not know about myself because of all of the [combinatorially explosive] facts.</li>
<li>There is so much I shall never know about myself because of all of the fate.</li>
<li>There is so much I refuse to see about myself because of all of my foolishness.</li>
<li>There is so much I am unable to see about myself because of all of my faults.</li>
</ul>
<p>Practicing the learned ignorance about the world:</p>
<ul>
<li>There is so much I do not know about the world because of all of the facts.</li>
<li>There is so much I shall never know about the world because of all of the fate.</li>
<li>There is so much I refuse to see about the world because of all of my foolishness.</li>
<li>There is so much I am unable to see about the world because of all of my faults.</li>
</ul>


</section>

 ]]></description>
  <category>philosophy</category>
  <guid>https://blog.economic-cybernetics.com/posts/2023-rules/principles.html</guid>
  <pubDate>Sun, 09 Apr 2023 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Statistical Analysis of Facebook Network of Friends</title>
  <dc:creator>Bizovi Mihai</dc:creator>
  <link>https://blog.economic-cybernetics.com/posts/2019-networks/networks.html</link>
  <description><![CDATA[ 





<p>The goal of this project is to get some hands-on experience in working with Graphs and Network Data based on a dataset from my personal life. It will involve some theoretical understanding and programming.</p>
<p>Graph Theory is a fascinating subject which I studied before, but as I found out – the understanding remains, but the skill perishes if left untrained or unused. Besides getting comfortable with this type of data, there is an opportunity to build the ground for future research<sup>1</sup>. Moreover, ideas from graph theory can be useful in the field of Machine Learning, with the emergence of Graph Neural Networks.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Paul Omerod <em><a href="http://evonomics.com/the-future-of-economics-uses-the-sciiece-of-real-life-social-networks/">The Future of Economics Uses the Science of Real-Life Social Networks</a></em> (Evonomics, 2016)</p></div></div><section id="crawling-the-data" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="crawling-the-data">Crawling the data</h2>
<p>In search of a suitable dataset for statistical analysis of social networks, I stumbled upon many interesting applications, from the classical Karate Club to Email, Citations of Scientific Articles, Transportation and Biology. Unfortunately, there is a tradeoff between how manageable it is to work with the data in terms of volume and how interesting are the patterns to be discovered. The most promising way seemed to explore the scientific literature or knowledge graphs like DBPedia, but it’s quite a difficult and heavy task.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/FB_Social_Network.png" class="img-fluid figure-img"></p>
<figcaption>Facebook Social Network extracted though lostcircles. The shadowed area over clusters represent different communities like the place I work at, chess, ASE and Moldavian diaspora. The red circles represent people connecting different communities or central figures for a given cluster.</figcaption>
</figure>
</div>
<p>This is why I extracted my own network of Facebook friends<sup>2</sup>, because I knew there are a few heterogeneous communities and the results could be interesting. It turned out even more surprising and insightful when applying the algorithms for the layout and node importance. <strong>The immediate contribution</strong> is that this anonimized dataset can be used for teaching and academic purposes, testing of algorithms on graphs.</p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Lostcircles.com <em><a href="https://lostcircles.com">A scraper for Facebook friend network</a></em> (Lostcircles, 2017)</p></div></div><p>Moreover, it enables us to validate the results of algorithms with the actual knowledge of the communities and “important” people in it. First, I use the default visualization of the crawler and encode the knowledge I have about the network on the graph in the next page. Then, using <strong>python</strong> and <strong>networkx</strong> package to visualize it, but this time by calculating importance metrics via <strong>PageRank</strong>, <strong>Hits</strong>, the central and peripheral nodes according to the definition.</p>
<p>The observation is that the overlap between statistical insights and “ground truth” is quite large. Because the network is real and not idealized, some “hubs”, i.e.&nbsp;people connecting different clusters aren’t identified. Instead, it finds important people within the clusters, and interpretation of this will be discussed later.</p>
</section>
<section id="interpretation" class="level2">
<h2 class="anchored" data-anchor-id="interpretation">Interpretation</h2>
<p>The first graph is quite complicated, so let’s review the <strong>communities</strong> which are clearly seen and shaded for clarity. The top cluster represents people from my highschool in Chisinau, Moldova. They are connected by a chess player to the quite large chess community of which I took part for many years. The chess cluster in turn is connected to my family, which joined later and are also interested in this game.</p>
<p>Moreover, the network doesn’t have timestamps, but hypothetically, if I had them, I could tell how the network developed to its final state. It is by no means an impediment of telling the story. We can move on to the times I moved to Bucharest for CSIE bachelor, and we see within the central, big cluster several sub-communities, the first one being the foreign students, with which other foreign students, of course interact a lot. The second part, people from university are pretty much the same during the Bachelor and Masters. The person connecting the highschool, diaspora and university is my roommate and chess with CSIE a grandmaster who is also a professor.</p>
<p>Another source of connections are different conferences, summer schools and workshops, for example in Risk Management, Mathematics and Data Science. These nodes are not differentiated too well in the network, but the fact that the algorithm placed them in the same neighborhood is very insightful. Inside the Cybernetics Masters’, the red point, also confirmed by PageRank is the most <strong>central</strong> person, the chief of students’ senate.</p>
<p>Moving next, we see a sparsely connected region of PhD students and Teaching Assistants. They, and the Cybernetics cluster are connected to the firm I’m working with by a professor who was a consultant at our firm. The last community is the Bucharest Machine Learning and Tech people, who use to be very active in meetups and conferences inside the capital.</p>
</section>
<section id="analysis-in-networkx" class="level2">
<h2 class="anchored" data-anchor-id="analysis-in-networkx">Analysis in NetworkX</h2>
<p>Now we are armed with the knowledge and interpretation of this graph and can start with a blank slate, using the raw data, which is stored in a <strong>json</strong> format <code>{’source’: 476, ’target’: 87}</code>, later transformed in an <strong>edge list</strong> and finally a <strong>networkx</strong> <code>Graph()</code> object. Now we can visualize this object in python, using different layout algorithms, calculate statistics and starting exploring the dataset as if we knew nothing about it. The node labels are numbers in order to anonimize the data, without much loss of insight.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> numpy <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> np</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pandas <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> pd</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> json</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> networkx <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> nx</span>
<span id="cb1-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> pprint <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pprint</span>
<span id="cb1-7"><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">%</span>matplotlib inline</span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">open</span>(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bizovi.json'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'rb'</span>) <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> f:</span>
<span id="cb1-10">    data <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.load(f)</span>
<span id="cb1-11">    </span>
<span id="cb1-12">df <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> pd.DataFrame([(x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"source"</span>], x[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"target"</span>]) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> data[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"links"</span>]], </span>
<span id="cb1-13">    columns<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>[<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n1"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"n2"</span>])</span>
<span id="cb1-14">df.head()</span>
<span id="cb1-15"></span>
<span id="cb1-16">G <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.from_pandas_edgelist(df, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'n1'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'n2'</span>)</span></code></pre></div>
<p>Classical measures of centrality like degree, average clustering, betweenness centrality, despite their simplicity and power they all have the downside of how complex of patterns can they represent or suggest. This is where iterative, algorithmic approaches like Scaled PageRank and HITS come into play. The first one can be interpreted as the probability of ending up in a node when traversing the graph by a random walk a <strong>sufficiently long</strong> time. Hits uses another approach with the concepts of Authority and Hubs. It is important to note that they both were developed for directed graphs in the context of web pages and hyperlinks pointing to them and they show their full power there. Nonetheless, they can still be used for undirected graphs, as I did here.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.hist(nx.pagerank(G).values(), bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.5'</span>)</span>
<span id="cb2-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PageRank"</span>)</span>
<span id="cb2-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Count"</span>)</span>
<span id="cb2-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distribution of the PageRank Score"</span>)</span>
<span id="cb2-5">plt.show()</span>
<span id="cb2-6"></span>
<span id="cb2-7">pagerank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.pagerank(G, alpha<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.8</span>)</span>
<span id="cb2-8">high_rank <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [key <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> key, val <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> pagerank.items() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0042</span>]</span>
<span id="cb2-9"></span>
<span id="cb2-10">hits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.hits(G, max_iter<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1000</span>)</span>
<span id="cb2-11">_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.hist(hits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].values(), bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.5'</span>)</span>
<span id="cb2-12">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Authority Score"</span>)</span>
<span id="cb2-13">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Count"</span>)</span>
<span id="cb2-14">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Hits"</span>)</span>
<span id="cb2-15">plt.show()</span>
<span id="cb2-16"></span>
<span id="cb2-17">high_hits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [key <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> key, val <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> hits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>].items() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> val <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.005</span>]</span></code></pre></div>
<p>First, I apply the algorithms and then look at the distribution of node “importance” in order to identify a good treshold for the visualization. The important nodes are then stored into <code>high_rank</code> and <code>high_hits</code> variables. The parameter <strong>alpha</strong> is essential for the PageRank not to get stuck into loops and distorting the distribution.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[40, 40]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/pagerank.png" class="img-fluid figure-img"></p>
<figcaption>Scaled PageRank</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/hits.png" class="img-fluid figure-img"></p>
<figcaption>Hits</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><em>The distribution of node importance according to PageRank and Hits algorithms. HITS needed a much larger number of iterations to converge and is much more <code>strict</code> in what it considers to be important nodes, most of them being concentrated around zero. For this particular network, PageRank scores seem a bit more sensible overall, but HITS differentiates better between importance.</em></p>
<p>As a further warm-up, let’s calculate a few statistics on this graph, which interpretation we will discuss later. The two statistics which can vary greatly in conclusions by network type, in this case pretty much agree: the <strong>average clustering</strong> (0.49) and <strong>transitivity</strong> (0.45). The central and peripheral nodes will be displayed directly on the graph. The <strong>diameter</strong> of the network is 8, <strong>radius</strong> is 4 and the <strong>average shortest path</strong> between any node to other node is 3.26.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">periphery <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.periphery(G)</span>
<span id="cb3-2">center <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.center(G)</span>
<span id="cb3-3">min_node_cut <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.minimum_node_cut(G)</span>
<span id="cb3-4">nx.average_clustering(G), nx.transitivity(G)</span>
<span id="cb3-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(nx.average_shortest_path_length(G))</span>
<span id="cb3-6"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(nx.diameter(G))</span></code></pre></div>
<p>Now it is time to visualize the network and add the nodes which were calculated above according to criteria of importance. It is important to note that <strong>networkx</strong> is NOT primarily a visualization of network package: it uses the standard matplotlib, but it is good enough for small networks such as this one. In just 12 lines of code we get a publication-ready visualization. For more advanced use-cases <strong>graphviz</strong> and <strong>Gephi</strong>, <strong>igraph</strong> in R can be more suitable. In the next part, these repetitive operations will be wrapped up in a generic function which automates this a little bit painful process.</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">np.random.seed(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3245</span>)</span>
<span id="cb4-2">legend_elements <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [Line2D([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wheat'</span>, </span>
<span id="cb4-3">        markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>, label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Periphery'</span>),</span>
<span id="cb4-4">    Line2D([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'steelblue'</span>, </span>
<span id="cb4-5">        label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'PageRank'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), </span>
<span id="cb4-6">    Line2D([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'indianred'</span>, </span>
<span id="cb4-7">        label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Centrality'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>), </span>
<span id="cb4-8">    Line2D([<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>], marker<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'o'</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, </span>
<span id="cb4-9">        label<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Min Node Cut'</span>, markersize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">10</span>)]</span>
<span id="cb4-10"></span>
<span id="cb4-11">plt.figure(figsize<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">13</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">11</span>))</span>
<span id="cb4-12">plt.axis(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"off"</span>)</span>
<span id="cb4-13">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Network of Friends. Fruchterman Reigold Layout"</span>)</span>
<span id="cb4-14">pos <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> nx.fruchterman_reingold_layout(G)</span>
<span id="cb4-15">nx.draw_networkx(G, node_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".3"</span>, pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pos, node_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">60</span>, </span>
<span id="cb4-16">    with_labels<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">False</span>, edge_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">".8"</span>)</span>
<span id="cb4-17">nx.draw_networkx_nodes(periphery, pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pos, </span>
<span id="cb4-18">    node_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'wheat'</span>, node_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb4-19">nx.draw_networkx_nodes(high_rank, pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pos, </span>
<span id="cb4-20">    node_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'steelblue'</span>, node_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>)</span>
<span id="cb4-21">nx.draw_networkx_nodes(center, pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pos, </span>
<span id="cb4-22">    node_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'indianred'</span>, node_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>)</span>
<span id="cb4-23">nx.draw_networkx_nodes(min_node_cut, pos<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>pos, </span>
<span id="cb4-24">    node_color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'green'</span>, node_size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">90</span>)</span>
<span id="cb4-25"></span>
<span id="cb4-26">plt.legend(legend_elements, [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Periphery"</span>, </span>
<span id="cb4-27">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"PageRank"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Centrality"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Min Node Cut"</span>])</span>
<span id="cb4-28">plt.show()</span></code></pre></div>
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/python_facebook.png" class="img-fluid" alt="Python Social Network"> <em>The final result of applying classical algorithms turned out extremely close to the <code>ground truth</code>, i.e.&nbsp;the interpretation I gave in the beginning</em></p>
<p>The network is <strong>reversed</strong>, but even cleaner than the one generated by lostcircles. We can see on the graph on the next page that the periphery is not that large as expected. Three, or even four important people who connect clusters are correctly identified either by <strong>centrality</strong> or PageRank algorithm. Moreover, it identifies very well connected people inside the clusters.</p>
<p>For the minimal node cut there are multiple choices, but it seems that the algorithm “thinks” that the family is the most poorly connected cluster to the other ones, which, looking at the picture is absolutely reasonable, as we have to remove only one node to achieve a disconnection, the green one.</p>
<p>One a side note, good graph datasets for teaching are relatively rare, and with closure of Facebook API and LinkedIn API, researchers working with network data may stick with the old datasets. This one seems interesting enough for tutorials and projects.</p>
<p>A few comments on what the code does: first it sets up the environment, defining the size of the picture and deleting the axis, then it calculates the position of each node by the Fruchterman Reigold Algorithm. The first layer of grey nodes is added, together with all of the connections. Then the peripheral nodes calculated above are displayed in beige with the function <code>nx.draw_networkx_nodes</code>, which also requires the position, the Centrality, Minimum Cut and PageRank are added in turn. The finishing touch is the legend.</p>
<p>The last two obvious things to try are calculating the Local Clustering Coefficient and seeing the correlation between Node degree and LCC. We see that the first follows a pretty symmetric distribution, with exception of the stack at zero, which is expected in such a network. It means that there are nodes which are highly clustered and some not at all, but the majority at around 0.5.</p>
<p>The correlation with the Node Degree shows that as node degree increases, the expected LCC drops. This relationship is usually visualized at a log scale, but our network is not enough for this to be necessary and for the pattern to emerge. There is also an interesting boundary on the top of the point cloud. Remember how we calculated the average clustering above (0.49) and it agreed in large with the alternative method of transitivity, which is based on the concept on “triangle closure”. It means that networks, in which pairs of 2 out of 3 nodes are connected, tend to “produce” closures within those triplets.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout="[40, 40]">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/lcc.png" class="img-fluid figure-img"></p>
<figcaption>LCC</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2019-networks/img/correlation.png" class="img-fluid figure-img"></p>
<figcaption>Correlation</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><em>Two classical measures of clustering</em></p>
<p>Below is the code which uses list comprehensions and nx functions on the graphs in order to compute and plot the distributions of the degree and LCC.</p>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.hist(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(nx.clustering(G).values()), bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.5'</span>)</span>
<span id="cb5-2">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Local clustering coefficient"</span>)</span>
<span id="cb5-3">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Count"</span>)</span>
<span id="cb5-4">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distribution of the Local clustering coefficient"</span>)</span>
<span id="cb5-5">plt.show()</span>
<span id="cb5-6"></span>
<span id="cb5-7">plt.scatter([x[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(nx.degree(G))], </span>
<span id="cb5-8">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(nx.clustering(G).values()))</span>
<span id="cb5-9">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Node Degree"</span>)</span>
<span id="cb5-10">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Local Clustering Coefficient"</span>)</span>
<span id="cb5-11">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distribution of the Node Degree"</span>)</span>
<span id="cb5-12">plt.show()</span>
<span id="cb5-13"></span>
<span id="cb5-14"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Bonus: Node degree distribution</span></span>
<span id="cb5-15">_ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> plt.hist([x[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>] <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> x <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">list</span>(nx.degree(G))], bins<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">30</span>, color<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'.5'</span>)</span>
<span id="cb5-16">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Node Degree"</span>)</span>
<span id="cb5-17">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Count"</span>)</span>
<span id="cb5-18">plt.title(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Distribution of the Node Degree"</span>)</span>
<span id="cb5-19">plt.show()</span></code></pre></div>
</section>
<section id="research-citations-and-ontologies" class="level2">
<h2 class="anchored" data-anchor-id="research-citations-and-ontologies">Research, Citations and Ontologies</h2>
<p>The techniques and algorithms applied on these toy examples can be and have been successfully used in discovering patterns of citation networks for research pages, knowledge graphs, blogs and web pages. It turns out to be an extremely important tool for discovery and even recommendation, as a researcher or reader has limited time and capacity, it is reasonable to suggest the most relevant or important research on her topic.</p>
<p>Also, papers which do meta-analyses, combined with Natural Language Processing techniques are extremely interesting, as they attempt to draw the landscape of research, opinions and topics. It can also be applied to News in real time, which was successfully accomplished by Marko Grobelnik and his group in Slovenia.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>As a data scientist/ machine learning researcher it was useful to step outside the (statistical) approaches on cross-sectional and panel data with which we usually work and explore applications driven by the network data.</p>
<p>We looked at a fun and practical example of analysing a network of friends by applying some classical algorithms on it, managing to characterize quite a large part of its features and structure. A future area of improvement and research would be to see how such networks evolve in time, by enriching the data with timestamps of <code>friendship requests acceptance</code>. In this process, I learned a lot about practical statistical analysis of graph data and hope to pass this hands-on experience to the readers.</p>


</section>


 ]]></description>
  <category>networks</category>
  <guid>https://blog.economic-cybernetics.com/posts/2019-networks/networks.html</guid>
  <pubDate>Tue, 08 Jan 2019 22:00:00 GMT</pubDate>
</item>
<item>
  <title>Understanding Caratheodori Extension Theorem</title>
  <dc:creator>Bizovi Mihai</dc:creator>
  <link>https://blog.economic-cybernetics.com/posts/2018-intro-measure/measure_theory.html</link>
  <description><![CDATA[ 





<p>The reason we need probability theory is that it’s a formal language of uncertainty. Even though you can go a long way as a practitioner with standard tools in probability theory, deeply understanding its <strong>measure-theoretic</strong> foundations could open up a whole new world to the researcher. It’s easy to take the results from statistics and probability for granted, but it’s useful to be aware what hides beneath the surface.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In order to build adequate models of economic and other complex phenomena, we have to take into account their inherent <em>stochastic nature</em>. Data is just the appearance, an external manifestation of some <code>latent processes</code> (seen as random mechanisms). Even though we won’t know the exact outcome for sure, we can model general regularities and relationships as a result of the large scale of phenomena. For more ideas see <span class="citation" data-cites="Ruxanda2011">(Ruxanda 2011)</span></p>
</div></div><p>As an economist and modeler, the goal of this project is to step-by-step explain the <code>Caratheodori Extension Theorem</code> in order to understand the language, gain some intuition and insight about it. The value of studying it comes not from the theorem itself, but from the process of discovery and understanding a proof forces you to go through. <span class="citation" data-cites="Landim2016">(Landim 2016)</span> If you’re a mathematician and for some reason reading this, you might ask:</p>
<blockquote class="blockquote">
<p>“Are you insane going into it without any knowledge of real analysis”?</p>
</blockquote>
<p>The answer is that I want to learn the language, and not to achieve excellence in Measure Theory. Also, I can hardly take things for granted and need a justification of why things (in probability) are done exactly this way. <span class="citation" data-cites="Rosenthal2006">(Rhosental 2006)</span> We’ll develop a plan of attack and a network of ideas and concepts that need to be understood in order to tackle the problem, hoping that resourcefulness and intuitions will compensate for the lack of rigor.</p>
<section id="motivation-for-measure-theory-in-a-practice-oriented-world" class="level2">
<h2 class="anchored" data-anchor-id="motivation-for-measure-theory-in-a-practice-oriented-world">Motivation for measure theory in a practice-oriented world</h2>
<p>The field of Data Mining moved a long way, becoming accessible and bringing value for individuals and industries. A lot of Machine Learning and Statistical models are available with a few lines of code. If it should be obvious why we need probability theory, it’s not so with measure theory.</p>
<blockquote class="blockquote">
<p>See the Andrew Gelan (a giant of Bayesian statistics) and Cosma Shalizi (an expert in Data Mining) disagreement on the <a href="http://andrewgelman.com/2008/01/14/what_to_learn_i/">subject</a></p>
</blockquote>
<p>I’ll give an analogy: even though the models can be easily applied in a high-level language like R or Python, understanding the Learning Theory can bring you on another level, closer to excellence. In the case of measure theory, some argue that there are alternative things to study which can bring more value, and they’re not wrong, but for an ambitious field like Bayesian Nonparametrics, it’s hard to make even little progress because of the understanding barrier. This is why I like to think of it as a language <span class="citation" data-cites="Lawrence2012">(Lawrence 2012)</span> extremely useful in the fields of stochastic processes and learning theory. So, the truth is somewhere in between and key to learning these subjects is a personally optimal balance of theoretical understanding, practice on real data problems and simulation exercises.</p>
<p>Studying measure theory might look like a gruesome process to do on your own, but it makes sense posing a reverse question: what do I need to know to understand all these awesome papers where there is a urge to ask: <strong>“The probability over what?”</strong></p>
<blockquote class="blockquote">
<p>A personal experience was in an attempt to study nonlinear state-space models, where there are some exciting papers on Bayesian Nonparametrics and Stochastic Filtering. Reading and working through papers felt like missing a good part of the story, because of some lacking fundamentals. It’s extremely important to recognize what you don’t know. That’s right, I want to be able to formulate meaningful statements about distributions of more abstract objects, like functions, graphs, etc and to reason about stochastic processes.</p>
</blockquote>
<p>For example, having a great understanding of probability helps to define in a clear and rigorous way difficult concepts used in statistics and econometrics (which might look deceptively simple at first) as p-values, confidence intervals, power, hypothesis testing and helps avoiding a lot of confusion. Let’s take the idea of power of the test in its simplest form (for a one-sided Z test), which a lot of practitioners struggle to define when asked.</p>
<blockquote class="blockquote">
<p>The mathematical formulation uncovers some of the assumptions we’re making and suggests the interpretation. Notice how <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cmu_a%20-%20%5Cmu_0%7D%7B%5Csigma%7D"> is a proxy for a “unit free” effect size.</p>
</blockquote>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbeta"> Type II error: Failure to reject <img src="https://latex.codecogs.com/png.latex?H_0"> when it’s false</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Calpha"> Type I error: Falsely rejecting a true <img src="https://latex.codecogs.com/png.latex?H_0"></li>
<li><img src="https://latex.codecogs.com/png.latex?1%20-%20%5Cbeta"> is the power</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu_0"> the null hypothesis</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmu_a"> the alternative hypothesis</li>
</ul>
<p>The response of the power being the probability of rejecting a null hypothesis when it’s false might not suggests that there is much going on.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign*%7D%0A%20%20%20%201-%20%5Cbeta%20&amp;=%20%5Cmathbb%7BP%7D%20%5Cbigg(%20%5Cfrac%7B%5Cbar%7BX%7D%20-%20%5Cmu_0%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D%20%3E%0A%20%20%20%20%5Cmathbf%7BZ%7D_%7B1-%5Calpha%7D%20%20%5Cbigg%5Clvert%20%5Cmu%20=%20%5Cmu_a%20%20%5Cbigg)%20%5C%5C%0A%20%20%20%20~%20&amp;=%20%5Cmathbb%7BP%7D%20%5Cbigg(%20%5Cfrac%7B%5Cbar%7BX%7D%20-%20%5Cmu_a%20+%20%5Cmu_a%20-%20%5Cmu_0%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D%20%3E%0A%20%20%20%20%5Cmathbf%7BZ%7D_%7B1-%5Calpha%7D%20%20%5Cbigg%5Clvert%20%5Cmu%20=%20%5Cmu_a%20%20%5Cbigg)%20%5C%5C%0A%20%20%20%20~%20&amp;=%20%5Cmathbb%7BP%7D%20%5Cbigg(%20%5Cfrac%7B%5Cbar%7BX%7D%20-%20%5Cmu_a%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D%20%3E%0A%20%20%20%20%5Cmathbf%7BZ%7D_%7B1-%5Calpha%7D%20-%20%5Cfrac%7B%5Cmu_a%20-%20%5Cmu_0%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D%20%5Cbigg%5Clvert%20%5Cmu%20=%20%5Cmu_a%20%20%5Cbigg)%20%5C%5C%0A%20%20%20%20~%20&amp;=%20%5Cmathbb%7BP%7D%20%5Cbigg(%20%5Cmathbf%7BZ%7D%20%3E%0A%20%20%20%20%5Cmathbf%7BZ%7D_%7B1-%5Calpha%7D%20-%20%5Cfrac%7B%5Cmu_a%20-%20%5Cmu_0%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D%20%5Cbigg%5Clvert%20%5Cmu%20=%20%5Cmu_a%20%20%5Cbigg)%0A%5Cend%7Balign*%7D"></p>
<p>But there is a lot going on, the power depending on the effect size, assumed level for the type I error and the sample size. The distribution of the term <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BZ%7D%20=%20%5Cfrac%7B%5Cbar%7BX%7D%20-%20%5Cmu_a%7D%7B%5Csigma%20/%20%5Csqrt%7Bn%7D%7D"> is actually the one under the alternative hypothesis. Note that power calculations done post-hoc are usually a terrible idea.</p>
<p>Also, it’s almost impossible to sense the dangers of interpretations of <strong>p-values</strong>, types of errors and <strong>confidence intervals</strong> without trying to understand the mathematics behind statistical testing. Tests are also models, little <strong>“Golemns of the Prague”</strong> and they might fail in unexpected ways when the assumptions do not hold. <span class="citation" data-cites="McElreath">(McElreath 2015)</span></p>
<p>As it’s often the case in mathematics, things have a deep justification behind them and even though you can successfully apply the models in practice, understanding is what separates a great modeler. Often, a breakthrough comes in the form of something that nobody have thought before.</p>
<blockquote class="blockquote">
<p>I think we’ll appreciate the input from mathematicians more in an applied field like Data Mining, as it will help figure out why deep neural networks work so well. Same is true for <em>Extreme Gradient Boosting</em> and other things that just seem to work. It took me some time trying to solve real world problems, in order to appreciate the usefulness of deeply understanding different ideas in mathematics.</p>
</blockquote>
<p>This is why, in the path to mastery of machine learning, certain topics appear which might change your perspective forever. One of these is Measure Theory. Is it useful in practice? Probability Theory taught in undergraduate courses might be what most people need, but it’s limited in a certain sense, imposing an <em>artificial dichotomy between discrete and continuous random variables</em> and thinking in terms of probability density functions and cumulative distribution functions.</p>
<p>Regarding (Caratheodori), it’s not the formulation of the theorem which brings the most insight, but ideas in the proof as measurable sets and outer measures.</p>
</section>
<section id="measure-theory-in-machine-learning" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="measure-theory-in-machine-learning">Measure Theory in Machine Learning</h2>
<blockquote class="blockquote">
<p>Some courses will mention it, but as a side for the mathematically inclined students and not appearing anywhere later</p>
</blockquote>
<p>In undergraduate probability we can get away with the lack of measure-theoretic notions, as we’re working on real spaces, continuous functions and the instruments we have in these tame cases seem enough. There are also wilder cases, in which we need new tools and language to be rigorous, as otherwise we would just hope for the best (that the probability measure is defined). In some of the fields mentioned above researchers have to deal with weird stuff like distributions which have continuous and discrete elements, when a mixture of a density with point masses isn’t very helpful to work with.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/measure_theory_comics.jpg" class="img-fluid"> Source: <a href="http://brownsharpie.courtneygibbons.org">brownsharpie</a></p>
</div></div><p>Evans Lawrence gives the following example of a function which is neither discrete nor continuous, for which you flip a coin and if it comes heads, draw from an uniform distribution and in case of tails a unit mass at one. If <img src="https://latex.codecogs.com/png.latex?%5Cchi_%7B%5B0,1%5D%7D(x)%20=%20(e%5E%7Bix%7D%20-%201)/ix"> is the characteristic function of the interval from zero to one, in a way you can formulate its density, but usually it’s not the case, nor is it very helpful to think about it in such terms.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20p(x)%20=%20w_1%20%5Cchi_%7B%5B0,1%5D%7D(x)%20+%20%20w_2%5Cdelta_1(x)%0A%5Cend%7Bequation%7D"></p>
<p>Even though you can visualize this in two dimensions as the uniform and a spike, or as a CDF with a discontinuity, this approach just breaks down in higher dimensions or more complicated combinations of functions.</p>
<p>Jeffrey Rosenthal begins his book <span class="citation" data-cites="Rosenthal2006">(Rhosental 2006)</span> by a similar motivation, constructing the following random variable as a coin toss between a discrete <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20Pois(%5Clambda)"> and continuous <img src="https://latex.codecogs.com/png.latex?Y%20%5Csim%20%5Cmathcal%7BN%7D(0,1)"> r.v.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20Z%20=%20%5Cbegin%7Bcases%7D%0A%20%20%20%20X,%20p%20=%200.5%20%5C%5C%0A%20%20%20%20Y,%20p%20=%200.5%0A%20%20%20%20%5Cend%7Bcases%7D%0A%5Cend%7Bequation%7D"></p>
<p>He then challenges the readers to come up with the expected value <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5BZ%5E2%5D"> and asks on what is it defined? It is indeed a hard question.</p>
<p>It is not surprising for me that measure theory becomes important in the Learning Theory, even though lighter courses from which I studied don’t mention it explicitly (Yaser Abu-Mostafa, Shai Ben-David, Reza Shadmehr). According to Mikio’s Brown <a href="https://www.quora.com/Is-Measure-Theory-relevant-to-Machine-Learning/answer/Mikio-L-Braun?srid=KONR">answer</a> it’s essential in the idea of <strong>uniform convergence</strong> and its bounds, where <em>“you consider the probability of a supremum over an infinite set of functions, but out of the box measure theory only allows for constructions with countably infinite index sets”</em>.</p>
<p>If we’re thinking about a regression from the nonparametric perspective <img src="https://latex.codecogs.com/png.latex?f(x)%20%5Cin%20%5Cmathscr%7BC%7D%5E2:X%20%5Crightarrow%20%5Cmathbb%7BR%7D">, we might want to know how a draw from a (infinite) set of continuous differentiable functions might look like. The questions arises: how to define a PDF in this space? In my thesis <span class="citation" data-cites="Bizovi">(Mihai 2017)</span> I got away with using Gaussian Processes, which are a very special class of stochastic processes. In this special case I could informally define an apriori distribution by defining the mean vector and Kernel (covariance function), then condition it on observed data with a Normal Likelihood.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/prior.png" class="img-fluid"> An example of reasoning about distributions of random functions from my Thesis. The prior distribution</p>
</div><div class="">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/conditionare_gp.png" class="img-fluid"> Only the functions that explain the data well survive</p>
</div></div>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0Ap(f(x)%20%5C,%20%7C%5Cleft%20%5C%7B%20x%5Cright%20%5C%7D)=%5Cfrac%7Bp(%5Cleft%20%5C%7B%20x%5Cright%20%5C%7D%7C%20%5C,%20f)%20%5C,%20%5Cmathbf%7Bp(f)%7D%7D%7Bp(%5Cleft%20%5C%7B%20x%5Cright%20%5C%7D)%7D%0A%5Cend%7Bequation%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation*%7D%0Af(x)%20%5Csim%20GP(%5Cmu(x);%20K(x,x'))%0A%5Cend%7Bequation*%7D"></p>
<p>The result was that only the functions that explained the data well survived. While this reasoning makes intuitive sense, there are things “swiped under the carpet”. If we were to model stock prices, where there are jumps and the process itself is less smooth, measure theory would be very hard to avoid. If we would want to reason in terms of densities, ask with respect to what? So, the focus is shifted towards the question of what is the probability of every possible event. This leads us back to the fundamental object of state (outcome) space <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7B%5COmega%7D">.</p>
<blockquote class="blockquote">
<p>Each element <img src="https://latex.codecogs.com/png.latex?%5Comega_i%20%5Cin%20%5Cmathbf%7B%5COmega%7D"> is an elementary event (outcome), while <img src="https://latex.codecogs.com/png.latex?A%20%5Csubset%20%5Cmathbf%7B%5COmega%7D"> is an event</p>
</blockquote>
<p>As we will shortly see, it’s impossible to define the probability (measure) on the set of all subsets <img src="https://latex.codecogs.com/png.latex?2%5E%5COmega">, except for the simple finite cases, without having to let go of a fundamental axiom like countable additivity. This will be the first step in getting closer to defining a Uniform distribution on <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D">.</p>
<blockquote class="blockquote">
<p>Surprisingly, to rigorously define an Uniform distribution is not a trivial task, because of the mentioned above impossibility, proved later by contradiction. In contrast, Caratheodori theorem allows us to do exactly that</p>
</blockquote>
<p>Tarun Chitra in the same thread argues that many classification problems are ill-posed mathematically, and the ones which can be formulated in a measure-theoretic way have very nice results, like SVMs with Reproducing Kernel Hilbert Spaces, where you cannot apply the Mercer’s Theorem unless the Kernel is measurable. The second example he gives is proving some results about Stochastic Gradient Descent, an optimization algorithm often successfully used, which has connection with Brownian Motion, thus Weiner measures.</p>
<p>Measure theory is also important in rigorously defining distances and divergences (for example between two distributions as in Kullback-Leiber)</p>
</section>
<section id="measure-theory-and-the-fundamentals-of-probability-theory" class="level2">
<h2 class="anchored" data-anchor-id="measure-theory-and-the-fundamentals-of-probability-theory">Measure Theory and the fundamentals of Probability Theory</h2>
<p>It is useful to step back and see where does Measure Theory fit in the framework of Probability Theory. The following list will be a summary of a lecture at doctoral school by <span class="citation" data-cites="Ruxanda2017">(Ruxanda 2017)</span> Here are 8 steps to mastery of the basics by Gheorghe Ruxanda:</p>
<ol type="1">
<li>A <strong>random experiment</strong> (<img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BE%7D">) is a set of <em>conditions which are favorable for an event</em> in a given form with the following properties:
<ul>
<li>Possible results are known apriori</li>
<li>It’s never known which of the results of <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BE%7D"> will exactly appear</li>
<li>Despite (b), <strong>there is a perceptible regularity</strong>, (encoding the idea of a probabilistic “law”) in the results. Also, it could be as a result of the large scale of the phenomena.</li>
<li>Repeatability of the conditions, i.e.&nbsp;the comparability and perservation of context are key.</li>
</ul></li>
<li><strong>Elementary event</strong> as an auxiliaty construction: one of the possible results of <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BE%7D">, <img src="https://latex.codecogs.com/png.latex?%5Comega_i%20%5Cin%20%5COmega"></li>
<li><strong>Universal set</strong> <img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5C%7B%20%5Comega_1,%20%5Comega_2,%20%5Cdots%20%5C%7D"> Also called (Outcome/ State/ Selection space), it suggests the idea of complementarity and stochasticity: we don’t know which <img src="https://latex.codecogs.com/png.latex?%5Comega_i">, is a key object for a further formalization of probability measures.</li>
<li>We care not only about <strong>an event</strong> <img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cbigcup%5Climits_%7Bi%20=%201%7D%5En%20%5Comega_i"> and its realization, but also about other events in the Universal Set, because they might add information about the probability of occurring of our event of interest</li>
<li>The <strong>event space</strong> <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D"> should be defined on sets of subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> and this is where measure theory shines. We’ll discuss later in extensive detail the following conditions on the way to defining sigma-algebras. As can be seen later, we usually can’t define a probability measure on all sets of subsets.</li>
<li><strong>Probability as an extension of the measure</strong>: chance of events realizing. Note that the perceptible regularity can be thought as the ability to assign a probability to elementary events: <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5Comega_i)">. This is where additivity properties are key. A long discussion on Frequentist vs Bayesian interpretation of it can follow from here.</li>
<li>A <strong>probability triple</strong> <img src="https://latex.codecogs.com/png.latex?(%5COmega,%20%5Cmathcal%7BF%7D,%20%5Cmathbb%7BP%7D)"></li>
<li>The idea of <strong>Random Variable</strong></li>
</ol>
<p>Before moving on to probability measures, it’s useful to think about what a random variable really is and does, because formally, it’s neither a variable, nor random. That should be another motivation for speaking the language of measure theory.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/random_variable.png" class="img-fluid figure-img"></p>
<figcaption>Idea: Norman Wildberger, Gheorghe Ruxanda. A graphical representation of the random variable</figcaption>
</figure>
</div>
<p>The idea of the random variable as being a quantificator of elementary events (function defined on the outcome space which maps the elementary events to the real line in 1d) that perserves the informational structure of the sample space is very powerful, is formally defined and related to the idea of <strong>measurability</strong>.</p>
<blockquote class="blockquote">
<p>Start from some phenomena of interest and a random experiment. The random variable is a necessary abstraction in order to mathematically define quantificable characteristics of the objects.</p>
</blockquote>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0AX(%5Comega):%5COmega%20%5Crightarrow%20%5Cmathbb%7BR%7D%20%5C%5C%0As.t.%20~~%20%5C%7B%5Comega%20%5Cin%20%5COmega%20%7C%20X(%5Comega)%20%5Cleq%20r,%20%5Cforall%20r%20%5Cin%20%5Cmathbb%7BR%7D%20%5C%7D%20%5Cin%20%5Cmathcal%7BF%7D%0A%5Cend%7Balign%7D"></p>
<p>The idea of conservation of the informational structure is actually equivalent to the one of measurablility. If this property doesn’t hold, it’s not possible to explicitly and uniquely refer to the sets (events) of interest. The idea is that the preimage defined above <img src="https://latex.codecogs.com/png.latex?X%5E%7B-1%7D((-%5Cinfty,r%5D)%20=%20E%20%5Cin%20%5Cmathcal%7BF%7D"> on the following interval corresponds to an event E which should be in the event space <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D">. Because the only thing that varies is the limit of the interval r, the randomness comes from it. Also, it automatically suggests the idea of the Cumulative Distribution Function, which is <img src="https://latex.codecogs.com/png.latex?F_X(X%20%5Cle%20r)">.</p>
</section>
<section id="cant-have-it-all-the-trouble-with-the-uniform" class="level2">
<h2 class="anchored" data-anchor-id="cant-have-it-all-the-trouble-with-the-uniform">Can’t have it all: The trouble with the Uniform</h2>
<p><strong>Following the previous discussions</strong> we would want to define a probability measure <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%20:2%5E%5COmega%20%5Crightarrow%20%5B0,%201%5D"> on the set of all subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5B0,%201%5D"> for the uniform distribution. Unfortunately we can’t have that <strong>and</strong> perserve essential properties of probability measures.</p>
<blockquote class="blockquote">
<p>All other properties can be easily derived from these, thus these are minimal requirements for a probability measure. Nonetheless, these conditions are too restrictive if we want to define an Uniform Distribution on <img src="https://latex.codecogs.com/png.latex?2%5E%5COmega">.</p>
</blockquote>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5COmega)%20=%201"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5Cvarnothing)%20=%200"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(A)%20%5Cin%20%5B0,%201%5D"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?A%20%5Ccap%20B%20=%20%5Cvarnothing%20%5Cimplies%20%20%5Cmathbb%7BP%7D(%20A%20%5Ccap%20B)%20=%20%20%5Cmathbb%7BP%7D(A)%20+%20%5Cmathbb%7BP%7D(B)"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5C%7B%20A_i%20%5C%7D_%7Bi=1%7D%5E%5Cinfty"> s.t. <img src="https://latex.codecogs.com/png.latex?A_i%20%5Cbigcap%5Climits_%7Bi%20%5Cne%20j%7D%20A_j%20=%20%5Cvarnothing%20%5Cimplies%20%5Cmathbb%7BP%7D%20(%20%5Cbigcup%5Climits_%7Bi%20=%201%7D%5E%5Cinfty%20A_i)%20=%20%5Csum%5Climits_%7Bi%20=%201%7D%5E%7B%5Cinfty%7D%5Cmathbb%7BP%7D(A_i)"></li>
</ol>
<p>The idea of uniform distribution is closely related to the one of <strong>length</strong>, area, volume, depending on what space are we into. That means the probability measure will look like this:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20%5Cmathbb%7BP%7D(%5Ba,%20b%5D)%20=%20b%20-%20a%0A%5Cend%7Bequation%7D"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D%20:2%5E%5COmega%20%5Crightarrow%20%5B0,%201%5D"> and <img src="https://latex.codecogs.com/png.latex?0%20%5Cle%20a%20%5Cle%20b%20%5Cle%201"></p>
<p>The proof is done by contradiction, but the implications are a little bit deeper, related to paradoxes like <strong>Banakh-Tarsky</strong> and <strong>Vitali Sets</strong>, which are counter-intuitive but closely related to the idea of something being unmeasurable. Because we can’t get rid of any of the axioms, we should deal with the fact that we can’t define the measure on <img src="https://latex.codecogs.com/png.latex?2%5E%5COmega">.</p>
<p>Instead define a set of subsets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubset%202%5E%5COmega"> such that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D:%20%5Cmathcal%7BA%7D%20%5Crightarrow%20%5B0,1%5D">. These sets will have to obey certain properties and this is where all the terminology from measure theory comes in with algebras, semi-algebras and sigma-algebras. Each of these concepts and objects will be stepping stones towards understanding Caratheodori</p>
</section>
<section id="background-concepts-towards-caratheodori" class="level2">
<h2 class="anchored" data-anchor-id="background-concepts-towards-caratheodori">Background concepts towards Caratheodori</h2>
<p>Let’s continue on this upbeat note and try to figure out what kind of sets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> are measurable, for which we can define their probabilities.</p>
<p><strong>Def: Algebra and Semi Algebra:</strong> A set of subsets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubset%202%5E%5COmega"> is an algebra (field) if the following holds:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5COmega%20%5Cin%20%5Cmathcal%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarnothing%20%5Cin%20%5Cmathcal%7BA%7D"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathcal%7BA%7D"> then <img src="https://latex.codecogs.com/png.latex?A%5EC%20%5Cin%20%5Cmathcal%7BA%7D"> (closed under complements)</li>
<li>If <img src="https://latex.codecogs.com/png.latex?A,%20B%20%5Cin%20%5Cmathcal%7BA%7D"> then $ A B $ (closed under union). Note that 2 and 3 imply that it’s closed under countable intersection</li>
<li>For sigma-algebra: <strong>sigma</strong> refers to countability} If <img src="https://latex.codecogs.com/png.latex?%5C%7B%20%20A_i%20%5C%7D_%7Bi%20%5Cge%201%7D%20%5Cin%20%5Cmathcal%7BA%7D"> then <img src="https://latex.codecogs.com/png.latex?%5Cbigcup%5Climits_%7Bi%20%5Cge%201%7D%20A_i%20%5Cin%20%5Cmathcal%7BA%7D"> (closed under <strong>countable union</strong>)</li>
</ol>
<p>On an intuitive note, we define the probability measure on sigma-algebras because if certain conditions did not hold, the measure wouldn’t make sense.</p>
<p><strong>Def: Probability Measure:</strong> Suppose we have defined a <strong>measurable space</strong> <img src="https://latex.codecogs.com/png.latex?(%5COmega,%20%5Cmathcal%7BA%7D)">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is a sigma-algebra. A <strong>probability measure</strong> is the function <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D:%5Cmathcal%7BA%7D%20%5Crightarrow%20%5B0,%201%5D"> such that:</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5COmega)%20=%201"> </li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cforall%20%5C%7B%20A_i%20%5C%7D_%7Bi%20%5Cge%201%7D"> where <img src="https://latex.codecogs.com/png.latex?A_i%20%5Cbigcap%5Climits_%7Bi%20%5Cne%20j%7D%20A_j%20=%20%5Cvarnothing"> (countable sequences of mutually disjoint effects), <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5Cbigcup%5Climits_%7Bi%20%5Cge%201%7D%20A_i)%20=%20%5Csum%5Climits_%7Bi%20%5Cge%201%7D%20%5Cmathbb%7BP%7D(A_i)"></li>
</ol>
<p>As stated earlier, for more difficult cases, when it’s hard or impossible to reason in terms of probability density functions, it is more convenient to talk about measures. For the previous cases of point masses <img src="https://latex.codecogs.com/png.latex?%5Cdelta_k(x)"> and continuous functions we can ask the question what is the probability of a certain outcome directly if using measure-theoretic formalism. <img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5Cmathbb%7BR%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20=%202%5E%5COmega"> and the point mass looks basically like a spike at <img src="https://latex.codecogs.com/png.latex?k%5E%7Bth%7D"> place in the real line.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20%5Cmathbb%7BP%7D(A)%20=%20%5Cbegin%7Bcases%7D%0A%20%20%20%201,%20~~%20k%20%5Cin%20A%5C%5C%0A%20%20%20%200,%20~~%20k%20%5Cnotin%20A%0A%20%20%20%20%5Cend%7Bcases%7D%0A%5Cend%7Bequation%7D"></p>
<p>In order to define the probability measure for the continuous measure, much deeper results should be invoked. &gt; The Borel Spaces in itself encode a chain of new concepts that need to be understood from Banakh Spaces, Normed Spaces and how to close them under complement and union.</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5Cmathscr%7BC%7D(%5B0,1%5D;%5Cmathbb%7BR%7D)"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20=%20%5Cmathcal%7BB%7D(%5Cmathscr%7BC%7D(%5B0,1%5D;%5Cmathbb%7BR%7D))"></li>
</ol>
<p>This might be one of the reasons why Stochastic Processes is such a difficult and powerful field, because of the amount of knowledge encoded even in the “simplest” Brownian Motion (where <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D"> is a Weiner measure).</p>
<p>Going back to our pursuit of Caratheodori theorem, it is useful to understand why do we need countable additivity. If the finite additivity is clear, for example in the case of disjoint segments of the uniform distribution <img src="https://latex.codecogs.com/png.latex?X%20%5Csim%20Unif(%5B0,1%5D)">, <img src="https://latex.codecogs.com/png.latex?%5Ba_1,%20b_1%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Ba_2,%20b_2%5D">, it’s essential that the following holds.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BP%7D(%5Ba_1,%20b_1%5D%20%5Ccup%20%5Ba_2,%20b_2%5D%20)%20=%20%5Cmathbb%7BP%7D(a_1%20%5Cle%20X%20%5Cle%20b_1)%20+%20%5Cmathbb%7BP%7D(a_2%20%5Cle%20X%20%5Cle%20b_2)%0A"></p>
<p>Countable additivity <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BP%7D(%5Cbigcup%5Climits_%7Bi%20%5Cge%201%7D%20A_i)%20=%20%5Csum%5Climits_%7Bi%20%5Cge%201%7D%20%5Cmathbb%7BP%7D(A_i)"> is useful to prove that limits exists, which is very important in various statistical procedures. We can’t say anything about uncountable additivity because the measure of each element on the r.h.s. will be zero, while the measure of the interval is one, which is a contradiction</p>
<p>To get our feet wet, let’s see what techniques are employed by various authors in order to prove the impossibility of constructing a measure which has the idea of length while keeping the axioms.</p>
<p><strong>Proposition:</strong> There does not exist a (probability) measure <img src="https://latex.codecogs.com/png.latex?%5Clambda(A)"> <em>(Note that we’ll switch conventions to the measure-theoretic one employed by the Claudio Landim’s course as it’s one of the very few available online and it should be easier to follow in parallel with this reading)</em> defined for all subsets of <img src="https://latex.codecogs.com/png.latex?A%20%5Csubseteq%20%5B0,%201%5D"> satisfying.</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda:%20%5Cmathscr%7BP%7D(%5B0,1%5D)%20%5Crightarrow%20%5B0,1%5D"> which could be all rational numbers, for example</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Clambda(%5Ba,%20b%5D)%20%20=%20b%20-%20a"> as an extension of the idea of length</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cforall%20A%20%5Csubseteq%20%5B0,1%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Cforall%20x%20%5Cin%20%5B0,1%5D"> translation invariance <img src="https://latex.codecogs.com/png.latex?%5Clambda(A%20+%20x)%20%20=%20%5Clambda(A)">. Alternatively stated, <img src="https://latex.codecogs.com/png.latex?A%20+%20x%20=%20%5C%7Bx%20+%20y%20~%7C~%20y%20%5Cin%20A%20%5C%7D">.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?A%20=%20%5Cbigcup%5Climits_%7Bj%20%5Cge%201%7D%20A_j"> is an union of mutually disjoint sets <img src="https://latex.codecogs.com/png.latex?A_i%20%5Ccap%20A_j%20=%20%5Cvarnothing"> then <img src="https://latex.codecogs.com/png.latex?%5Clambda(A)%20=%20%5Csum%5Climits_%7Bj%20%5Cge%201%7D%20%5Clambda(A_j)"> This is exactly the notion of sigma-additivity encontered over and over again.</li>
</ol>
<blockquote class="blockquote">
<p>We can use <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> as in Rosenthal without loss of generality. Landim uses <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BR%7D_+%5Ccup%5C%7B%20+%5Cinfty%20%5C%7D">. Note that <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BP%7D(%5Ccdot)"> is the power set}</p>
</blockquote>
<p><strong>Proof:</strong> Assume that <img src="https://latex.codecogs.com/png.latex?%5Cexists"> a measure <img src="https://latex.codecogs.com/png.latex?%5Clambda"> such that above conditions hold. First, we need to introduce the notion of equivalence relation (in order to say “x is related to y”: <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20y">), which is key to proving this. The point is that the equivalence relation will partition a set, which allows us by invoking the Axiom of Choice to get towards the desired contradiction.</p>
<blockquote class="blockquote">
<p><strong>Relation set</strong>: S is a boolean function with <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20S"> <img src="https://latex.codecogs.com/png.latex?R:S%20%5Ctimes%20S%20%5Crightarrow%20%5C%7B0,%201%5C%7D"> Thus <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20y"> means x is <strong>related to</strong> y.</p>
</blockquote>
<p>Given an equivalence relation ~ and <img src="https://latex.codecogs.com/png.latex?x%20%5Cin%20S"> the <strong>equivalence class</strong> of x is <img src="https://latex.codecogs.com/png.latex?%5C%7B%20y%20%5Cin%20S%20%5Clvert%20y%20%5Csim%20x%20%20%5C%7D">. If x is an equivalence class then any pair of equivalence classes is either identical or disjoint. So, the relation forms equivalence classes, which form a partition on S.</p>
<p><strong>Def:</strong> A relation is an equivalence relation if</p>
<ul>
<li>reflexive: <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20x"> <img src="https://latex.codecogs.com/png.latex?~~%20%5Cforall%20x%20%5Cin%20S"></li>
<li>symmetric: <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20y%20%5Cimplies%20y%20%5Csim%20x"> <img src="https://latex.codecogs.com/png.latex?~~%20%5Cforall%20x,%20y%20%5Cin%20S"></li>
<li>tranzitive: <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20y"> and <img src="https://latex.codecogs.com/png.latex?y%20%5Csim%20z%20%20%5Cimplies%20x%20%5Csim%20z"> <img src="https://latex.codecogs.com/png.latex?~~%20%5Cforall%20x,%20y,%20z%20%5Cin%20S"></li>
</ul>
<p>Both Rosenthal and Landim use a special equivalence class involving a relation <img src="https://latex.codecogs.com/png.latex?x%20%5Csim%20y">, <img src="https://latex.codecogs.com/png.latex?x,%20y%20%5Cin%20%5Cmathbb%7BR%7D"> for rational numbers: <img src="https://latex.codecogs.com/png.latex?y%20-%20x%20%5Cin%20%5Cmathbb%7BQ%7D">. The equivalence class for x becomes</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%20%20%20%20%5Bx%5D%20=%20%5C%7B%20y%20%5Cin%20%5Cmathbb%7BR%7D%20%7C%20y%20-%20x%20%5Cin%20%5Cmathbb%7BQ%7D%20%5C%7D%20%5C%5C%0A%20%20%20%20%20%5CLambda%20=%20%5C%7B%20%5Calpha,%20%5Cbeta%20%5Cdots%20%5C%7D%20=%20%5Cmathbb%7BR%7D%20%5Clvert%20%5Csim%0A%5Cend%7Balign%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5CLambda"> is another important object (the set of equivalence classes) which is R modulo the equivalence relation, clearly uncountable, because <img src="https://latex.codecogs.com/png.latex?%5Bx%5D"> are countable. Now, using the Axiom of Choice a new set <img src="https://latex.codecogs.com/png.latex?%5COmega"> is constructed in the following way: for each equivalence class <img src="https://latex.codecogs.com/png.latex?%5Calpha,%20%5Cbeta"> <strong>one</strong> and only one element is selected.</p>
<blockquote class="blockquote">
<p>There are deep philosophical discussions regarding it, but it’s outside the scope of current project. Basically what we need to know is that it allows us to “simultaneously” choose from <img src="https://latex.codecogs.com/png.latex?%5CLambda_j"></p>
</blockquote>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/transition.png" class="img-fluid figure-img"></p>
<figcaption>A graphical representation assuming the Axiom of Choice.</figcaption>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%5Bx%5D"> can be chosen in such a way that <img src="https://latex.codecogs.com/png.latex?%5COmega%20%5Cin%20%5B0,%201%5D"></p>
<p>We reached a little milestone, as now there is a handle and structure to the problem, in contrast it wasn’t clear where to start from in the beginning. Here, Rosenthal is very brief, finishes the proof quickly and it seems that Landim chooses a much longer, but much more explicit way. A link to the alternative <a href="https://www.youtube.com/watch?v=qaCOTKh8o4w">proof</a>. I’ll choose the first one, because there are not many concepts used later for us to benefit by going through it.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20%5COmega%20=%20%5C%7B%20%5Comega_1%20%5Cin%20%5Calpha%20,%20%5Comega_2%20%5Cin%20%5Cbeta,%20%5Cdots%20%20%5C%7D%20%5Cin%20%5B0,%201%5D%0A%5Cend%7Bequation%7D"></p>
<p>A key claim is that if we translate <img src="https://latex.codecogs.com/png.latex?%5COmega"> by <img src="https://latex.codecogs.com/png.latex?p,%20q%20%5Cin%20%5Cmathbb%7BQ%7D">, the following dichotomy is true: either the sets are equal or disjoint (which was mentioned at the beginning of the proof). </p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%20%5COmega%20+%20q%20=%20%5COmega%20+%20p%20%20%20%5C%5C%0A%20%20%20%20(%5COmega%20+%20q)%20%5Ccap%20(%5COmega%20+%20p)%20=%20%5Cvarnothing%0A%20%20%20%20%5Cend%7Bcases%7D%0A%5Cend%7Bequation%7D"></p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5COmega"> contains an element from each equivalence class, each point in <img src="https://latex.codecogs.com/png.latex?(0,%201%5D%20%5Csubseteq%20%5Cbigcup%5Climits_%7Br%20%5Cin%20%5Cmathbb%7BQ%7D%7D%20(%5COmega%20+%20r)"> (is contained in the union of rational shifts of <img src="https://latex.codecogs.com/png.latex?%5COmega">).</p>
<p>Since <img src="https://latex.codecogs.com/png.latex?%5COmega"> has only one element <img src="https://latex.codecogs.com/png.latex?%5Cforall%20%5Bx%5D%20%5Cimplies%20%5COmega%20+%20r,%20%5Cforall%20r%20%5Cin%20%5B0,%201%5D"> are disjoint. Thus for <img src="https://latex.codecogs.com/png.latex?r%20%5Cin%20%5B0,%201%5D">,</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Balign%7D%0A%20%20%20%20%5Clambda(%5B0,%201%5D)%20&amp;=%20%5Csum%5Climits_%7Br%20%5Cin%20%5Cmathbb%7BQ%7D%7D%20%5Clambda(%5COmega%20+%20r)%20%5C%5C%0A%20%20%20%20&amp;=%20%5Csum%5Climits_%7Br%20%5Cin%20%5Cmathbb%7BQ%7D%7D%20%5Clambda(%5COmega)%0A%5Cend%7Balign%7D"></p>
<p>Notice the rhs is countably infinite sum, so it can be either zero or <img src="https://latex.codecogs.com/png.latex?+%5Cinfty"> or <img src="https://latex.codecogs.com/png.latex?-%5Cinfty">, but lhs is one. So, we arrive at the contradiction. For the last steps, some more understanding is needed. This is why the second proof is great and even though longer, very explicit.</p>
<p>So, what’s the trouble with the Uniform? Nothing particular, it’s just that not all subsets are measurable (have an associated measure). This is why we need concepts like semi-algebra, algebra and sigma-algebra in order to reason about what subsets of the power set are measurable.</p>
</section>
<section id="plan-of-attack" class="level2">
<h2 class="anchored" data-anchor-id="plan-of-attack">Plan of Attack</h2>
<p><strong>The idea</strong> is to extend the following measure <img src="https://latex.codecogs.com/png.latex?%5Clambda(%5Ba,%20b%5D)%20=%20b%20-%20a"> (of the length) to increasingly more strict scope with respect to <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BP%7D(%5Cmathbb%7BR%7D)">, while keeping the desired properties. I jumped a little bit ahead of myself defining the sigma-algebra, as there are more prerequisites and intermediary steps needed. Defining the following objects (<strong>classes of subsets</strong>) will help demistify a lot of terminology used</p>
<ul>
<li>semi-algebra</li>
<li>algebra</li>
<li>sigma-algebra</li>
</ul>
<p>We’ll construct an increasing set of more restrictive conditions. Basically, semi-algebra is weaker than algebra which is weaker than sigma-algebra. These classes of subsets have certain properties and subtle relationships between them and without understanding these, it is very hard to move on to extend the measure.</p>
<p><strong>Def:</strong> Semi-Algebra is a class of subsets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D%20%5Csubseteq%20%5Cmathscr%7BP%7D(%5COmega)"> if the following holds</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5COmega%20%5Cin%20%5Cmathcal%7BS%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarnothing%20%5Cin%20%5Cmathcal%7BS%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?A,%20B%20%5Cin%20%5Cmathcal%7BS%7D%20%5Cimplies%20A%20%5Ccap%20B%20%5Cin%20%5Cmathcal%7BS%7D"> (closed by finite intersections)</li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cforall%20A%20%5Cin%20%5Cmathcal%7BS%7D">, <img src="https://latex.codecogs.com/png.latex?%5Cexists%20%5C%7B%20E_1%20,%20%5Cdots,%20E_n%20%5C%7D%20%5Cimplies%20A%5EC%20=%20%5Csum%5Climits_%7Bj%20=%201%7D%5En%20E_j"> Complement viewed as a finite union of elements of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D"></li>
</ol>
<p>Imagine a segment <img src="https://latex.codecogs.com/png.latex?%5Ba,%20b%5D"> on <img src="https://latex.codecogs.com/png.latex?%5B0,%201%5D"> and take its complement, it is obvious that it can be represented as a finite union of sets of <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BS%7D">. Actually, these simple examples inspired the definition of semi algebra, but as we can see the conditions are weaker than the ones for algebra. The algebra and sigma-algebra were defined above, but it doesn’t hurt to inspect the relationship between the two in more detail.</p>
<p><strong>Def:</strong> Algebra is a class of subsets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D%20%5Csubseteq%20%5Cmathscr%7BP%7D(%5COmega)"> if the following holds</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5COmega%20%5Cin%20%5Cmathcal%7BA%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarnothing%20%5Cin%20%5Cmathcal%7BA%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?A,%20B%20%5Cin%20%5Cmathcal%7BA%7D%20%5Cimplies%20A%20%5Ccap%20B%20%5Cin%20%5Cmathcal%7BA%7D"> (closed by finite intersections)</li>
<li><img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathcal%7BA%7D%20%5Cimplies%20A%5EC%20%5Cin%20%5Cmathcal%7BA%7D"> (closed under complements, a stronger condition)</li>
</ol>
<p><strong>Remark:</strong> If <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is a sigma-algebra then <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is also a semi-algebra. This is important because we want to make statements about algebras generated by semi-algebras which have very nice properties.</p>
<p><strong>Remark:</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D_i%20%5Csubseteq%20%5Cmathscr%7BP%7D(%5COmega)"> be an algebra of subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> where <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20I"> could be any index. Then <img src="https://latex.codecogs.com/png.latex?%5Cbigcap%5Climits_%7Bi%20%5Cin%20I%7D%20%5Cmathcal%7BA%7D_i%20=%20%5Cmathcal%7BA%7D"> is also an algebra. This is verified by the definition of algebra.</p>
<p><strong>Def:</strong> Sigma-Algebra is a class of subsets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D%20%5Cin%20%5Cmathscr%7BP%7D(%5COmega)"> if the following holds</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5COmega%20%5Cin%20%5Cmathcal%7BF%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cvarnothing%20%5Cin%20%5Cmathcal%7BF%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?A_j%20%5Cin%20%5Cmathcal%7BF%7D"> <img src="https://latex.codecogs.com/png.latex?%5Cimplies%20%5Cbigcap%5Climits_%7Bj%20%5Cge%201%7D%20A_j%20%5Cin%20%5Cmathcal%7BF%7D"> (closed by <strong>countable</strong> intersections)</li>
<li><img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cmathcal%7BF%7D%20%5Cimplies%20A%5EC%20%5Cin%20%5Cmathcal%7BF%7D"> (closed under complements)</li>
</ol>
<p><strong>Remark:</strong> Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D_i%20%5Cin%20%5Cmathscr%7BP%7D(%5COmega)"> be a sigma-algebra of subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> where <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20I"> could be any index. Then <img src="https://latex.codecogs.com/png.latex?%5Cbigcap%5Climits_%7Bi%20%5Cin%20I%7D%20%5Cmathcal%7BF%7D_i%20=%20%5Cmathcal%7BF%7D"> is also an algebra. Same as in case of algebra</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2018-intro-measure/img/extension.png" class="img-fluid figure-img"></p>
<figcaption>The plan of attack. A sigma-additive measure defined on semi-algebra is extended to the algebra generated by the semi-algebra. The latter in turn is extended by Caratheodori theorem to the sigma-additive measure defined on sigma-algebra generated by the semi-algebra</figcaption>
</figure>
</div>
<p>Also needs to be shown that these extensions are unique at each step. Let’s take a break in order not to get lost in detail and terminology: What are we doing here? It was proven that we cannot define a measure extending the idea of length on <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BP%7D(%5COmega)"> under standard axioms of probability.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5COmega%20=%20%5B0,%201%5D,%20%5Cmathbb%7BP%7D(%5Ba,%20b%5D)%20=%20b%20-%20a">, but what is <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D">? A sigma-algebra, but it’s not obvious at all why.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bequation%7D%0A%20%20%20%20Unif(%5B0,%201%5D)%20%5Clongrightarrow%20(%5COmega,%20%5Cmathcal%7BF%7D,%20%5Cmathbb%7BP%7D)%0A%5Cend%7Bequation%7D"></p>
<p>That gave a motivation of defining measures on <strong>algebras</strong> with conditions of ever increasing strength/restriction. Relationships between these types of objects and their properties are key in making progress, which is quite clear from the diagram above. Even though these steps might look similar, the techniques and concepts employed are quite different. Nonetheless, the question always stays conceptually the same: On what can we define a (probability) measure? In essence, the whole language is developed in order to reason and make meaningful statements about sets of subsets and whether it can be “measured”.</p>
<blockquote class="blockquote">
<p>Also notice that we haven’t still reached even the start of what most measure-theoretic probability courses begin from. This is good, because we’re spending time on understanding the fundamentals on which all that theory is built.</p>
</blockquote>
<p>The next element we need is to define relationships between fields such as algebras generated by semi-algebras and so on. First, C. Landim introduces an algebra <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D(%5Cmathscr%7BC%7D)"> generated by a class of sets <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D%20%5Csubseteq%20%5Cmathscr%7BP%7D(%5COmega)"> such that the following holds</p>
<ol type="1">
<li><img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D%20%5Csubseteq%20%5Cmathcal%7BA%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D"> is the smallest algebra that contains <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D"></li>
<li>If <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BB%7D%20%5Csupseteq%20%5Cmathscr%7BC%7D"> is a sigma-algebra <img src="https://latex.codecogs.com/png.latex?%5Cimplies%20%5Cmathscr%7BB%7D%20%5Csupseteq%20%5Cmathcal%7BA%7D"></li>
</ol>
<p>Remarkably, the same is true for sigma-algebras, proven by a chain of thought involving all sets <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D_%5Calpha"> which contain <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D">. Their intersection <img src="https://latex.codecogs.com/png.latex?%5Cbigcap%5Climits_%5Calpha%20%5Cmathcal%7BA%7D_%5Calpha%20%20=%20%5Cmathcal%7BA%7D"> contains <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BB%7D"> and since it contains $ $, it belongs to that intersection, hence, it’s the smallest sigma-algebra that contains <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D">.</p>
<p>If the underlying class <img src="https://latex.codecogs.com/png.latex?%5Cmathscr%7BC%7D"> is a semi-algebra, then <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BA%7D(%5Cmathscr%7BC%7D)"> has an explicit form as a finite union of elements of semi-algebra. In the case of sigma-algebras, other techniques and arguments are needed because we don’t have such a form, which will be a major blocker.</p>
<ol type="1">
<li>Prove and formalize the last statement about sigma-algebras generated by semi-algebras</li>
<li>Explore additivity in measure functions</li>
<li>Explore sigma-additivity in measure functions</li>
<li>Understand Continuity from Above and Below and its connections with additivity</li>
<li>Extend the measure defined by semi-algebra on sigma-algebra generated by semi-algebra. Prove uniqueness</li>
</ol>
<p><strong>Theorem:</strong> (Caratheodori) Let <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D(%5COmega)"> be a semi-algebra of subsets of <img src="https://latex.codecogs.com/png.latex?%5COmega"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi:%20%5Cmathcal%7BF%7D%20%5Crightarrow%20%5B0,%201%5D"> with <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Cvarnothing)%20=%200"> and <img src="https://latex.codecogs.com/png.latex?%5Cpi(%5COmega)%20=%201"> satisfying the superadditivity property:</p>
<ol type="1">
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cpi(%5Cbigcup%5Climits_%7Bi%20=%201%7D%5Ek%20A_i)%20%5Cge%20%5Csum%5Climits_%7Bi%20=%201%7D%5Ek%20%5Cpi(A_i)"> where <img src="https://latex.codecogs.com/png.latex?A_i%20%5Cin%20%5Cmathcal%7BF%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbigcup%5Climits_%7Bi%20=%201%7D%5Ek%20A_i%20%5Cin%20%5Cmathcal%7BF%7D"> disjoint.</p></li>
<li><p><img src="https://latex.codecogs.com/png.latex?%5Cpi(A)%20%5Cle%20%5Csum%5Climits_n%20%5Cpi(A_n)"> where <img src="https://latex.codecogs.com/png.latex?A_i%20%5Cin%20%5Cmathcal%7BF%7D"> and <img src="https://latex.codecogs.com/png.latex?A%20%5Cin%20%5Cbigcup%5Climits_n%20A_n"></p>
<p>Then <img src="https://latex.codecogs.com/png.latex?%5Cexists"> a sigma-algebra <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D%20%5Csupseteq%20%5Cmathcal%7BF%7D"> and a countably additive probability measure <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*:%5Cmathcal%7BM%7D%5Crightarrow%20%5B0,%201%5D"> such that <img src="https://latex.codecogs.com/png.latex?%5Cpi%5E*(A)%20=%20%5Cpi(A)%20~~%20%5Cforall%20A%20%5Cin%20%5Cmathcal%7BF%7D%20%5Cimplies%20(%5COmega,%20%5Cmathcal%7BF%7D,%20%20%5Cpi%5E*)"> is a valid probability triple, which agrees with previous probabilities on <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BF%7D"> <span class="citation" data-cites="Rosenthal2006">(Rhosental 2006)</span></p></li>
</ol>
</section>
<section id="proof-left-as-an-exercise-just-kidding" class="level2">
<h2 class="anchored" data-anchor-id="proof-left-as-an-exercise-just-kidding">Proof Left as an Exercise [Just Kidding]</h2>
<p>We’re middle way through and already discovered a lot of insights, but there are more things to be done:</p>
<ol type="1">
<li>Define and understand the concept of outer measure</li>
<li>Prove that <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BM%7D"> is a sigma-algebra</li>
<li>Construct the extension on the new restriction</li>
<li>Learn about monotone classes</li>
<li>Prove the extension is unique via monotone classes</li>
<li>Look into “Extension of Caratheodori Extension Theorem”</li>
</ol>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-Landim2016" class="csl-entry">
Landim, Claudio. 2016. <span>“Caratheodori Theorem.”</span> <a href="https://www.youtube.com/watch?v=ZNH4eDM7cJo" class="uri">https://www.youtube.com/watch?v=ZNH4eDM7cJo</a>.
</div>
<div id="ref-Lawrence2012" class="csl-entry">
Lawrence, Evans. 2012. <span>“Why Measure Theory for Probability?”</span> <a href="https://www.youtube.com/watch?v=rAYA2Mu51bw" class="uri">https://www.youtube.com/watch?v=rAYA2Mu51bw</a>.
</div>
<div id="ref-McElreath" class="csl-entry">
McElreath, Richard. 2015. <em>Statistical Rethinking: A Bayesian Course with Examples in r and Stan</em>. Second. CRC.
</div>
<div id="ref-Bizovi" class="csl-entry">
Mihai, Bizovi. 2017. <span>“Stochastic Modeling and Bayesian Inference.”</span> ASE.
</div>
<div id="ref-Rosenthal2006" class="csl-entry">
Rhosental, Jeffrey S. 2006. <em>First Look at Rigorous Probability Theory</em>. Second. World Scientific.
</div>
<div id="ref-Ruxanda2011" class="csl-entry">
Ruxanda, Gheorghe. 2011. <span>“Consideratii Privind Abordarea Stochastica in Domeniul Economic.”</span> <a href="http://doccent.ase.ro/media/20111108_Abordarea_Cantitativa_Stochastica.pdfs" class="uri">http://doccent.ase.ro/media/20111108_Abordarea_Cantitativa_Stochastica.pdfs</a>.
</div>
<div id="ref-Ruxanda2017" class="csl-entry">
———. 2017. <span>“Probability Theory and Stochastic Modeling.”</span> Lecture.
</div>
</div></section></div> ]]></description>
  <category>probability</category>
  <guid>https://blog.economic-cybernetics.com/posts/2018-intro-measure/measure_theory.html</guid>
  <pubDate>Thu, 31 May 2018 21:00:00 GMT</pubDate>
</item>
<item>
  <title>Making Friends with Probability</title>
  <dc:creator>Bizovi Mihai</dc:creator>
  <link>https://blog.economic-cybernetics.com/posts/2017-probability/stochastic.html</link>
  <description><![CDATA[ 





<section id="foreword-from-2023" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="foreword-from-2023">Foreword from 2023</h2>
<p>Thinking deeply about uncertainty and probability theory changed my worldview and eventually became an essential part of my job. In 2023, I’m looking back at the game-changing encounters which instilled a passion for probability, followed by some paragraphs from the bachelor’s thesis<sup>1</sup>.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Bizovi Mihai - Stochastic Modeling and Bayesian Inference (2017)</p></div></div><ul>
<li>In 2013, it was Richard Feynman’s “The pleasure of finding things out”, which put the question of uncertainty somewhere on the back of my mind</li>
<li>In 2014, Taleb Nassim’s “Fooled by randomness” and “Black Swan” deeply resonated with me. There were many instances when I asked myself: “What would the academic do?”, “What would Taleb and Fat Tony do?”, “What would Hume and Popper say about that?”</li>
<li>In 2015, I was obsessed about systems’ dynamics and nonlinear differential equations for modeling the stocks and flows in an economy. The big question arose: how to actually represent the uncertainty and do inference for those parameters? It was also when the <a href="https://www.youtube.com/watch?v=BrK7X_XlGB8">“Visual guide to Bayesian thinking”</a> by Julia Galef popped up on my youtube recommendation feed.</li>
<li>In 2016, it was clear that the best job I could get was in ML and Data Science. Probabilistic ML and Probabilistic Graphical Models were the thing which I wanted to master, after Zoubin Ghahramani’s <a href="https://www.youtube.com/watch?v=kjo9Y_Vrgn4">brilliant lectures</a> in the Tubingen MLSS.
<ul>
<li>A bit earlier, we had our probability and mathematical statistics class – which I skipped to study after Joe Blitzstein’s <a href="https://projects.iq.harvard.edu/stat110/home">lectures</a> and book. Brilliant story proofs, relatable examples!</li>
</ul></li>
<li>In 2017, after the brilliant lectures in Multidimensional Data Analysis by Gheorghe Ruxanda at our university, I realized I know nothing about probability. There was so much more depth, nuance, philosophy, and history to it: Kolmogorov’s breakthrough<sup>2</sup>, measure-theoretic foundations and those stochastic processes.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn2"><div class="quarto-figure quarto-figure-center"><sup>2</sup>&nbsp;
<figure class="figure">
<p><img src="https://blog.economic-cybernetics.com/posts/2017-probability/kolmogorov.jpeg" class="img-fluid figure-img"></p>
<figcaption>Andrey Kolmogorov</figcaption>
</figure>
</div>
</div></div><p>In the thesis, I was interested in probabilistic approaches to modeling complex, dynamic economic phenomena. What stayed the same all these years, is a preference for the development of custom models for specific applications, where it is possible to explicitly declare domain and statistical assumptions, over an out-of-the-box Machine Learning solution.</p>
<p>Since then, after years of experience and practice, you can find an evolving presentation of the subject in the <a href="https://course.economic-cybernetics.com/01_fundamentals/stat_foundations.html">course website</a>. It is a work-in-progress, with the purpose of reminding students why did they study probability and what really matters.</p>
</section>
<section id="thoughts-from-the-thesis" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="thoughts-from-the-thesis">Thoughts from the thesis</h2>
<blockquote class="blockquote">
<p>”I can live with doubt and uncertainty and not knowing. We absolutely must leave room for doubt or there is no progress and there is no learning. There is no learning without having to pose a question. And a question requires doubt. People search for certainty, but there is no certainty.” — Richard Feynman, <em>The pleasure of finding things out</em></p>
</blockquote>
<section id="the-need-for-stochastic-modeling-and-bayesian-thinking" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-need-for-stochastic-modeling-and-bayesian-thinking">The need for stochastic modeling and Bayesian thinking</h3>
<p>Current challenges in economics need an interdisciplinary, multidimensional approach, and different perspectives. In order to have more powerful models in our toolbox, we’ll investigate statistical modeling based on three schools of thought: <strong>Stochastic Modeling</strong><sup>3</sup>, <strong>Machine Learning</strong>, and <strong>Bayesian Analysis</strong> – which has applications ranging from economics and finance to genetics, linguistics and artificial intelligence.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Stochastic, meaning probabilistic, specifically in the Fisherian and Neyman-Pearson frameworks.</p></div></div><p>This approach is extremely general and allows us to make inferences about latent quantities and relationships, to identify what is hidden beneath appearance, to account for the uncertainty and nonlinearities of economic phenomena. The idea of learning from data, flexibility of representing uncertainty and the parsimony principle leads to adaptive and robust modeling, able to capture non-trivial aspects of the problem. The focus will be very much on models and a deep understanding of concepts on which they’re built.</p>
<p>The ugly truth is that humans are not good at prediction, neither consistent with probability theory. We are “suffering” from multiple evolutionary mechanisms, cognitive biases and even statisticians, formally trained in the language of uncertainty, aren’t guarded against pitfalls that arise in practice and everyday life.</p>
<p>The same story is in reconciliation of new evidence with prior beliefs, processing multidimensional data, ignorance of feedback loops and dynamics. The reason we are building models is to better understand the world, phenomena around us and in the end to improve our mental representations<sup>4</sup>. Predictions that are less wrong usually take into account a variety of perspectives, a proven fact in ensemble modeling.</p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;The only thing which I would add here in 2023, is an appreciation for Action and Decisions. Back then, knowledge was foregrounded for me.</p></div></div></section>
<section id="three-perspectives" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="three-perspectives">Three perspectives</h3>
<p>Machine learning models have become an essential part of online services and is taking over new fields, bringing firms and people lots of value. The multidimensional approach is what makes it all this possible. In contrast with classical statistics and NHST (Null Hypothesis Significance Testing)<sup>5</sup>, our focus will be on generalization performance and the idea of a model. Even though these models have their roots in statistics, the field can be viewed in its own right.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;In 2023, I wouldn’t equivocate between NHST and the Neyman-Pearson, action-oriented frequentism, which can be done well</p></div></div><p>An important extension for which there is an acute need is dynamical models. A lot of interesting problems are multidimensional time series, which might invalidate the hypotheses behind classical data mining models. Coupled with the need of accounting for uncertainty, this brings us to the idea of introducing <strong>stochastic</strong> elements and to extend the idea of <strong>learning</strong> and <strong>generalization</strong>.</p>
<p>The third perspective which brings together stochastic elements with machine learning is <em>bayesian inference</em>, as a method to update beliefs and knowledge based on data from different sources. Bayesian modeling enables us to build hierarchical models with latent structures, while encoding the uncertainty in parameters and operating with probability distributions. On the shoulders of these three “giants”, a stochastic perspective over Machine Learning emerges over the last two decades, which is flexible, general and more transparent than neural networks, works on “thin”<sup>6</sup> data.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Perhaps I meant small samples at group level, which would make sense in the context of multilevel models.</p></div></div><p>There are several reasons the field is not yet in the mainstream: scalability and complexity of the models, in the sense of necessary knowledge and skill to start modeling. Once the software implementations, approximation methods and probabilistic programming languages will become better, the Bayesian methods will see another rise in popularity.</p>
<p>The next chapters will be dedicated to each of these fundamental perspectives, in which we’ll try to get to the essence. The last ones will be on supervised and unsupervised probabilistic models such as Gaussian Processes and Mixture Models. An essential concept will be the ARD (Automatic Relevance Determination) and Kernels which encourage sparse solutions.</p>
</section>
<section id="the-goal" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-goal">The goal</h3>
<p>The motivation behind the choice of probabilistic machine learning models is that there are a lot of opportunities to extend them and reuse as parts of a more complicated model.</p>
<p>For example, Gaussian Processes<sup>7</sup> can be transformed into a Relevance Vector Machine (the probabilistic version of SVM) or used for Bayesian Optimization of expensive functions, in learning of complex dynamical phenomena, spatio-temporal modeling (also known as Kreiging). The Probabilistic Principal Component Analysis can be used for a latent representation of a multidimensional time series and intrinsic dimensionality determination. On the over hand, Mixture Models are a powerful model-based clustering technique, especially coupled with nonparametric techniques.</p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;This is still true, although fallen a bit out-of-fashion. However, the Bayesian Additive Regression Trees have risen to the challenge, especially in the field of Causal Inference.</p></div></div><p>This approach is extremely powerful, intellectually fascinating and vibrant. It is also in the spirit of economic complexity, with ideas which bridge together different branches in data analysis. The catch is, that there are enormous barriers for beginners to start developing such models. First, a qualitative jump in understanding of probability theory and stochastic processes, Bayesian inference and theory of statistical learning is needed. Another barrier is how to go from understanding the models towards the implementation in actual software products.</p>
<p>So, the main goal is to find connections between the three perspectives described before, to recognize Bayesian generalizations of classical machine learning models. The most important is the process of exploration and understanding, internalization of models, which will suggest new areas of study and practical opportunities.</p>


</section>
</section>


 ]]></description>
  <category>probability</category>
  <category>bayes</category>
  <category>philosophy</category>
  <guid>https://blog.economic-cybernetics.com/posts/2017-probability/stochastic.html</guid>
  <pubDate>Sun, 06 Aug 2017 21:00:00 GMT</pubDate>
</item>
</channel>
</rss>
