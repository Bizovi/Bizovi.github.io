---
title: "R Tutorials for Behavioral Economics Class"
author: "Bizovi Mihai"
date: "2017-11-22"
output: html_document
---

In the last blog post I took a bird's eye (personal) perspective of R programming and suggested not to be discouraged by early encounters with this seemingly weird language. The conclusion was that by following "the right tool for the right job" principle, R is a great language for statistical research and the ecosystem of packages improves the data analysis workflow and gives the modeler more tools to extract insights from data.

> What is the link to Behavioral Economics? R will be used (in class) as a tool for exploring domain-specific data in order to make informed decisions, fitting different kinds of models and validating the intuition from readings like Tversky & Kahneman, Thaler on real data.

## A tutorial on Metropolitan Area Housing

```{r, message = FALSE, warning=FALSE}
library(tidyverse) # the ecosystem of data processing tools
library(tidyr)     # useful for getting the data in short-long formats
library(reshape2)  # for the function melt (long format data)
library(glue)      # string formatting
library(assertive) # assertions (active tests)
library(ochRe)     # stunning color palettes
library(readxl)    # read from excel spreadsheets
```

```{r}
data <- read_excel("data/landdata-msas-2016q1.xls", skip = 1)
data <- data %>% 
  mutate(MSA = as.factor(MSA), 
         Date = zoo::as.yearqtr(Date, format = "%YQ%q")
         ) %>%
  dplyr::rename(home_value = `Home Value`, 
                structure_cost = `Structure Cost`, 
                land_value = `Land Value`, 
                land_share = `Land Share (Pct)`, 
                home_pi = `Home Price Index`, 
                land_pi = `Land Price Index`
                )
data %>% head()
```

```{r fig.widht = 9}
ny <- data %>% dplyr::filter(MSA == "NEWYORK") 
ny %>% 
  tidyr::gather(key = variable, value = value, -c(MSA, Date)) %>% 
  dplyr::select(-MSA) %>% 
  ggplot(aes(x = value, fill = variable)) + 
  geom_histogram(alpha = 0.5) + 
  facet_wrap(~ variable, scale = "free") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs(y = "Frequency", title = "Distributions of the variables for NY") + 
  theme(legend.position = "none") + 
  scale_fill_ochre(palette = "nolan_ned")
```


## Plotting theoretical Utility and Value functions

Hyperbolic Absolute Risk Aversion (Linear Risk Tolerance) is a class of utility functions, that can easily describe the particular cases of Constant Absolute Risk Aversion (CARA) and Constant Relative Risk Aversion (CRRA).
$$U(x;\gamma,a,b) = \frac{1 - \gamma}{\gamma}\bigg( \frac{ax}{1-\gamma} +b   \bigg) ^ \gamma$$
It is designed in such a way that the risk tolerance is linear
$$\tau(x)=-\frac{U'(x)}{U''(x)} = \frac{1}{1-\gamma}x + 
\frac{b}{a}
$$
where $a > 0$ and $\frac{ax}{1-\gamma} + b > 0$

The function tends to logarithmic if gamma tends to zero (by L'Hopital rule) and linear when goes to one.
$$U(x) \xrightarrow[\gamma \to 0]{} log(ax+b)$$

> The reason we're going to overkill a simple plotting of HARA utility function is to showcase some important R concepts.

First, we test user inputs via the `assertthat` package, which is a first step moving from manual validation to more automated methods of code testing. This doesn't kick in when doing an exploratory analysis, but hopefully when starting to build Packages, learning how to test code in R^[Testing R Code http://r-pkgs.had.co.nz/tests.html] will pay off. 

```{r}
hara <- function(x, a, b, gamma) {
  # check that inputs to the function are good
  # using assetthat package
  assert_all_are_non_negative(x)
  assert_all_are_greater_than(x = a * x / (1 - gamma) + b, y = 0)
  assert_all_are_true(
    c(    
      is_positive(a), 
      is_not_equal_to(a, 0), 
      is_in_range(gamma, lower = 0, upper = 1)
      )
  )
  
  # return the function itself
  return(
    ((1 - gamma) / gamma) * (a * x / (1 - gamma) + b)^gamma
  )
}
```

Second, we want to run the function a number of times by varying a parameter. The first idea of would be to write a for loop, but there is a more elegant way. The apply family of functions allows to run a function on elements of objects like lists. In this particular case I used a `sapply` function which returns a data frame with a column for values of function given a parameter. 

```{r}
# check if the function returns expected values
# hara(x = 1:20, a = 36, b = 52, gamma = 0.5)

gammas <- c(0.5, 0.4, 0.3, 0.25, 0.1)
a <- 36 # the functions start to look more linear as a is small
b <- 52 

# apply the function on using 4 different gamma parameters
df <- sapply(X = gammas, FUN = hara, x = 0:30,  a = a, b = b) %>% 
  as_tibble()
# add the appropriate values to column names
colnames(df) <- as.character(gammas)
```

Third, we use the tidy way of data processing by heavily relying on the pipe operator `%>%`, which passes an output of a function or an object to the next one. 
This makes the code much more readable and shows its power in Exploratory Data Analysis. The concept of Tidy Data^[Hadley Wickham's R for Data Science http://r4ds.had.co.nz/] is inspired from relational databases and in order to use R eficiently one needs to understand the use-cases for long and wide data formats, what classifies as tidy.

The fourth point is that using `ggplot2` as the go-to visualization tool is not harder or more advanced, but simpler! Once you understand the `grammar of graphics`^[Paper on the grammar of Graphics http://byrneslab.net/classes/biol607/readings/wickham_layered-grammar.pdf] you get an incredibly general tool for data visualization, in which you can do stunning graphs with very little effort once you get the data into the right format. 

```{r}
df %>% 
  mutate(x = 1:nrow(df)) %>% 
  reshape2::melt(id.vars = "x") %>% # could use tidyr::gather as an alternative
  rename(gamma = variable) %>%
  ggplot(aes(x = x, y = value, color = gamma)) + 
  geom_line(size = 1) + 
  theme_minimal() + 
  labs(title = "Hyperbolic Absolute Risk Aversion function", 
       subtitle = glue("with hyperparameters a = {a}, b = {b}")) + 
  scale_color_ochre(palette = "nolan_ned") # if you feel fancy today

## Explain what is a color palette and why is it useful
```


## A tutorial on Analysis of Returns and Investment Decisions

<!--
```{r message=FALSE, warning=FALSE}
library(tidyquant) # tidy data analysis and financial data interfacing
```

```{r}
# get stock prices for seven years using the Yahoo Finance API
stocks <- c("ORCL", "FSLR", "AMZN", "GOOG", "GE") %>% tq_get(
  get  = "stock.prices", 
  from = "2010-01-01", 
  to   = "2017-10-30"
)

# calculate the monthly returns
monthly.returns <- stocks %>% 
  group_by(symbol) %>% 
  tq_transmute(
    select     = adjusted, # column to calculate returns from 
    mutate_fun = periodReturn, # function from quantmod
    period     = "monthly",    # alternatively, "weekly" or daily
    col_rename = "return")

# need to compare against a general price index
baseline <- "SPX" %>%
    tq_get(
      get  = "stock.prices",
      from = "2010-01-01",
      to   = "2017-10-30") 


monthly.baseline <- baseline %>%
    tq_transmute(
      select     = adjusted, 
      mutate_fun = periodReturn, 
      period     = "monthly", 
      col_rename = "base.returns")

```

```{r}
# In order to perserve some information when averaging at monthly level, it would be great to represent the uncecrtainty

custom_stat_fun <- function(x, na.rm = TRUE, ...) {
    # ...   = additional arguments
    c(mean    = mean(x, na.rm = na.rm),
      stdev   = sd(x, na.rm = na.rm),
      quantile(x, na.rm = na.rm, ...)) 
}

# quantiles
pr <- c(0, 0.025, 0.25, 0.5, 0.75, 0.975, 1)

# Applying the custom function by week
monthly_returns <- stocks %>%
  group_by(symbol) %>%
    tq_transmute(
        select = close,
        mutate_fun = apply.monthly, 
        FUN = custom_stat_fun,
        na.rm = TRUE,
        probs = pr
    )

monthly_returns %>%
    ggplot(aes(x = date, y = `50%`, color = symbol)) +
    geom_ribbon(aes(ymin = `25%`, ymax = `75%`), 
                color = NA, fill = palette_light()[[1]], alpha = 0.3) +
    geom_point() +
    geom_vline(xintercept = as.Date(c("2011-09-01", "2012-07-01", 
                                      "2013-07-01", "2014-02-01", 
                                      "2015-01-01", "2016-02-01")), 
               lty = 2, color = "grey40") + 
    geom_ma(n = 5, size = 1, color = "steelblue2", lty = 1) + 
    facet_wrap(~ symbol, ncol = 2, scale = "free_y") +
    labs(title = "Standard and Poor's Average Monthly Price", x = "",
         subtitle = "Volatility shown as 1st and 3rd quantiles",
         y = "Price (k$)") +
  #  expand_limits(y = 50) + 
    scale_color_ochre(palette = "lorikeet") +
    theme_bw() + 
  theme(legend.position = "none")
```

-->









  