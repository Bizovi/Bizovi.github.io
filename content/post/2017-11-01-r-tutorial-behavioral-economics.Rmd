---
title: "R tutorial for Behavioral Economics Class"
author: "Bizovi Mihai"
date: "2017-11-01"
output: 
  html_document:
    toc: true
---
>  Why would R be a great choice for Behavioral Economics labs?
  
  R is a flexible and in my opinion underrated^[Especially with all the hype surrounding Deep Learning and Python, frameworks like Tensorflow] language for *statistical research*, which means it's specialized and really good at data analysis. It's a nice match, as in Behavioral Economics we want to look at evidence, real choices, analyze and model it in a reproducible fashion. It took me some time to appreciate some of the unconventional methods and techniques of R, the elegance of graphics and data processing. As I believe in using `the right tool for the right job`, both in modeling and developing data products, learning R paid off. 
  
  
  Because of the fast-growing open-source community and the ecosystem of R packages, a lot of cutting-edge research is made freely available. Some of these extensions changed in a fundamental way how we analyze data in R. Even though its flexibility allows to have a lot of solutions to the same problem, we should keep in mind the pitfalls of flexible languages and be rigurous in the way we write our code and do the analysis. 
  
## Towards Reproducible Research in R
  
  The replication crisis^[https://en.wikipedia.org/wiki/Replication_crisis] is a hot topic right now in different fields. There are many great meta-studies on this topic, but my goal is very modest modest here: to suggest simple techniques that will help improve the reproducibility and transparency of the research. By making the source code freely available and reproducible we reduce the "research debt"^[https://distill.pub/], making the progress a lot faster by involving people that have the curiosity to reproduce and improve on exsiting research. 
  
It has three main components and R does have nice solutions:

  * Code reproducibility and versioning 
  * Data and Analysis
  * But it works on my machine: Keeping the environment consistent
  
  First, we can use `Rmarkdown`^[On using Rmarkdown https://www.youtube.com/watch?v=DNS7i2m4sB0] to combine the content, code and outputs rendered in a single pdf document or html page, which can be easily published on the web using a static page generator like Hugo/Jekyll. Second, using a code versioning tool like Git^[Rstudio github integration: http://www.datasurg.net/2015/07/13/rstudio-and-github/] is a no-brainer, even if you're working alone. It has nice integrations with RStudio and it will save the future you a lot of time. 

  What cannot be solved by packages alone, should be obtained as a result of a rigurous statistical methodology^[Columbia University stats course: https://courses.edx.org/courses/course-v1:ColumbiaX+DS101X+1T2016/course/], combined with a quality, clean code. It requires a lot of discipline amid seemingly endless possibilities of R. 
  
  The third key element to reproducibility is also very challenging. The hot solution right now is using `Docker Containers`^[An intro to Docker: https://www.youtube.com/watch?v=NBrQbkZ3Lzw&t=2177s], which are lightweight, immutable and disposable, providing necessary libraries, binaries and OS components to run the analysis. They should be viewed more like a process, than a virtual machine which is started from an image (remember .iso). It saves a lot of time and effort trying to install all the packages manually, dealing with versioning issues and so on. Imagine some of the harder to install setups for Bayesian Analysis, involving STAN or JAGS are taken care of by somebody else, so you can focus on really important things like improving the method, the quality, robustness and generality of the code.


## Tidy Data Analysis and Visualization

  There is an emerging way of data analysis in R which old-school users might mistakenly ignore: the `tidy` way.^[A playlist on the tidyverse https://www.youtube.com/watch?v=K-ss_ag2k9E&list=PLNtpLD4WiWbw9Cgcg6IU75u-44TrrN3A4] It had such an impact on the community, that it made R cool again. It's not without a flaw, especially when talking about production environments and writing packages from scratch, but that's why we have the right tool for the right job principle. If you can do an analysis using this ecosystem and principles much easier and in a readable, intuitive way.
  
  For R novices, a simple advice is that everything you see is an object and everything that does something is a function. The second thing is that the power of R lies in vectorisation, done behind the scenes by extremely fast libraries. Of course there are cases when you might use a for loop and it will be a good solution, but for the most purposes it isn't, so pay a close attention to the functions in `*apply` family. 

## A tutorial on Metropolitan Area Housing

## Plotting theoretical Utility and Value functions

Hyperbolic Absolute Risk Aversion
$$U(x) = \frac{1 - \gamma}{\gamma}\bigg( \frac{ax}{1-\gamma} +b   \bigg) ^ \gamma$$
where $a > 0$ and $\frac{ax}{1-\gamma} +b > 0$
```{r, message = FALSE, warning=FALSE}
library(tidyverse) # the ecosystem of data processing tools
library(tidyr)     # useful for getting the data in short-long formats
library(reshape2)  # for the function melt (long format data)
library(glue)      # meaningful plot annotations
library(assertive) # assertions on the code
```


```{r}
hara <- function(x, a, b, gamma) {
  # check that inputs to the function are good
  # using assetthat package
  assert_all_are_non_negative(x)
  assert_all_are_greater_than(x = a * x / (1 - gamma) + b, y = 0)
  assert_all_are_true(
    c(    
      is_positive(a), 
      is_not_equal_to(a, 0), 
      is_in_range(gamma, lower = 0, upper = 1)
      )
  )
  
  # return the function itself
  return(
    ((1 - gamma) / gamma) * (a * x / (1 - gamma) + b)^gamma
  )
}

# check if the function returns expected values
# hara(x = 1:20, a = 36, b = 52, gamma = 0.5)

gammas <- c(0.5, 0.4, 0.3, 0.25)
a <- 36
b <- 52

# apply the function on using 4 different gamma parameters
df <- sapply(X = gammas, FUN = hara, x = 0:30,  a = a, b = b) %>% 
  as_tibble()
# add the appropriate values to column names
colnames(df) <- as.character(gammas)

df %>% 
  mutate(x = 1:nrow(df)) %>% 
  reshape2::melt(id.vars = "x") %>% # could use tidyr::gather as an alternative
  rename(parameter = variable) %>%
  ggplot(aes(x = x, y = value, color = parameter)) + 
  geom_line(size = 1) + 
  theme_minimal() + 
  labs(title = "Hyperbolic Absolute Risk Aversion function", 
       subtitle = glue("with hyperparameters a = {a}, b = {b}"))
```

## A tutorial on Analysis of Returns and Investment Decisions

<!--  
## How to grow as an R programmer

* Learn the basics and the particular way of doing things in R
* Try to apply best practices of clean code, versioning and tools used
* Practice a lot in data cleanup, processing, visualization and modeling
* Develop Shiny Apps
* Develop an R package
* Automated Testing of your code
* Using Docker Containers
* Implementing Machine Learning Pipelines in Production Environment
* Taking R to the limit 
* Use R with C++/Fortran routines, for customly developed algorithms
* Reading RJournal
* Alocate time to go though vignettes of interest to your field of study
* Design an architecture of packages for a given application
-->
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  