---
title: "R Programming. The big picture"
author: "Bizovi Mihai"
date: "2017-11-01"
output: 
  html_document:
    toc: true
---

> I wish somebody showed me the real power of R earlier and explained the big picture 

This is not an usual tutorial on R, my goal being to make you aware and curious about various topics related to Data Analysis in R, which I learned the hard way during a year of nearly daily use. It is not supposed to be easy or have a particular application in mind, but rather to suggest many possibilities. Also, even though this was originally written for a Behavioral Economics class, I'll be speaking from a Data Scientist's perspective and often drift away and not get in too much details, preferring to provide great (external) resources for learning. 

## A personal encouter with R

First, a few words on how I decided to choose R as my go-to language. Stepping back, my first encounter with scientific computing was Nathan Kutz's brilliant course on `Mathematical Methods for Data Analysis`, emphasizing that we'd be doing well if we were MATLAB superstars. Another motivation for using MATLAB was my passion for the field of economic complexity, which involved nonlinear differential equations and required simulation. I played around with software like Steve Keen's `Minsky` for System Dynamics Models, `NetLogo` for Agent-Based Models. This was during a time when I wasn't thrilled by the teaching of statistics and econometrics, as those (linear, gaussian) models seemed far from satisfactory for explaining complex economic phenomena.


The first course in R did not leave the impression that R was a powerful language, but rather weird and potentially useful for statistics. As time passed I shifted completely from Economic Research and criticising Neoclassical Economics towards Machine Learning and Data Science. I decided to learn Python over MATLAB, as it was clear I would probably apply Machine Learning models in a business environment and not in academia. R seemed at the time as a second choice to Python, but I had the chance to discover it a little bit more, especially the `John Hopkins R Programming` courses. This led to me finding a lot of great blogs, books and articles by following the experts in R community. The priority then was getting a great theoretical understanding of Learning Theory and Data Science landscape, which brought me back to appreciating some of the deceptively "simple" concepts and models in statistics and econometrics. 


It was a matter of time, readings and practice which made me prefer `RMarkdown` and `knitr` over Jupyter Notebooks (IPython), and R over Python for the problems I wanted to solve at `Adore Me`. I shifted my focus completely on understanding R and taking advantage of the features of this domain-specific language. In the end, it depends on what problems are you trying to solve. If I wanted to focus on Deep Learning research, it's extremely probable that my language of choice would be Python. In a business setting, in order to bring value through statistical modeling, diverse perspectives are needed: from Econometrics and Machine Learning to Bayesian Statistics, Time Series Analysis and Simulation. R is perfect in that respect, while also helping in communicating the data analysis results and incentivising researchers to adhere to good practices. 


During a year I learned a lot of tricks in Exploratory Data Analysis using `tidyverse` and `ggplot2`, automated report generation using RMarkdown and Latex, built an interactive decision-support tool using Shiny, implemented my first end-to-end machine learning project, including a package with a custom model for TV Attribution, learned the importance of code versioning, wrote my bachelor thesis and created this website in R. 


Still, every time I find out new things to learn, like the tidy way of doing time series analysis using `tidyquant`, the rlang evaluations, unit testing, paralell computation and lots of cutting-edge statistical models.  


>  Why would R be a great choice for Behavioral Economics labs?
  
  R is a flexible and in my opinion underrated (Especially with all the hype surrounding Deep Learning and Python, frameworks like Tensorflow) language for *statistical research*, which means it's specialized and really good at data analysis. It's a nice match, as in Behavioral Economics we want to look at evidence, real choices, analyze and model it in a reproducible fashion. It took time to appreciate some of the unconventional methods and techniques in R, the elegance of graphics and data processing. As I believe in using `the right tool for the right job`, both in modeling and developing data products, learning R paid off. 
  
  Because of the fast-growing open-source community and the ecosystem of R packages, a lot of cutting-edge research is made freely available. Some of these extensions changed in a fundamental way how we analyze data in R. Even though its flexibility allows to have a lot of solutions to the same problem, we should keep in mind the pitfalls of flexible languages and be rigurous in the way we write our code and do the analysis. 
  
## Towards Reproducible Research in R
  
  The replication crisis^[https://en.wikipedia.org/wiki/Replication_crisis] is a hot topic right now in different fields. There are many great meta-studies, but my goal is very modest here: to suggest simple techniques that will help improve the reproducibility and transparency of the research. By making the source code freely available and reproducible we reduce the "research debt"^[https://distill.pub/], making the progress a lot faster by involving people that have the curiosity to reproduce and improve on existing research. 
  
It has three main components and R does have nice solutions:

  * Code reproducibility and versioning 
  * Data and Analysis
  * But it works on my machine: Keeping the environment consistent
  
  First, we can use `Rmarkdown`^[On using Rmarkdown https://www.youtube.com/watch?v=DNS7i2m4sB0] to combine the content, code and outputs rendered in a single pdf document or html page, which can be easily published on the web using a static page generator like Hugo/Jekyll. Second, using a code versioning tool like Git^[Rstudio github integration: http://www.datasurg.net/2015/07/13/rstudio-and-github/] is a no-brainer, even if you're working alone. It has nice integrations with RStudio and will save the future you a lot of time. 

  What cannot be solved by packages alone, should be obtained as a result of a rigorous statistical methodology^[Columbia University stats course: https://courses.edx.org/courses/course-v1:ColumbiaX+DS101X+1T2016/course/], combined with a quality, clean code. It requires a lot of discipline amid seemingly endless possibilities of R. 
  
  The third key element to reproducibility is also very challenging. The hot solution right now is using `Docker Containers`^[An intro to Docker: https://www.youtube.com/watch?v=NBrQbkZ3Lzw&t=2177s], which are lightweight, immutable and disposable, providing necessary libraries, binaries and OS components to run the analysis. They should be viewed more like a process, than a virtual machine which is started from an image (remember .iso). It saves a lot of time and effort trying to install all the packages manually, dealing with versioning issues and so on. Imagine some of the harder to install setups for Bayesian Analysis, involving STAN or JAGS are taken care of by somebody else, so you can focus on really important things like improving the method, the quality, robustness and generality of the code.


## Tidy Data Analysis and Visualization

  There is an emerging way of data analysis in R which old-school users might mistakenly ignore: the `tidy` way.^[A playlist on the tidyverse https://www.youtube.com/watch?v=K-ss_ag2k9E&list=PLNtpLD4WiWbw9Cgcg6IU75u-44TrrN3A4] It had such an impact on the community, that it made R cool again. It's not without a flaw, especially when talking about production environments and writing packages from scratch, but that's why we have the right tool for the right job principle. If you can do an analysis using this ecosystem and principles much easier and in a readable, intuitive way, why not use the simpler tool.
  
  For R novices, a simple advice is that everything you see is an object and everything that does something is a function. The second thing is that the power of R lies in vectorisation, done behind the scenes by extremely fast libraries. Of course there are cases when you might use a for loop and it will be a good solution, but for the most purposes it isn't, so pay a close attention to the functions in `*apply` family. 


## A tutorial on Analysis of Returns and Investment Decisions
  
  
  
  
  
  
  
  