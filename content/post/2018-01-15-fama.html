---
title: "CAPM and Eugene Fama's devastating critique"
author: "Bizovi Mihai"
date: "2018-01-15"
output: html_document
tags: ["R", "behavioral-economics"]
---



<blockquote>
<p>I was impressed by the down-to-earth <a href="https://www.youtube.com/watch?v=bM9bYOBuKF4">debate</a> between Eugene Fama and Richard Thaler. Their discussion was very insightful in order to make sense of what’s going on with Efficient Market Hypothesis, CAPM, Fama and French 3 Factor Model, Markowitz and where is the field moving. This will be my last blog post on economics for a while, so expect lots of Machine Learning and Statistics topics next. This is a continuation that is supposed to add some missing pieces to the analysis done in the <a href="https://bizovi.github.io/post/2017-12-28-investment-decisions/">partI</a> and <a href="https://bizovi.github.io/post/2017-12-30-investment-decisions-2/">partII</a></p>
</blockquote>
<p>My first impression is that the microfoundation, theory-based CAPM is just an invalid model from a variety of perspectives, there is too much evidence against it<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. On the other line of analysis, the empirical approach by Fama relies on the EMH, which he admitted is almost untestable. This means that the focus is on asset pricing models that work in real world and that the data should show us the way. For <code>E.F.</code>, rigorous statistical testing of the hypotheses is of the foremost importance. His objection to the notion of a bubble can be understood as “show me the statistical procedure/test”, as it is easy to say in retrospective that something was a bubble. Usually, I might disagree with the theory that CAPM is build on, but here I can rely on one of the fathers of modern finance and the whole body of evidence to disregard it. <code>R.T.</code> separated the EMH in an useful way by asking two questions, which could help avoid lots of confusion:</p>
<ul>
<li>Can you beat the market?</li>
<li>Are the prices right?</li>
</ul>
<p>From other videos of E.Fama, it is clear that he is frustrated by the behaviorists cherrypicking anomalies without regard to the psychological theory and in order to invalidate a theory you should come with your own, so, “what’s your theory”? From a practical point of view, the points they were debating do not make a huge difference. <code>R.T.</code> argues we can have a pretty good guess if something is a bubble in order to intervene very gently, even though there might not be a formal test for it.</p>
<p>I considered going through the Chicago’s Booth <a href="https://www.youtube.com/playlist?list=PLAXSVuGaw0KxTEN_cy-RCuEzzRdnF_xtx">Asset Pricing (PartI)</a> and <a href="https://www.youtube.com/playlist?list=PLAXSVuGaw0KxVUym8IRkObSbUPEFaSbPt">Asset Pricing (PartII)</a> course in order to understand all these issues deeply, but the outlook of saying in the end <code>&quot;just go buy an index fund&quot;</code> looks grim. After all it is worth it, because when we build an empirical model, we might implicitly make some huge assumptions about expectations or predictability. So, in order not to make stupid mistakes it’s better to know the theory well. I’ll throw in another red pill related to the <a href="https://paulromer.net/wp-content/uploads/2016/09/WP-Trouble.pdf">state of macroeconomics</a>, which should do justice to Eugene saying that Finance is the most successful part of economics.</p>
<blockquote>
<p>Another thing which became clear, is that the previous questions I was raising and doubts I had are totally founded. It should’t be something new for the people in the field, but it’s certaily useful for a newcomer</p>
</blockquote>
<div id="analysing-portfolio-models-without-reading-fama-is-useless" class="section level2">
<h2>Analysing portfolio models without reading Fama is useless</h2>
<p>Before digging into the data, I would like to go through the Fama and French 2004 <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/0895330042162430">paper</a>. As a lot of things in economics, this powerful and intuitive theoretical model does not hold against empirical evidence. Last <a href="https://bizovi.github.io/post/2017-12-30-investment-decisions-2/">time</a>, we built portfolios using ideas from Markowitz (in a toy form) at time <span class="math inline">\(t - 1\)</span> resulting in a stochastic return at time <span class="math inline">\(t\)</span>, where we as investors care about the mean and return (implicitly assuming risk aversion). If taken one step further, we’d have to solve one of the dual problems of maximizing expected returns given variance or minimizing variance given returns.</p>
<p><img src="/post/2018-01-15-fama_files/figure-html/meanvar.png" width="672" /></p>
<p>In order to “tell the CAPM story” we should add to the usual picture the risk-free rate, which could bring additional investment opportunities, the efficient fronteer becoming a straight line. Formally:</p>
<p><span class="math display">\[
R_p = wR_f + (1 - w)R_{rp}
\]</span> <span class="math display">\[ 
\mathbb{E}(R_p)=wR_f + (1-w)\mathbb{E}(R_{rp})
\]</span> <span class="math display">\[
\mathbb{V}(R_p) = (1-w)\mathbb{V}(R_{rp})
\]</span> Basically what we care about, is that the efficient portfolio will be a linear combination of the risk-free asset and a risky portfolio, tangent to the risky efficiency fronteer.</p>
<p>Now, in order to have a testable relationship between return and risk, CAPM adds two more assumptions to the Markowitz model. Let’s take the quote from Fama, as it’s quite the pill to swallow:</p>
<blockquote>
<p><em>The first assumption is complete agreement: given market clearing asset prices at <span class="math inline">\(t-1\)</span>, investors agree on the joint distribution of asset returns from <span class="math inline">\(t-1\)</span> to <span class="math inline">\(t\)</span>. And this distribution is the true one-that is, it is the distribution from which the returns we use to test the model are drawn. The second assumption is that there is borrowing and lending at a risk-free rate, which is the same for all investors and does not depend on the amount borrowed or lent.</em></p>
</blockquote>
<p>In the end, it implies that the market portfolio <span class="math inline">\(M\)</span> must be on the minimum variance fronteer, so the minimum variance portfolio should hold for the market portfolio (then all investors should see the same opportunity set?). For <span class="math inline">\(i = 1, ..., N\)</span> risky assets <span class="math display">\[
\mathbb{E}(R_i)=\mathbb{E}(R_{ZM}) + \beta_{iM}[\mathbb{E}(R_M)- \mathbb{E}(R_{ZM})]
\]</span> This expression is easily recognizable if we replace <span class="math inline">\(\mathbb{E}(R_{ZM})\)</span> with <span class="math inline">\(R_f\)</span>, which is the security market line and it tells the story of the linear relationship between <code>beta</code> (non-diversifiable, systematic risk) and asset returns, conceptually holding for negative values of beta. All the interpretations are extremely tricky and even more, we swipe under the carpet the assumptions of quadratic utility functions (or probability distribution with two moments).</p>
<p><span class="math display">\[
\beta_{iM}=\frac{cov(R_i, R_M)}{\mathbb{V}(R_M)}
\]</span> As beta is basically a regression coefficient, it shows how does a return of the asset vary as a result of variation in market return given all other factors stay constant. Fama gives an even more insightful interpretation: the risk of market portfolio <span class="math inline">\(\mathbb{V}(R_M)\)</span> is a weighted average of covariance risks of assets in Market, i.e. proportional to the contribution to portfolio risk.</p>
<p><span class="math display">\[
\mathbb{V}(R_m)=Cov(R_M, R_M)=Cov \bigg(\sum_{i=1}^N x_{iM}R_i, R_M \bigg) \\ = \sum_{i=1}^N x_{iM}Cov(R_i, R_M )
\]</span></p>
<p>Last step is to get rid of <span class="math inline">\(\mathbb{E}(R_{ZM})\)</span>. If we assume <code>unrestricted risk-free borrowing and lending</code>, that asset doesn’t contribute to market risk (variance), so it can be replaced by the risk free rate. Fisher Black shows we don’t have to make this assumption, but the alternative is not much more realistic.</p>
<p>Fama and French go on to describe different approaches for testing the empirical validity of CAPM and find multiple problems with it. Nonetheless, it’s a stepping stone to the more relevant models. I am losing my interest quickly here, but at least I know what I don’t know right now and might come back at it some day.</p>
<blockquote>
<p>One thing is clear, that the exercise that will be done below misses the larger point of all this (theoretical) struggle to find the “perfect portfolio”. It seems like all of this is pointless unless you go back and read all foundational papers by Fama, Black, Merton, Fisher and the following most cited empirical tests of theoretical models.</p>
</blockquote>
</div>
<div id="toy-example-of-capm" class="section level2">
<h2>Toy Example of CAPM</h2>
<blockquote>
<p>See the previous blog <a href="https://bizovi.github.io/post/2017-12-28-investment-decisions/">posts</a> for a justification of why the following stocks were chosen and a visualization, exploratory data analysis. We’ll also throw in a “risk-free” asset and the S&amp;P price index.</p>
</blockquote>
<pre class="r"><code>library(tidyquant)
library(timekit)
library(ochRe)</code></pre>
<p>The question on how much data to estimate the CAPM is very tricky, and basically should be calibrated for, in order for the estimations to be reliable. I stumbled upon an amazing answer on <a href="https://www.quora.com/How-many-years-historical-returns-should-be-used-to-calculate-the-beta-of-a-company">Quora</a> by S. Hakala, so here is a brief summary: it depends. There are issues of adjusting for changes in the company or industry over time, financial leverage and to accurately reflect the state of it at the current moment. Financial crisis will distort betas, but also looking only at the recovery, removing outliers is also important. There seems so much more to it than the recommended rule of thumb of three years. If you just estimate betas without asking all those questions, the estimates can be very unreliable, which was indeed shown decade after decade by Fama and French.</p>
<p>We’ll do the most naive thing here, following the workflow from <a href="https://cran.r-project.org/web/packages/tidyquant/vignettes/TQ05-performance-analysis-with-tidyquant.html">tiqyquant</a>. The right thing to do though, is to seriously think about all of the above questions and the problem becomes really hard. Also, the time period for calculating returns, is not at all given or restricted to daily returns.</p>
<pre class="r"><code>risky.list &lt;- c(&quot;GOOGL&quot;, &quot;AMZN&quot;, &quot;NFLX&quot;, &quot;JNJ&quot;, &quot;TSLA&quot;)
Ra &lt;- risky.list %&gt;% 
  tq_get(
    get  = &quot;stock.prices&quot;, 
    from = &quot;2015-01-01&quot;, 
    to   = &quot;2017-12-31&quot;
  ) %&gt;% 
  group_by(symbol) %&gt;% 
  tq_transmute(
    select     = adjusted,
    mutate_fun = periodReturn,
    period     = &quot;daily&quot;, 
    col_rename = &quot;Ra&quot;
  )
Ra</code></pre>
<pre><code>## # A tibble: 3,775 x 3
## # Groups:   symbol [5]
##    symbol       date           Ra
##     &lt;chr&gt;     &lt;date&gt;        &lt;dbl&gt;
##  1  GOOGL 2015-01-02  0.000000000
##  2  GOOGL 2015-01-05 -0.019053850
##  3  GOOGL 2015-01-06 -0.024679487
##  4  GOOGL 2015-01-07 -0.002940986
##  5  GOOGL 2015-01-08  0.003484133
##  6  GOOGL 2015-01-09 -0.012211246
##  7  GOOGL 2015-01-12 -0.007309480
##  8  GOOGL 2015-01-13  0.009536052
##  9  GOOGL 2015-01-14  0.008230381
## 10  GOOGL 2015-01-15 -0.003794958
## # ... with 3,765 more rows</code></pre>
<pre class="r"><code>SP &lt;- getSymbols(&quot;SP500&quot;, src = &quot;FRED&quot;, env = NULL)
Rb &lt;- SP %&gt;% na.omit() %&gt;% tk_tbl(rename_index = &quot;date&quot;) %&gt;% 
  rename(Rb = SP500) %&gt;% 
  filter(date &gt; as.Date(&quot;2015-01-01&quot;) &amp; 
           date &lt; as.Date(&quot;2017-12-31&quot;)) %&gt;% 
  tq_transmute(
    select     = Rb,
    mutate_fun = periodReturn, 
    period     = &quot;daily&quot;, 
    col_rename = &quot;Rb&quot;
  )
# check the number of days
# nrow(Rb); nrow(Ra[Ra$symbol == &quot;GOOGL&quot;,])
Rb</code></pre>
<pre><code>## # A tibble: 755 x 2
##          date           Rb
##        &lt;date&gt;        &lt;dbl&gt;
##  1 2015-01-02  0.000000000
##  2 2015-01-05 -0.018278107
##  3 2015-01-06 -0.008893486
##  4 2015-01-07  0.011629823
##  5 2015-01-08  0.017888346
##  6 2015-01-09 -0.008403891
##  7 2015-01-12 -0.008093662
##  8 2015-01-13 -0.002578565
##  9 2015-01-14 -0.005813063
## 10 2015-01-15 -0.009247888
## # ... with 745 more rows</code></pre>
<pre class="r"><code>data &lt;- left_join(Ra, Rb, by = &quot;date&quot;)
data</code></pre>
<pre><code>## # A tibble: 3,775 x 4
## # Groups:   symbol [?]
##    symbol       date           Ra           Rb
##     &lt;chr&gt;     &lt;date&gt;        &lt;dbl&gt;        &lt;dbl&gt;
##  1  GOOGL 2015-01-02  0.000000000  0.000000000
##  2  GOOGL 2015-01-05 -0.019053850 -0.018278107
##  3  GOOGL 2015-01-06 -0.024679487 -0.008893486
##  4  GOOGL 2015-01-07 -0.002940986  0.011629823
##  5  GOOGL 2015-01-08  0.003484133  0.017888346
##  6  GOOGL 2015-01-09 -0.012211246 -0.008403891
##  7  GOOGL 2015-01-12 -0.007309480 -0.008093662
##  8  GOOGL 2015-01-13  0.009536052 -0.002578565
##  9  GOOGL 2015-01-14  0.008230381 -0.005813063
## 10  GOOGL 2015-01-15 -0.003794958 -0.009247888
## # ... with 3,765 more rows</code></pre>
<p>It is useful to observe how the estimations are sensitive to the time interval compared to the last estimation which had more data. The <span class="math inline">\(R^2\)</span> are decent, but nothing too thrilling. All the KPIs below have some interpretation, and people came up with rules involving Taynor ratios and so on.</p>
<pre class="r"><code># tq_performance_fun_options()
capm &lt;- data %&gt;% tq_performance(
  Ra = Ra, 
  Rb = Rb, 
  performance_fun = table.CAPM
)
capm  </code></pre>
<pre><code>## # A tibble: 5 x 13
## # Groups:   symbol [5]
##   symbol ActivePremium  Alpha AnnualizedAlpha   Beta `Beta-` `Beta+`
##    &lt;chr&gt;         &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1  GOOGL        0.1668 0.0006          0.1674 1.0415  1.0237  1.0681
## 2   AMZN        0.4689 0.0015          0.4583 1.1323  1.1661  1.0771
## 3   NFLX        0.4771 0.0016          0.5022 1.3698  1.6434  1.0343
## 4    JNJ        0.0413 0.0003          0.0705 0.6958  0.6639  0.8136
## 5   TSLA        0.0328 0.0003          0.0781 1.1883  1.4545  0.9449
## # ... with 6 more variables: Correlation &lt;dbl&gt;,
## #   `Correlationp-value` &lt;dbl&gt;, InformationRatio &lt;dbl&gt;, `R-squared` &lt;dbl&gt;,
## #   TrackingError &lt;dbl&gt;, TreynorRatio &lt;dbl&gt;</code></pre>
<pre class="r"><code># for single assets
data %&gt;% tq_performance(
  Ra = Ra, 
  Rb = NULL, 
  performance_fun = SharpeRatio, 
  Rf = 0.01 / 12, 
  p = 0.95)</code></pre>
<pre><code>## # A tibble: 5 x 4
## # Groups:   symbol [5]
##   symbol `ESSharpe(Rf=0.1%,p=95%)` `StdDevSharpe(Rf=0.1%,p=95%)`
##    &lt;chr&gt;                     &lt;dbl&gt;                         &lt;dbl&gt;
## 1  GOOGL               0.033305503                   0.012430554
## 2   AMZN               0.099488946                   0.060601819
## 3   NFLX               0.056259544                   0.048889087
## 4    JNJ              -0.018435266                  -0.034349641
## 5   TSLA              -0.001536649                  -0.003683382
## # ... with 1 more variables: `VaRSharpe(Rf=0.1%,p=95%)` &lt;dbl&gt;</code></pre>
<pre class="r"><code># for portfolios
w &lt;- tibble(
  symbols = c(&quot;GOOGL&quot;, &quot;AMZN&quot;, &quot;NFLX&quot;, &quot;JNJ&quot;, &quot;TSLA&quot;),
  weights = c(0.4, 0.25, 0.25, 0.05, 0.05)
  )
p1 &lt;- Ra %&gt;% tq_portfolio(
  assets_col = symbol, 
  returns_col = Ra, 
  weights    = w, 
  col_rename = &quot;Ra&quot;
)
# mean(p1$Ra); sd(p1$Ra)</code></pre>
<p>From a brief look at the scatterplots, a removal of outliers might help get more robust estimates.</p>
<pre class="r"><code>data %&gt;% 
  ggplot(aes(x = Rb, y = Ra, color = symbol)) + 
  geom_point(alpha = 0.2) + 
  geom_hline(yintercept = c(0.06, -0.07), lty = 2, color = &quot;grey40&quot;) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) + 
  facet_wrap(~symbol, ncol = 3) +
  scale_color_ochre(palette = &quot;tasmania&quot;) +
  theme_minimal() + 
  labs(x = &quot;Baseline SP500 Return&quot;, y = &quot;Asset Return&quot;, 
       title = &quot;Relationship between asset return and market&quot;)</code></pre>
<p><img src="/post/2018-01-15-fama_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<blockquote>
<p>Assess the validity of the model using t-tests, Ramsey test for linearity, homoskedasticity test, Autocorrelation of first and higher order (Durbin-Watson statistic), Kolmogorov-Smirnov Normality test, Chow test. I always wondered about the carelessness these tests are taught, as tests are also models and might fail in unexpected ways, are based on assumptions. As a practitioner of statistics it would be nice not to abuse them myself.</p>
</blockquote>
<blockquote>
<p>Evaluate the performance of CAPM on hold-out dataset</p>
</blockquote>
</div>
<div id="soft-event-analysis" class="section level2">
<h2>Soft Event Analysis</h2>
<blockquote>
<p>What happened at the global level? Market Level?</p>
</blockquote>
<blockquote>
<p>Extensive descriptive statistics for a pair of market index and a stock (for daily returns)</p>
</blockquote>
<p>Cohraine and Larsen, other two giants of finance argue that there’s a whole world beyond analysing returns: Volumes and Prices themselves.</p>
<blockquote>
<p>ACF, PACF, ARMA, ARCH-GARCH and model selection</p>
</blockquote>
</div>
<div id="concluding-remarks" class="section level2">
<h2>Concluding remarks</h2>
<p>In conclusion, it’s impossible to rigorously study and build models of portfolios without knowing the assumptions. After carefully listening to these giants of finance, “technical analysis” presented in the last two posts seems like woodoo. If you show me a great performance by someone using technical anaysis, how much can be attributed to chance (when you take into account the sample size)? Suddenly, Taleb Nassim’s “Fooled by Randomness” presses the right pain points, even if you don’t like it.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Fama, Eugene F; French, Kenneth R (Summer 2004). “The Capital Asset Pricing Model: Theory and Evidence”<a href="#fnref1">↩</a></p></li>
</ol>
</div>
